
@inproceedings{cidSpaceWeatherConditions2017,
  title = {Space {{Weather Conditions}} during the {{Galaxy}} 15 and {{Telstar}} 401 {{Satellites Anomalies}}},
  abstract = {Two GEO satellite anomalies have been related to electrostatic discharge during 1997 - 2010: Telstar 401 and Galaxy 15. This paper compares space weather conditions at the time of the anomaly for both events to check if there were similar conditions at the geospace that might increase risk of electrical discharge for surface and internal charging.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm37},
  author = {Cid, C. and Saiz, E. and Palacios, J. and Guerrero, A. and Cerrato, Y.},
  month = sep,
  year = {2017},
  keywords = {artificial satellites,Clouds,Discharges (electric),electrical discharge,electrostatic discharge,Electrostatic discharges,ESD,Galaxy 15,GEO satellite anomalies,hazards,magnetic storms,Meteorology,Satellite broadcasting,Satellites,solar activity,Space vehicles,space weather,space weather conditions,Telstar 401 satellites},
  pages = {149-150},
  file = {/home/yuri/Zotero/storage/5S267ENX/Cid et al. - 2017 - Space Weather Conditions during the Galaxy 15 and .pdf;/home/yuri/Zotero/storage/E5M6HR5J/Cid et al. - 2017 - Space Weather Conditions during the Galaxy 15 and .pdf;/home/yuri/Zotero/storage/MP84AAT5/Cid et al. - 2017 - Space Weather Conditions during the Galaxy 15 and .pdf;/home/yuri/Zotero/storage/3XFHAWF2/8227557.html;/home/yuri/Zotero/storage/5YS3PYHA/8227557.html;/home/yuri/Zotero/storage/W65AUZI8/8227557.html}
}
% == BibTeX quality report for cidSpaceWeatherConditions2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{feldmannQualityIncreasingDevelopmentProcess2017,
  title = {A {{Quality}}-{{Increasing Development Process}} for {{LEO Satellite Software}}},
  abstract = {In this paper, an overview of a testing and simulation approach for a DTN protocol implementation (named {$\mu$}PCN) is provided. This implementation is intended to render low-cost world-wide satellite communication possible. It will be flighttested during the ESA OPS-SAT mission. To ensure its quality, a simulation environment has been developed used for testing its functionality in simulated LEO satellite network constellations.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm36},
  author = {Feldmann, M. and Walter, F. and B{\"o}hm, R. and Chorin, J. and Jonck{\`e}re, O. D. and Hareau, T. and Nitsch, M. and Ritter, H.},
  month = sep,
  year = {2017},
  keywords = {artificial satellites,Satellites,aerospace computing,delay tolerant networks,delay-tolerant networking protocol,DTN protocol,ESA OPS-SAT mission,LEO satellite network constellations,LEO satellite software,Low earth orbit satellites,low-cost world-wide satellite communication,protocols,Protocols,quality-increasing development process,Roads,satellite communication,Software,Testing,Tools,Î¼PCN},
  pages = {147-148},
  file = {/home/yuri/Zotero/storage/9D9KJDVL/Feldmann et al. - 2017 - A Quality-Increasing Development Process for LEO S.pdf;/home/yuri/Zotero/storage/E46FKS3Z/Feldmann et al. - 2017 - A Quality-Increasing Development Process for LEO S.pdf;/home/yuri/Zotero/storage/7T9QXJPS/8227556.html;/home/yuri/Zotero/storage/7VJGLNP9/8227556.html}
}
% == BibTeX quality report for feldmannQualityIncreasingDevelopmentProcess2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{roperoVirtualRealityMission2017,
  title = {A {{Virtual Reality Mission Planner}} for {{Mars Rovers}}},
  abstract = {For the operation of Mars rovers the ground team has a set of utilities to generate safe mission planning. A relevant aspect of such planning are the paths that the rover has to to follow to reach the scientific objectives. For path planning, operators use a combination of orbital and surface imagery to analyze the terrain topography to generate a cost map. Applying path planning algorithms over the cost map it is possible to obtain (sub)optimal solutions to safely drive the rover through different waypoints. As well, the paths obtained can be assessed using simulators and handmade improved by means of Virtual Reality techniques. In this paper we present an integrated planner for the mission planning and its evaluation using the recent technological advances in Virtual Reality. We have developed a low cost application to provide a three-dimensional view of the path generated exploiting real Mars surfaces with the aim of helping operators in deciding the best paths to follow.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm35},
  author = {Ropero, F. and Mu{\~n}oz, P. and {R-Moreno}, M. D. and Barrero, D. F.},
  month = sep,
  year = {2017},
  keywords = {Space vehicles,aerospace engineering,AR,collision avoidance,control engineering computing,Google,Headphones,Mars,Mars rovers,Mars surfaces,mobile robots,path planning,Path planning,path-planning,planetary rovers,planning,Planning,Robots,rover operations,safe mission planning,surface imagery,terrain topography,Three-dimensional displays,virtual reality,Virtual Reality mission planner,Virtual Reality techniques,VR},
  pages = {142-146},
  file = {/home/yuri/Zotero/storage/FAN3IRN5/Ropero et al. - 2017 - A Virtual Reality Mission Planner for Mars Rovers.pdf;/home/yuri/Zotero/storage/XLRYE7XF/Ropero et al. - 2017 - A Virtual Reality Mission Planner for Mars Rovers.pdf;/home/yuri/Zotero/storage/57E9CND5/8227555.html;/home/yuri/Zotero/storage/MVKT9W8B/8227555.html}
}
% == BibTeX quality report for roperoVirtualRealityMission2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{anderssonLEONProcessorDevices2017,
  title = {{{LEON Processor Devices}} for {{Space Missions}}: {{First}} 20 {{Years}} of {{LEON}} in {{Space}}},
  shorttitle = {{{LEON Processor Devices}} for {{Space Missions}}},
  abstract = {The LEON series of processors has enabled space missions during the two past decades. This paper discusses the past, present and future of the LEON series of SPARC 32-bit space-grade microprocessors and system-on-chip devices.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10.1109/SMC-IT.2017.31},
  author = {Andersson, J. and Hjorth, M. and Johansson, F. and Habinc, S.},
  month = sep,
  year = {2017},
  keywords = {system-on-chip,Software,Clocks,Computer architecture,Europe,high-reliability,LEON,LEON processor devices,low-power electronics,microcontroller,Microcontrollers,microprocessor chips,multi-core,processor,radiation tolerant,Random access memory,space missions,space vehicle electronics,spacegrade,SPARC,SPARC 32-bit space-grade microprocessors,System-on-chip,system-on-chip devices},
  pages = {136-141},
  file = {/home/yuri/Zotero/storage/9H659PIM/Andersson et al. - 2017 - LEON Processor Devices for Space Missions First 2.pdf;/home/yuri/Zotero/storage/AL8K7S4F/8227554.html}
}
% == BibTeX quality report for anderssonLEONProcessorDevices2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{fraireIntroducingContactPlan2017,
  title = {Introducing {{Contact Plan Designer}}: {{A Planning Tool}} for {{DTN}}-{{Based Space}}-{{Terrestrial Networks}}},
  shorttitle = {Introducing {{Contact Plan Designer}}},
  abstract = {Either by resource-constrained communication systems, high signal propagation delay, or planet occlusion, space-terrestrial networks can neither expect a continuous nor instantaneous end-to-end connectivity. However, episodes of communications can be precisely computed based on orbital elements and then imprinted in contact plans to optimize Delay/Disruption Tolerant Networking (DTN) routing solutions. In this work, we present the first prototype of Contact Plan Designer, a tool conceived not only to generate accurate contact plans but also to fine-tune them, resolve future resource conflicts and analyze or iterate over expected network metrics.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm33},
  author = {Fraire, J. A.},
  month = sep,
  year = {2017},
  keywords = {Satellites,delay tolerant networks,Tools,Contact Plan Design,Contact Plan Designer,Delay/Disruption Tolerant Networking,Delay/Disruption Tolerant Networking routing solutions,DTN-based space-terrestrial networks,high signal propagation delay,Internet,planet occlusion,Planetary orbits,planning tool,resource-constrained communication systems,Routing,Satellite Networks,space communication links,Space-Terrestrial Networking,telecommunication network routing,Topology},
  pages = {124-127},
  file = {/home/yuri/Zotero/storage/CTG55D9R/Fraire - 2017 - Introducing Contact Plan Designer A Planning Tool.pdf;/home/yuri/Zotero/storage/WHECBSYU/Fraire - 2017 - Introducing Contact Plan Designer A Planning Tool.pdf;/home/yuri/Zotero/storage/2L89N43X/8227551.html;/home/yuri/Zotero/storage/MU2ULYK4/8227551.html}
}
% == BibTeX quality report for fraireIntroducingContactPlan2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{fraireDtnSimBridgingGap2017,
  title = {{{DtnSim}}: {{Bridging}} the {{Gap}} between {{Simulation}} and {{Implementation}} of {{Space}}-{{Terrestrial DTNs}}},
  shorttitle = {{{DtnSim}}},
  abstract = {Existing Internet protocols assume persistent end-to-end connectivity, which cannot be guaranteed in disruptive and high-latency space-terrestrial networks. To operate over these challenging networks, Delay/Disruption Tolerant Networking (DTN) architecture has been proposed. Although recent advances, evaluating DTN routing in large-scale networks spanning prolonged time-evolving topologies remain an unsolved issue. In this work, we introduce DtnSim, an event-driven DTN simulation tool that leverages routing models with flight-software routing algorithms implementations. This paper describes how DtnSim allows to conveniently and intuitively determine network flow metrics which otherwise would require complex prototyping and prolonged experiment times.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm32},
  author = {Fraire, J. A. and Madoery, P. and Raverta, F. and Finochietto, J. M. and Velazco, R.},
  month = sep,
  year = {2017},
  keywords = {Satellites,Space vehicles,delay tolerant networks,Protocols,Delay/Disruption Tolerant Networking,Internet,Routing,Satellite Networks,Space-Terrestrial Networking,Delay/Disruption Tolerant Networking architecture,DTN routing,DTN simulation tool,DtnSim,Earth,flight-software routing algorithms implementations,high-latency space-terrestrial networks,Internet protocols,Ions,large-scale networks,network flow metrics,persistent end-to-end connectivity,prolonged time-evolving topologies,routing protocols,telecommunication network topology},
  pages = {120-123},
  file = {/home/yuri/Zotero/storage/3E2M2YGC/Fraire et al. - 2017 - DtnSim Bridging the Gap between Simulation and Im.pdf;/home/yuri/Zotero/storage/KDWJ35HE/Fraire et al. - 2017 - DtnSim Bridging the Gap between Simulation and Im.pdf;/home/yuri/Zotero/storage/FBBWNN9X/8227550.html;/home/yuri/Zotero/storage/GG87ZXM4/8227550.html}
}
% == BibTeX quality report for fraireDtnSimBridgingGap2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{dubeyDREMSOSOperatingSystem2017,
  title = {{{DREMS}}-{{OS}}: {{An Operating System}} for {{Managed Distributed Real}}-{{Time Embedded Systems}}},
  shorttitle = {{{DREMS}}-{{OS}}},
  abstract = {Distributed real-time and embedded (DRE) systems executing mixed criticality task sets are increasingly being deployed in mobile and embedded cloud computing platforms, including space applications. These DRE systems must not only operate over a range of temporal and spatial scales, but also require stringent assurances for secure interactions between the system's tasks without violating their individual timing constraints. To address these challenges, this paper describes a novel distributed operating system focusing on the scheduler design to support the mixed criticality task sets. Empirical results from experiments involving a case study of a cluster of satellites emulated in a laboratory testbed validate our claims.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3z},
  author = {Dubey, A. and Karsai, G. and Gokhale, A. and Emfinger, W. and Kumar, P.},
  month = sep,
  year = {2017},
  keywords = {Satellites,cloud computing,distributed operating system,distributed processing,distributed real-time embedded systems,DRE systems,DREMS-OS,embedded cloud computing platforms,embedded systems,individual timing constraints,mixed criticality task sets,mobile cloud computing platforms,operating system,operating systems (computers),partitioned operating systems,real-time operating system,Real-time systems,Schedules,scheduling,Sensors,space applications,spatial scales,temporal scales},
  pages = {114-119},
  file = {/home/yuri/Zotero/storage/4GKMFJEC/Dubey et al. - 2017 - DREMS-OS An Operating System for Managed Distribu.pdf;/home/yuri/Zotero/storage/S7BPBMHI/Dubey et al. - 2017 - DREMS-OS An Operating System for Managed Distribu.pdf;/home/yuri/Zotero/storage/KN6WCAGZ/8227549.html;/home/yuri/Zotero/storage/TKUQ7EAS/8227549.html}
}
% == BibTeX quality report for dubeyDREMSOSOperatingSystem2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{rosaSensorPlanningSystem2017,
  title = {Sensor {{Planning System}} for the {{Space Situational Awareness}} ({{SSA}}) {{Project}}},
  abstract = {Space observation has become one of the most common fields for showcasing planning and scheduling technology. This paper describes the solutions adopted in the solver of the Sensor Planning System, part of the Space Situational Awareness (SSA) Preparatory Programme of the European Space Agency (ESA). The goal of this solver is to find a feasible conflict-free assignment of a set of objects to be tracked by a set of sensors, given several constraints. We use an off-the-shelf temporal planner to solve this problem, Optic. A key aspect of this task is that problems are usually huge. Thus, current temporal planners can not solve those problems directly. We explain first how to model the problem as a temporal planning problem, and then we present an approach for partitioning the problem in order to be able to solve it that allows a multi-core processing. The experimental results, both with simulated and real data, confirm the approach is useful for solving target problems of realistic size.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3x},
  author = {d l Rosa, T. and Fuentetaja, R. and Borrajo, D. and L{\'o}pez, C. L.},
  month = sep,
  year = {2017},
  keywords = {Meteorology,Planning,scheduling,electric sensing devices,feasible conflict-free assignment,off-the-shelf temporal planner,Optical sensors,planning (artificial intelligence),scheduling technology,sensor planning,Sensor Planning System,solver,Space observation,Space Situational Awareness project,SSA,Standards,Surveillance,target problems,temporal planning,temporal planning problem},
  pages = {107-113},
  file = {/home/yuri/Zotero/storage/H2GUH652/Rosa et al. - 2017 - Sensor Planning System for the Space Situational A.pdf;/home/yuri/Zotero/storage/HMT49RIL/Rosa et al. - 2017 - Sensor Planning System for the Space Situational A.pdf;/home/yuri/Zotero/storage/EKAGGZR5/8227548.html;/home/yuri/Zotero/storage/JM5W6SNJ/8227548.html}
}
% == BibTeX quality report for rosaSensorPlanningSystem2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{garcia-piquerEvolutionaryComputationARIEL2017,
  title = {Evolutionary {{Computation}} for the {{ARIEL Mission Planning Tool}}},
  abstract = {The ARIEL mission main goal is the measurement of atmospheres of transiting planets. This requires the observation of two types of events: primary and secondary eclipses. In order to yield measurements of sufficient Signal-to-Noise Ratio to fulfill the mission objectives, the events of each exoplanet have to be observed several times. In addition, several criteria have to be considered to carry out each observation, such as the exoplanet visibility, its event duration, its potential significance in the survey, and no overlapping with other tasks. Consequently, obtaining a long term mission plan becomes unaffordable for human planners due to the complexity of computing the huge number of possible combinations for finding an optimum solution. In this contribution we present a mission planning tool based on Evolutionary Algorithms, which are focused on solving optimization problems such as the planning of several tasks. Specifically, the proposed tool finds a solution that highly optimizes the defined objectives, which are based on the maximization of the time spent on scientific observations and the scientific return. The results obtained on the large experimental set up support that the proposed scheduler technology is robust and can function in a variety of scenarios, offering a competitive performance which does not depend on the collection of exoplanets to be observed.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3w},
  author = {{Garcia-Piquer}, A. and Morales, J. C. and Colom{\'e}, J. and Ribas, I.},
  month = sep,
  year = {2017},
  keywords = {artificial satellites,Tools,Planning,scheduling,ARIEL mission main goal,ARIEL mission planning tool,Calibration,Constraint-based reasoning,event duration,Evolutionary algorithms,Evolutionary Algorithms,evolutionary computation,Evolutionary computation,exoplanet visibility,extrasolar planetary atmospheres,Extrasolar planets,genetic algorithms,human planners,long term mission plan,mission objectives,Observatory operations,optimisation,Optimization,optimization problems,optimum solution,Planets,primary eclipses,Scheduling,scientific observations,secondary eclipses,Signal-to-Noise Ratio,Space applications,transiting planets},
  pages = {101-106},
  file = {/home/yuri/Zotero/storage/6VF3YDZF/Garcia-Piquer et al. - 2017 - Evolutionary Computation for the ARIEL Mission Pla.pdf;/home/yuri/Zotero/storage/B4VEU6N7/Garcia-Piquer et al. - 2017 - Evolutionary Computation for the ARIEL Mission Pla.pdf;/home/yuri/Zotero/storage/4Z4M6JMY/8227547.html;/home/yuri/Zotero/storage/GFSK7X7R/8227547.html}
}
% == BibTeX quality report for garcia-piquerEvolutionaryComputationARIEL2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{munozDefiningMetricsAutonomous2017,
  title = {Defining {{Metrics}} for {{Autonomous Controllers Assessment}}},
  abstract = {Intelligent systems capabilities are increasing and its application to robotics has become largely popular. Some literature has been produced about autonomous controllers for robotics, demonstrating that they can face a variety of domains. However, we can observe that the experiments performed lack of a common scientific methodology. Generally, isolated case studies are presented, without providing enough details to enable other researchers to replicate or to compare their works with the previous results in the field. The aim of this work is to contribute defining and operationalizing a framework for plan-based controllers assessment. Such effort is supported on a set of generally applicable metrics to entail evaluation of different aspects of robotics controllers. Besides these metrics, we have used OGATE, a domain independent tool that automatically carries on with controller assessments. Then, following a well-defined methodology, we are able to generate reproducible benchmarks, providing objective evidences of the experiments performed. To test our framework, we have used two plan-based controllers on the same robotic problem and platform. Results show that we are able to extract relevant data, allowing comparison of different autonomous controllers.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3v},
  author = {Mu{\~n}oz, P. and Cesta, A. and Orlandini, A. and {R-Moreno}, M. D.},
  month = sep,
  year = {2017},
  keywords = {mobile robots,path planning,Planning,artificial intelligence,autonomous controllers assessment,Benchmark testing,common scientific methodology,controller assessments,defining metrics,Dispatching,domain independent tool,generally applicable metrics,intelligent systems capabilities,isolated case studies,Measurement,OGATE,plan-based controllers assessment,platform,robot programming,Robot sensing systems,robotic problem,robotics controllers},
  pages = {94-100},
  file = {/home/yuri/Zotero/storage/CG2PSANF/MuÃ±oz et al. - 2017 - Defining Metrics for Autonomous Controllers Assess.pdf;/home/yuri/Zotero/storage/KJA9DFLT/MuÃ±oz et al. - 2017 - Defining Metrics for Autonomous Controllers Assess.pdf;/home/yuri/Zotero/storage/8F2DYEH4/8227546.html;/home/yuri/Zotero/storage/J4A72XV3/8227546.html}
}
% == BibTeX quality report for munozDefiningMetricsAutonomous2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{breskvarPredictingThermalPower2017,
  title = {Predicting {{Thermal Power Consumption}} of the {{Mars Express Satellite}} with {{Machine Learning}}},
  abstract = {The thermal subsystem of the Mars Express (MEX) orbiter keeps the on-board equipment within its pre-defined operating temperatures range. To plan and optimize the scientific operations of MEX, its operators need to estimate in advance, as accurately as possible, the power consumption of the thermal subsystem. The residual power can then be allocated for scientific purposes. We present a machine learning-based pipeline for the prediction of MEX's thermal power consumption. We show that the proposed pipeline is superior in accuracy to the models currently used by MEX's operators. We also demonstrate that machine learning can provide the operators with insight about the orbiter's thermal behavior. Better understanding of the thermal subsystem and improved predictive accuracy of the thermal power consumption could help operators to improve science return and to prolong the operating life of MEX.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3t},
  author = {Breskvar, M. and Kocev, D. and Levati{\'c}, J. and Osojnik, A. and Petkovi{\'c}, M. and Simidjievski, N. and {\v Z}enko, B. and Boumghar, R. and Lucas, L.},
  month = sep,
  year = {2017},
  keywords = {artificial satellites,Space vehicles,Mars,ensembles,feature ranking,machine learning,Machine learning algorithms,mars express orbiter,Mars Express orbiter,Mars Express satellite,MEX's operators,MEX's thermal power consumption,on-board equipment,Orbits,pipeline,Pipelines,power consumption,Power demand,predictive accuracy,predictive clustering trees,Predictive models,random forests,residual power,thermal power consumption,thermal subsystem},
  pages = {88-93},
  file = {/home/yuri/Zotero/storage/FQER8MKJ/Breskvar et al. - 2017 - Predicting Thermal Power Consumption of the Mars E.pdf;/home/yuri/Zotero/storage/V9MU53KL/Breskvar et al. - 2017 - Predicting Thermal Power Consumption of the Mars E.pdf;/home/yuri/Zotero/storage/S2U7X3HP/8227545.html;/home/yuri/Zotero/storage/TGSCSNWP/8227545.html}
}
% == BibTeX quality report for breskvarPredictingThermalPower2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{lucasMachineLearningSpacecraft2017,
  title = {Machine {{Learning}} for {{Spacecraft Operations Support}} - {{The Mars Express Power Challenge}}},
  abstract = {Mars Express (MEX) has been orbiting Mars, generating great science for over 13 years. The aging spacecraft faces challenges; eclipse seasons are getting longer, eclipse durations are increasing and battery degradation is worsening. Accurate power modelling, respecting the power budget, becomes more vital as this ensures MEX health and maximum science. Empirical thermal power models provide reliable long term predictions but lack sensitivity. Telemetry data accumulated during the mission is a rich information source from which to derive a new model. This paper shows how the MEX Flight Control Team released 3 Martian years of data and reached out to Machine Learning (ML) enthusiasts asking them to predict a fourth year of spacecraft telemetry. In return, the participants were invited to an open data day to meet the MEX operators, tour ESA and find out about many exciting missions, and present and share their solutions with other candidates and ESA staff members. But the reward of solving a complex challenge, in a novel environment for space application, lead an incredible response from the worldwide ML community. Using open source solutions, candidates built data-driven models which have been able to predict the power consumption of 33 thermal lines, every hour, over a full Martian year (687 Earth days). These models improve sensitivity and accuracy by an order of magnitude. The number of scientific observations may increase even during power constrained periods when incorporating such models into MEX mission planning. By releasing spacecraft data and engaging with ML communities, ESA has gained a novel means to better exploit spacecraftresources, increase scientific return, and so, prolong mission life.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10.1109/SMC-IT.2017.21},
  author = {Lucas, L. and Boumghar, R.},
  month = sep,
  year = {2017},
  keywords = {Space vehicles,Mars,Planetary orbits,machine learning,Predictive models,ai,battery degradation,challenge,Data models,data-driven models,eclipse durations,empirical thermal power models,learning (artificial intelligence),Machine Learning,Mars Express power challenge,Martian year,MEX Flight Control Team,MEX mission planning,open data,open source solutions,power budget,Radio transmitters,space operations,space vehicles,spacecraft telemetry,telemetry,telemetry data},
  pages = {82-87},
  file = {/home/yuri/Zotero/storage/4YB79LDN/Lucas and Boumghar - 2017 - Machine Learning for Spacecraft Operations Support.pdf;/home/yuri/Zotero/storage/U2S3ZCEP/Lucas and Boumghar - 2017 - Machine Learning for Spacecraft Operations Support.pdf;/home/yuri/Zotero/storage/EI98R3YD/8227544.html;/home/yuri/Zotero/storage/PGRUB47C/8227544.html}
}
% == BibTeX quality report for lucasMachineLearningSpacecraft2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{liMachineLearningSpacecraft2017,
  title = {Machine {{Learning}} in {{Spacecraft Ground Systems}}},
  abstract = {A machine learning approach for the operational situational awareness (OSA) in flight operations is presented. The spacecraft health and safety telemetry are generally time dependent and periodical. The machine learning algorithms, such as neural networks, are used to capture the time dependent trend of the telemetry datasets characterized by their data patterns and noise level, which provides a direct insights into the health and safety status of telemetry datasets. As the time dependent trends are highly sensitive to changes above the noise level, the potential anomalies can be detected at much early stage, which leads to a more proactive flight operations and a more resilient system. The challenges for the machine-learning approach in the spacecraft ground system are to develop a systematic, accurate, adaptive and efficient data training strategy, and a representation to meet persistent requirement. The focus of this paper is to present a machine-learning approach for the time dependent trending of spacecraft datasets with arbitrary scales. The data training algorithms have been developed and implemented in neural networks, which are shown to generate highly accurate time dependent trend. The OSA tool, ASRC Intelligent Monitoring System (AIMS), is presented, which implements the machine-learning algorithm. The extensions of machine-learning algorithms in developing capabilities to improve the mission efficiency and enable more autonomous operations are discussed.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10.1109/SMC-IT.2017.20},
  author = {Li, Z.},
  month = sep,
  year = {2017},
  keywords = {Space vehicles,aerospace computing,Machine learning algorithms,Data models,learning (artificial intelligence),space vehicles,telemetry,accurate data training strategy,adaptive data training strategy,aerospace safety,Algorithm design and analysis,data analysis,data training algorithms,efficient data training strategy,Flight Operations,ground support systems,Health and safety,machine learning algorithms,machine learning approach,Machine-learning,Market research,Neural network,noise level,operational situational awareness,OSA,proactive flight operations,safety status,safety telemetry,Satellite,Situational-awareness,spacecraft datasets,spacecraft ground system,spacecraft health,systematic data training strategy,telemetry datasets,time dependent trending,Training},
  pages = {76-81},
  file = {/home/yuri/Zotero/storage/GEVZSGJP/Li - 2017 - Machine Learning in Spacecraft Ground Systems.pdf;/home/yuri/Zotero/storage/QW3NNBN8/Li - 2017 - Machine Learning in Spacecraft Ground Systems.pdf;/home/yuri/Zotero/storage/D4KGQTE2/8227543.html;/home/yuri/Zotero/storage/MR4Y76K8/8227543.html}
}
% == BibTeX quality report for liMachineLearningSpacecraft2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{fernandezTelemetryAnomalyDetection2017,
  title = {Telemetry {{Anomaly Detection System Using Machine Learning}} to {{Streamline Mission Operations}}},
  abstract = {Spacecraft housekeeping telemetry is monitored at flight control centers by the operations engineers using tools that can perform limit checking or simple trend analysis. Recent developments in machine learning techniques for anomaly detection enables the implementation of more sophisticated systems that aim to augment current state-of-the-art mission tools to provide valuable decision support for the spacecraft operators, assisting in anomaly detection and potentially saving console time for the engineers. We will show some results of the implementation of an anomaly detection tool for the NASA Mars Science Laboratory mission.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10.1109/SMC-IT.2017.19},
  author = {Fern{\'a}ndez, M. M. and Yue, Y. and Weber, R.},
  month = sep,
  year = {2017},
  keywords = {Space vehicles,aerospace computing,Tools,Mars,machine learning,learning (artificial intelligence),Machine Learning,space vehicles,telemetry,aerospace control,Anomaly detection,Anomaly Detection,flight control centers,Monitoring,NASA,NASA Mars Science Laboratory mission,space research,spacecraft housekeeping telemetry,spacecraft operators,streamline mission operations,Telemetry,telemetry anomaly detection system,valuable decision support},
  pages = {70-75},
  file = {/home/yuri/Zotero/storage/V2DXCSUF/FernÃ¡ndez et al. - 2017 - Telemetry Anomaly Detection System Using Machine L.pdf;/home/yuri/Zotero/storage/SA667JDL/8227542.html}
}
% == BibTeX quality report for fernandezTelemetryAnomalyDetection2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{beswickComputerSecurityEngineering2017,
  title = {Computer {{Security}} as an {{Engineering Practice}}: {{A System Engineering Discussion}}},
  shorttitle = {Computer {{Security}} as an {{Engineering Practice}}},
  abstract = {Design principles gathered from over 20 years of experience in research, implementing and protecting mission critical flight systems used by the Navigation and Mission Design Section at the Jet Propulsion Laboratory are reviewed. The work of spacecraft navigation involves rigorous requirements for accuracy and completeness, often carried out under uncompromising critical time pressures. Robust and fault tolerant design for the ground data system is crucial for the numerous space missions we support, from the Mars rover Curiosity to Cassini in orbit around Saturn. We begin by examining the design principles learned from fault tolerant design efforts to protect against random failure, and consider computer security engineering as a derivative effort to protect against intelligent actors promoting malicious failure. Examples for best practices of reliable system design from computer and aviation industry are considered and security fault tolerance principles are derived from this effort. Computer security design approaches are reviewed, both as abstract principles (starting from cornerstones in Confidentiality, Integrity, and Availability) and from implementation. Strategic design principles such as defense in depth, least privilege, and vulnerability removal are used in this design. We evaluate system design from external access data flows, through internal host security mechanisms, and finally to user access controls. A complementary intersection - the balance between the protection of the system and promoting its ease of use by engineers (making their experience as user friendly and efficient as possible) is evaluated. Finally, we consider future refinements to secure system architecture.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3p},
  author = {Beswick, R. M.},
  month = sep,
  year = {2017},
  keywords = {aerospace computing,Mars,aerospace safety,abstract principles,aviation industry,complementary intersection,Computer security,computer security design approaches,computer security engineering,defense in depth,derivative effort,engineering practice,external access data flows,failure analysis,fault tolerance,Fault tolerance,fault tolerant design efforts,Fault tolerant systems,ground data system,internal host security mechanisms,Jet Propulsion Laboratory,least privilege,Mars rover,mission critical flight systems,Navigation,random failure,Saturn,security fault tolerance,security fault tolerance principles,security of data,Servers,software fault tolerance,spacecraft navigation,strategic design principles,system architecture,system engineering discussion,systems engineering,uncompromising critical time pressures,vulnerability removal},
  pages = {61-69},
  file = {/home/yuri/Zotero/storage/DH6JX26Q/Beswick - 2017 - Computer Security as an Engineering Practice A Sy.pdf;/home/yuri/Zotero/storage/NITKZVXI/Beswick - 2017 - Computer Security as an Engineering Practice A Sy.pdf;/home/yuri/Zotero/storage/ALIMKD5H/8227541.html;/home/yuri/Zotero/storage/EFEJFS3W/8227541.html}
}
% == BibTeX quality report for beswickComputerSecurityEngineering2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{gillhamAttitudeControlSmall2017,
  title = {Attitude {{Control}} of {{Small Probes}} for {{De}}-{{Orbit}}, {{Descent}} and {{Surface Impact}} on {{Airless Bodies Using}} a {{Single PWM Thruster}}},
  abstract = {A single thruster attitude and de-orbital control method is proposed, capable of delivering a small spin stabilized probe with payload to the surface of an airless body such as the Moon. Nutation removal, attitude control and fast large angle maneuvers have been demonstrated and shown to be effective using a model of a commercially available single standard cold gas pulse width modulated controlled thruster. Maximum final impact angle due to drift and residual velocities was found to be less than 5 degrees and the maximum angle of attack to be 4 deg. The conventional 3-axis control would require as many as twelve thrusters a more substantial structure with complex pipe-work, and a more sophisticated controller. The single thruster concept reduces launch mass and thus cost of the mission, making the concept of small networked surface probes for extended science missions more viable. Experiments based on computer simulation have shown that strict design and mission profile requirements can be fulfilled using the single thruster control method.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3m},
  author = {Gillham, M. and Howells, G.},
  month = sep,
  year = {2017},
  keywords = {Sensors,space vehicles,3-axis control,aerospace propulsion,air-less body,attitude control,Attitude control,de-orbital control,Firing,Floors,Moon,Probes,Proposals,pulse width modulation,single PWM thruster,single thruster,single thruster attitude control,single thruster control,surface penetrator},
  pages = {50-55},
  file = {/home/yuri/Zotero/storage/5FEAAQEJ/Gillham and Howells - 2017 - Attitude Control of Small Probes for De-Orbit, Des.pdf;/home/yuri/Zotero/storage/VB4DCMGI/Gillham and Howells - 2017 - Attitude Control of Small Probes for De-Orbit, Des.pdf;/home/yuri/Zotero/storage/49E5GVZN/8227539.html;/home/yuri/Zotero/storage/XWKEBE93/8227539.html}
}
% == BibTeX quality report for gillhamAttitudeControlSmall2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{fernandezSharingTelemetryOrganizations2017,
  title = {Sharing {{Telemetry}} across {{Organizations}} and {{Systems}}},
  abstract = {The XTCE (XML Telemetric and Command Exchange) standard provides a way to describe space mission telemetry and command "databases" (or dictionaries) to be exchanged across centers and space agencies. Having a standard format for describing the telemetry and command formats allows for the development or adoption of compatible tools and significantly reduces the amount of custom software development often needed to ensure all system components have access to consistent format definitions. The main objective of this paper is to show how powerful XTCE is in terms of interoperability across organizations. This paper summarizes work which entailed converting the mission telemetry database for a current NASA mission, in XTCE format, into several target mission operation databases associated with different telemetry and command toolchains, and then, comparing the results of the telemetry processing and display. The target toolchains selected were Ball Aerospace/COSMOS, NASA-GSFC/ITOS (Goddard Space Flight Center/Integrated Test and Operations System), and NASA-AMMOS/AMPCS (Advanced Multi-Mission Operations System/Mission Data Processing and Control System) - all real-time telemetry and command processing systems.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3k},
  author = {Fernandez, M. M. and Rice, J. K. and Smith, D. and Jones, R.},
  month = sep,
  year = {2017},
  keywords = {aerospace computing,Tools,space vehicles,telemetry,NASA,Telemetry,Advanced MultiMission Operations System,Ball Aerospace,COSMOS,database management systems,Databases,Goddard Space Flight Center,Indium tin oxide,Integrated Test and Operations System,interoperability,Mission Data Processing and Control System,mission telemetry database,NASA mission,NASA-AMMOS AMPCS,NASA-GSFC ITOS,open systems,real-time telemetry processing,software development,space agencies,space mission telemetry,space telemetry,standards,Standards organizations,target mission operation databases,XML,XML Telemetric and Command Exchange,XTCE,XTCE standard},
  pages = {44-49},
  file = {/home/yuri/Zotero/storage/9L6BK2DZ/Fernandez et al. - 2017 - Sharing Telemetry across Organizations and Systems.pdf;/home/yuri/Zotero/storage/LGLZ3C23/Fernandez et al. - 2017 - Sharing Telemetry across Organizations and Systems.pdf;/home/yuri/Zotero/storage/HK2SLEH7/8227538.html;/home/yuri/Zotero/storage/Q7HXGNT6/8227538.html}
}
% == BibTeX quality report for fernandezSharingTelemetryOrganizations2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{rodriguez-garciaSolarOrbiterObserving2017,
  title = {Solar {{Orbiter Observing Plans}} for {{Understanding}} the {{Physics}} of {{Solar Energetic Particles}}: {{Definition}} and {{Simulation}}},
  shorttitle = {Solar {{Orbiter Observing Plans}} for {{Understanding}} the {{Physics}} of {{Solar Energetic Particles}}},
  abstract = {Solar Orbiter is a European Space Agency mission dedicated to Solar and Heliospheric Physics, currently planned for launch in February 2019. Due to its telemetry limitation, the mission planning should be carefully done to maximize scientific return. This work defines and simulates the scientific plan to address all sub-objectives under general Objective 3 of the Solar Orbiter mission: 'how do solar eruptions produce energetic particle radiation that fills the heliosphere?' Five new complete Solar Orbiter Observing Plans (SOOPs), for coordinating operations from several (up to 10) instruments, are defined. The SOOPs are scheduled in the Medium-Term Periods (MTPs) within the Science Activity Plan of the full mission, following specific planning strategies, which consider telemetry downlink rates and specific Sun-Earth-S/C configurations. The MTPs are coded and simulated to check when the instruments will be switched on and off and what the resource usage will be, in terms of power and telemetry generation, against the mission's constraints. Before running the code in Experiment Planning Software (EPS), the Objective 3 SOOP timelines are checked with Sous-Chef, a visualization tool for individual instrument observations and SOOPs. The results obtained prove that the process fully addresses Solar Orbiter Objective 3 and it is valid for planning and simulation. It could also be easily adapted to suit other needs in the future, like different trajectories or scientific alternatives, in order to have the best output for science and operations.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3j},
  author = {{Rodr{\'i}guez-Garc{\'i}a}, L. and Groof, A. D. and Prieto, S. S. and Walsh, A. and Williams, D. and Zouganelis, Y. and Lefort, J. and {G{\'o}mez-Herrero}, R. and {Rodr{\'i}guez-Pacheco}, J.},
  month = sep,
  year = {2017},
  keywords = {artificial satellites,Space vehicles,Planning,Orbits,telemetry,Telemetry,astronomical instruments,energetic particle radiation,EPS,European Space Agency mission,Experiment Planning Software,heliosphere,Heliospheric Physics,Instruments,Medium-Term Periods,mission planing,mission planning,Objective 3 SOOP timelines,observing plans,Science Activity Plan,scientific plan,scientific return,solar energetic particles,Solar energetic particles,solar eruptions,Solar Orbiter,Solar Orbiter mission,Solar Orbiter Objective 3,Solar Orbiter Observing Plans,solar wind,specific planning strategies,specific Sun-Earth-S/C configurations,SPICE,Sun,telemetry downlink rates,telemetry generation,telemetry limitation},
  pages = {38-43},
  file = {/home/yuri/Zotero/storage/53XW7HBF/RodrÃ­guez-GarcÃ­a et al. - 2017 - Solar Orbiter Observing Plans for Understanding th.pdf;/home/yuri/Zotero/storage/7FDVEDAQ/RodrÃ­guez-GarcÃ­a et al. - 2017 - Solar Orbiter Observing Plans for Understanding th.pdf;/home/yuri/Zotero/storage/725XF7FF/8227537.html;/home/yuri/Zotero/storage/PN5IYNY3/8227537.html}
}
% == BibTeX quality report for rodriguez-garciaSolarOrbiterObserving2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{alvarezMEDAInstrumentProcessing2017,
  title = {{{MEDA Instrument Processing}} and {{Data Management}} for the {{Mars2020 Mission}}},
  abstract = {This paper describes the MEDA instrument and its different subsystems, including the proposed data management architecture (hardware and software) supporting MEDA data acquisition, storage, processing and transmission. Mission planning and operations are based on Observation Tables that allow the instrument to operate completely autonomously during the mission (i.e. even when the rover computers are off) while retaining flexibility to accommodate additional observation opportunities that may appear during the mission, e.g. in case of areas of special interest, extra power, unused communication bandwidth, etc.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3g},
  author = {Alvarez, J. F. M. and Manfredi, J. A. R. and Diaz, C. and Merino, F. T. and Godino, A. P.},
  month = sep,
  year = {2017},
  keywords = {Space vehicles,aerospace computing,Mars,planetary rovers,Planning,Earth,Sensors,Orbits,ground data system,Instruments,mission planning,aerospace instrumentation,autonomy,computer,data acquisition,data management,data management architecture,instrument,Mars2020,Mars2020 Mission,MEDA,MEDA data acquisition,MEDA data storage,MEDA data transmission,MEDA instrument processing,observation table,Observation Tables,operations,processing,software},
  pages = {26-32},
  file = {/home/yuri/Zotero/storage/5QU9GWS7/Alvarez et al. - 2017 - MEDA Instrument Processing and Data Management for.pdf;/home/yuri/Zotero/storage/VLGAUC4U/Alvarez et al. - 2017 - MEDA Instrument Processing and Data Management for.pdf;/home/yuri/Zotero/storage/G6FJCDN8/8227535.html;/home/yuri/Zotero/storage/GXFXD3VT/8227535.html}
}
% == BibTeX quality report for alvarezMEDAInstrumentProcessing2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{segura-murosIntegrationAutomatedHierarchical2017,
  title = {Integration of an {{Automated Hierarchical Task Planner}} in {{ROS Using Behaviour Trees}}},
  abstract = {In this paper we present an architecture that embeds an automatic task planner into the Robotic Operating System (ROS). The plans generated by the planner are automatically converted into executable behaviour trees. These behaviour trees are used as the basis of an execution monitoring process that is able to detect when the plan fails, trying to repair it or triggering a replanning process. This makes our architecture able to adapt to unexpected changes in the world, modifying the robot's behaviour in a reactive way while trying to achieve a given goal. We have tested our approach in a simulated scenario.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm3f},
  author = {{Segura-Muros}, J. {\'A} and {Fern{\'a}ndez-Olivares}, J.},
  month = sep,
  year = {2017},
  keywords = {control engineering computing,mobile robots,path planning,Planning,Computer architecture,operating systems (computers),robot programming,automated hierarchical task planner,Automated Task Planning,Behaviour Trees,Blackboard-Based Architecture,Compounds,executable behaviour trees,execution monitoring process,HTN Planning,Peer-to-peer computing,replanning process,robot behaviour,Robot kinematics,Robotic Operating System,Robotics,ROS,Task Execution,trees (mathematics)},
  pages = {20-25},
  file = {/home/yuri/Zotero/storage/GV9NEJRW/Segura-Muros and FernÃ¡ndez-Olivares - 2017 - Integration of an Automated Hierarchical Task Plan.pdf;/home/yuri/Zotero/storage/XL5E3TGP/Segura-Muros and FernÃ¡ndez-Olivares - 2017 - Integration of an Automated Hierarchical Task Plan.pdf;/home/yuri/Zotero/storage/C8INT2WY/8227534.html;/home/yuri/Zotero/storage/LDDRDWVI/8227534.html}
}
% == BibTeX quality report for segura-murosIntegrationAutomatedHierarchical2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{wyattNewCapabilitiesDeep2017,
  title = {New {{Capabilities}} for {{Deep Space Robotic Exploration Enabled}} by {{Disruption Tolerant Networking}}},
  abstract = {For much of the past decade NASA and other organizations have been developing the Disruption Tolerant Networking protocol suite with a primary focus on bringing internet-like functionality to space missions. Now that the core protocols are complete, engineers are beginning to incorporate DTN into mission system designs. When we look across the NASA mission needs over the next decade, there are multiple ways DTN can work in concert with end-to-end mission system software to enhance mission capabilities or even enable new types of missions. DTN-enabled space systems can mitigate data loss when radio links are interrupted, improve the efficiency of operations by simplifying downlink planning, automate relay operations and enable coordinated operations among multiple spacecraft. This paper describes key elements of the protocol suite and software applications along with the mission capabilities they can enable.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10.1109/SMC-IT.2017.8},
  author = {Wyatt, E. J. and Belov, K. and Burleigh, S. and {Castillo-Rogez}, J. and Chien, S. and Clare, L. and Lazio, J.},
  month = sep,
  year = {2017},
  keywords = {Space vehicles,protocols,Protocols,Software,space missions,Internet,NASA,NASA mission,Space missions,autonomy,aerospace robotics,coordinated observations,deep space robotic exploration,Disruption tolerant networking,Disruption Tolerant Networking,Downlink,DTN,end-to-end mission system software,mission architecture,Relays,space networking,space systems},
  pages = {1-6},
  file = {/home/yuri/Zotero/storage/5SK7SRPX/Wyatt et al. - 2017 - New Capabilities for Deep Space Robotic Exploratio.pdf;/home/yuri/Zotero/storage/YU9RCGR9/Wyatt et al. - 2017 - New Capabilities for Deep Space Robotic Exploratio.pdf;/home/yuri/Zotero/storage/76KGQMS7/8227531.html;/home/yuri/Zotero/storage/P2DA6GVR/8227531.html}
}
% == BibTeX quality report for wyattNewCapabilitiesDeep2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{berlatiImplementationCGRONE2017,
  title = {Implementation of ({{O}}-){{CGR}} in {{The ONE}}},
  abstract = {Routing in Delay-/Disruption-Tolerant Networking (DTN) requires specific solutions as link impairments prevent the use of ordinary Internet algorithms, based on a timely dissemination of network topology information. Among DTN routing algorithms there is a dichotomy between opportunistic and deterministic (scheduled) solutions. The former are numerous and apply to terrestrial environments; CGR is the most widely supported algorithm designed for scheduled connectivity, and it is usually applied to space networks. However, in an attempt to provide a unified approach, an opportunistic variant of CGR, Opportunistic CGR (OCGR) has been recently proposed by some of the authors. Performance evaluations are normally carried out for opportunistic solutions by means of simulators, such as The ONE considered in this paper. CGR by contrast is more often studied by means of small testbeds. As the simulation approach could be complementary for CGR, and essential for OCGR, the authors have recently ported both of them into The ONE, by developing and releasing as free software a specific additional package. The aim of this paper is to show the rationale of this choice and discuss the many challenges that needed to be tackled to achieve this primary goal.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm4d},
  author = {Berlati, A. and Burleigh, S. and Caini, C. and Fiorini, F. and Messina, J. J. and Pozza, S. and Rodolfi, M. and Tempesta, G.},
  month = sep,
  year = {2017},
  keywords = {delay tolerant networks,Computer architecture,Internet,Routing,telecommunication network routing,Ions,telecommunication network topology,Algorithm design and analysis,DTN,CGR,component,Delay-/Disruption- Tolerant Networking,Delay-/Disruption-Tolerant Networking,Internet algorithms,Java,network topology information,OCGR,Routing protocols,space networks,Space Networks,telecommunication scheduling,The ONE},
  pages = {132-135},
  file = {/home/yuri/Zotero/storage/JV539LT9/Berlati et al. - 2017 - Implementation of (O-)CGR in The ONE.pdf;/home/yuri/Zotero/storage/9XKGMCRX/8227553.html}
}
% == BibTeX quality report for berlatiImplementationCGRONE2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{jonckereSimulationEnvironmentNetwork2017,
  title = {Simulation {{Environment}} for {{Network Coding Research}} in {{Ring Road Networks}}},
  abstract = {The so-called Ring Road approach has been identified as interesting application area of DTN protocols. It leverages low-cost low-earth orbit satellites in order to inter-connect isolated networks with the Internet. Network coding may be used to introduce redundancy to address e.g. lossy links. In this paper, a simulator setup for network coding in Ring Road networks is presented and discussed. The simulator is based on The ONE and is embedded into an overall toolchain rendering very efficient analysis of different configurations possible.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm4c},
  author = {Jonck{\`e}re, O. D. and Chorin, J. and Feldmann, M.},
  month = sep,
  year = {2017},
  keywords = {artificial satellites,Satellites,delay tolerant networks,Roads,satellite communication,Internet,Routing,routing protocols,Computational modeling,DTN protocols,Encoding,isolated networks,Libraries,low-earth orbit satellites,network coding,Network coding,network coding research,Ring Road approach,Ring Road networks,simulation environment,simulator setup,toolchain},
  pages = {128-131},
  file = {/home/yuri/Zotero/storage/DNX9JN2E/JonckÃ¨re et al. - 2017 - Simulation Environment for Network Coding Research.pdf;/home/yuri/Zotero/storage/FEUKYWNR/8227552.html}
}
% == BibTeX quality report for jonckereSimulationEnvironmentNetwork2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{garridoAnisotropicFastMarching2017,
  title = {Towards an {{Anisotropic Fast Marching Method Applied}} to the {{Path Planning Task}} for {{Mars Rovers}}},
  abstract = {This paper presents the application of the Anisotropic Fast Marching Method to the path planning problem of robots moving in spatial environments. The slope of a terrain should be considered in a tensorial way because at any point on a mountainside there are two main slopes: the maximum, which is the slope of the gradient, and the minimum, which is the perpendicular slope to the gradient. The resulting trajectory of the path planning should take both into account so that the slopes in the trajectory are minimized, just as the Romans used flocks of cows to make Roman roads and so that the descent of the road was small as possible.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm4b},
  author = {Garrido, S. and Martin, F. and Alvarez, D. and Moreno, L.},
  month = sep,
  year = {2017},
  keywords = {Mars,mobile robots,path planning,planetary rovers,Robots,Measurement,Anisotropic Fast Marching Method,Anisotropic magnetoresistance,fast marching,Indexes,Mars Rovers,path planning task,perpendicular slope,spatial environments,Tensile stress,terrain slope,trajectory,Trajectory,trajectory control},
  pages = {13-19},
  file = {/home/yuri/Zotero/storage/HKBAQ67P/Garrido et al. - 2017 - Towards an Anisotropic Fast Marching Method Applie.pdf;/home/yuri/Zotero/storage/ZLZVU3JP/8227533.html}
}
% == BibTeX quality report for garridoAnisotropicFastMarching2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{hernandezAddingUncertaintyObject2017,
  title = {Adding {{Uncertainty}} to an {{Object Detection System}} for {{Mobile Robots}}},
  abstract = {Autonomous mobile robots use vision sensors for navigation due to its ability to provide detailed information of the environment. Visual perception of an environment, is an important capability for mobile robots, so many efforts are leading the research community for such a fundamental and challenging task. An aspect to emphasize is that vision systems have to cope with uncertainty because sensors have noise and the previous knowledge is unclear or inaccurate. In this paper, we propose an uncertainty estimation model applied to an object detection system. The vision system is designed to recognize objects in usual human environments, working on a mobile robot. To calculate the uncertainty, we consider the model accuracy of the system, the probability of detection after the prediction process and the empirical probability of detecting each object according to the distance. The experimental results demonstrate the feasibility and usefulness of incorporating uncertainty information into an object detection system. Finally, the results motivate us to continue improving the uncertainty model in order to use the information generated to strengthen mobile robot navigation systems, as well as for the development of a place categorization system.},
  booktitle = {2017 6th {{International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}})},
  doi = {10/gfzm39},
  author = {Hern{\'a}ndez, A. C. and G{\'o}mez, C. and Crespo, J. and Barber, R.},
  month = sep,
  year = {2017},
  keywords = {mobile robots,Sensors,Navigation,autonomous mobile robots,classification,Feature extraction,mobile robot navigation systems,Mobile robots,navigation,object detection,object detection system,Object recognition,place categorization system,probability,robot vision,uncertainty,Uncertainty,uncertainty estimation model,uncertainty model,vision sensors,vision system},
  pages = {7-12},
  file = {/home/yuri/Zotero/storage/7RU7AKUM/HernÃ¡ndez et al. - 2017 - Adding Uncertainty to an Object Detection System f.pdf;/home/yuri/Zotero/storage/SFAQ67TR/8227532.html}
}
% == BibTeX quality report for hernandezAddingUncertaintyObject2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@book{wickhamDbplyrDplyrBack2019,
  title = {Dbplyr: {{A}} 'dplyr' {{Back End}} for {{Databases}}},
  author = {Wickham, Hadley and Ruiz, Edgar},
  year = {2019},
  keywords = {\#nosource}
}
% == BibTeX quality report for wickhamDbplyrDplyrBack2019:
% Missing required field 'publisher'

@inproceedings{yvernesCopernicusGroundSegment2018,
  address = {{Marseille, France}},
  title = {Copernicus {{Ground Segment}} as a {{Service}}: {{From Data Monitoring}} to {{Performance Analysis}}},
  isbn = {978-1-62410-562-3},
  shorttitle = {Copernicus {{Ground Segment}} as a {{Service}}},
  language = {en},
  booktitle = {15th {{International Conference}} on {{Space Operations}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2018-2317},
  author = {Yvernes, Aymeric},
  month = may,
  year = {2018},
  file = {/home/yuri/Zotero/storage/47U3FEJF/Yvernes - 2018 - Copernicus Ground Segment as a Service From Data .pdf}
}
% == BibTeX quality report for yvernesCopernicusGroundSegment2018:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{hyvonenDevelopmentInfinityService2018,
  address = {{Marseille, France}},
  title = {Development of the {{Infinity Service}}, a Data-Centric Ground Network Service with High Capacity for Small Satellites and Large Constellations},
  isbn = {978-1-62410-562-3},
  language = {en},
  booktitle = {15th {{International Conference}} on {{Space Operations}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2018-2501},
  author = {Hyv{\"o}nen, Petrus and Vidmark, Axel and Andersson, Michael and Liljeblad, Michael and Alvarez, Juan and Massey, Dave},
  month = may,
  year = {2018},
  file = {/home/yuri/Zotero/storage/8LBU7ZEC/HyvÃ¶nen et al. - 2018 - Development of the Infinity Service, a data-centri.pdf}
}
% == BibTeX quality report for hyvonenDevelopmentInfinityService2018:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle

@inproceedings{schulsterCHARTingFutureOffline2018,
  address = {{Marseille, France}},
  title = {{{CHARTing}} the {{Future}} \textendash{} {{An}} Offline Data Analysis and Reporting Toolkit to Support Automated Decision-Making in Flight-Operations},
  isbn = {978-1-62410-562-3},
  language = {en},
  booktitle = {15th {{International Conference}} on {{Space Operations}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2018-2637},
  author = {Schulster, Jonathan and Evill, Ry and Phillips, Stephan and Feldmann, Nico and Rogissart, Julien and Dyer, Richard and Argemandy, Alessandro},
  month = may,
  year = {2018},
  file = {/home/yuri/Zotero/storage/A3DGY53T/Schulster et al. - 2018 - CHARTing the Future â An offline data analysis and.pdf}
}
% == BibTeX quality report for schulsterCHARTingFutureOffline2018:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle

@inproceedings{edwardsDealingBigData2018,
  address = {{Marseille, France}},
  title = {Dealing with the {{Big Data}} - {{The Challenges}} for {{Modern Mission Monitoring}} and {{Reporting}}},
  isbn = {978-1-62410-562-3},
  language = {en},
  booktitle = {15th {{International Conference}} on {{Space Operations}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2018-2507},
  author = {Edwards, Tristan},
  month = may,
  year = {2018},
  file = {/home/yuri/Zotero/storage/PQSH5DHI/Edwards - 2018 - Dealing with the Big Data - The Challenges for Mod.pdf;/home/yuri/Zotero/storage/L5CJSYS4/6.html}
}
% == BibTeX quality report for edwardsDealingBigData2018:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{trollopeAnalysisAutomatedTechniques2018,
  address = {{Marseille, France}},
  title = {Analysis of Automated Techniques for Routine Monitoring and Contingency Detection of In-Flight {{LEO}} Operations at {{EUMETSAT}}},
  isbn = {978-1-62410-562-3},
  language = {en},
  booktitle = {2018 {{SpaceOps Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2018-2532},
  author = {Trollope, Ed and Dyer, Richard and Francisco, Tiago and Miller, James and Pagan Griso, Mauro and Argemandy, Alessandro},
  month = may,
  year = {2018},
  file = {/home/yuri/Zotero/storage/B5SEISNH/Trollope et al. - 2018 - Analysis of automated techniques for routine monit.pdf}
}
% == BibTeX quality report for trollopeAnalysisAutomatedTechniques2018:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle

@inproceedings{rahmanInterplanetaryNetworkBrief2014,
  title = {Interplanetary Network: {{A}} Brief Introduction},
  shorttitle = {Interplanetary Network},
  abstract = {Interplanetary Network defines the architecture and protocols necessary to permit inter-operation of the Internet residents or systems on earth/spacecrafts with other remotely located Internet resident/systems on other systems/spacecrafts in transit in the hostile and unpredictable environment of space. Inter-networking in such environment requires new techniques other than traditional communication protocols. This paper focuses on the understanding of the current system and protocols used in Interplanetary communication. This paper is mainly based on the study of different layers of CCSDS (Consultative Committee for Space Data Systems) protocol, which is the recommended protocol used by all the Space Communication Organizations including NASA. The resources used for this paper are mainly reports from CCSDS, NASA and JPL. This paper will pave the easier way of studying and understanding of Interplanetary Network. In the CCSDS protocol Stack, Security is shown as a protocol layer. But, the reality is, every layer has its own security concerns. In this paper, we have tried to reflect this idea too.},
  booktitle = {16th {{Int}}'l {{Conf}}. {{Computer}} and {{Information Technology}}},
  doi = {10.1109/ICCITechn.2014.6997385},
  author = {Rahman, K. M. S. and Islam, M. M. and Kabir, M. H.},
  month = mar,
  year = {2014},
  keywords = {Space vehicles,Protocols,Internet,space communication links,Standards,NASA,security of data,Space missions,Aerospace electronics,CCSDS protocol,communication protocols,Consultative Committee for Space Data Systems protocol,internetworking,interplanetary communication,interplanetary network,Interplanetary Network,JPL,protocol,security,Security,Space Communication Organizations,spacecrafts,transport protocols},
  pages = {261-266},
  file = {/home/yuri/Zotero/storage/SCUG9M9P/Rahman et al. - 2014 - Interplanetary network A brief introduction.pdf;/home/yuri/Zotero/storage/JQZBNMZQ/6997385.html}
}
% == BibTeX quality report for rahmanInterplanetaryNetworkBrief2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{giulianiBuildingEarthObservations2017,
  title = {Building an {{Earth Observations Data Cube}}: Lessons Learned from the {{Swiss Data Cube}} ({{SDC}}) on Generating {{Analysis Ready Data}} ({{ARD}})},
  volume = {1},
  issn = {2096-4471},
  shorttitle = {Building an {{Earth Observations Data Cube}}},
  abstract = {Pressures on natural resources are increasing and a number of challenges need to be overcome to meet the needs of a growing population in a period of environmental variability. Some of these environmental issues can be monitored using remotely sensed Earth Observations (EO) data that are increasingly available from a number of freely and openly accessible repositories. However, the full information potential of EO data has not been yet realized. They remain still underutilized mainly because of their complexity, increasing volume, and the lack of efficient processing capabilities. EO Data Cubes (DC) are a new paradigm aiming to realize the full potential of EO data by lowering the barriers caused by these Big data challenges and providing access to large spatio-temporal data in an analysis ready form. Systematic and regular provision of Analysis Ready Data (ARD) will significantly reduce the burden on EO data users. Nevertheless, ARD are not commonly produced by data providers and therefore getting uniform and consistent ARD remains a challenging task. This paper presents an approach to enable rapid data access and pre-processing to generate ARD using interoperable services chains. The approach has been tested and validated generating Landsat ARD while building the Swiss Data Cube.},
  number = {1-2},
  journal = {Big Earth Data},
  doi = {10/gdz3kz},
  author = {Giuliani, Gregory and Chatenoux, Bruno and Bono, Andrea De and Rodila, Denisa and Richard, Jean-Philippe and Allenbach, Karin and Dao, Hy and Peduzzi, Pascal},
  month = dec,
  year = {2017},
  keywords = {Analysis Ready Data,automatic processing,Data Cube,Earth Observations,Landsat},
  pages = {100-117},
  file = {/home/yuri/Zotero/storage/J76746J2/Giuliani et al. - 2017 - Building an Earth Observations Data Cube lessons .pdf;/home/yuri/Zotero/storage/JP9SS22B/20964471.2017.html}
}

@phdthesis{AzevedoAmbrViei::EsSoTe,
  address = {{S{\~a}o Jos{\'e} dos Campos}},
  title = {{Estudo sobre t{\'e}cnicas de detec{\c c}{\~a}o autom{\'a}tica de anomalias em sat{\'e}lites}},
  language = {pt},
  school = {Instituto Nacional de Pesquisas Espaciais},
  author = {Azevedo, Denise Nunes Rotondi and Ambr{\'o}sio, Ana Maria and Vieira, Marco},
  year = {2011},
  keywords = {detecÃ§Ã£o automÃ¡tica de anomalias,mineraÃ§Ã£o de dados.},
  file = {/home/yuri/Zotero/storage/VDDHSJ9D/AmbrÃ³sio et al. - 2011 - ESTUDO SOBRE TÃCNICAS DE DETECÃÃO AUTOMÃTICA DE AN.pdf},
  targetfile = {DeteccaoDeAnomaliasRT.pdf}
}

@incollection{OrlandoKuga:2007:SaSCSC,
  address = {{S{\~a}o Paulo}},
  edition = {cap. 5},
  title = {Os Sat{\'e}lites {{SCD1}} e {{SCD2}} Da {{Miss{\~a}o Espacial Completa Brasileira}} - {{MECB}}},
  isbn = {978-85-88325-89-0},
  abstract = {Os sat{\'e}lites SCD1 e SCD2 (Sat{\'e}lite de Coleta de Dados 1  e 2) foram de extrema import{\^a}ncia para o programa espacial  brasileiro. Neste ca{\'p}\i{}tulo, apresentase uma  narra{\c c}{\~a}o dos fatos e eventos relevantes relacionados a  esses dois sat{\'e}lites, dois marcos importantes do envolvimento  do Brasil na {\'a}rea espacial por serem, nada menos, que o  primeiro e segundo sat{\'e}lites brasileiros totalmente  concebidos, projetados, desenvolvidos e operados em {\'o}rbita  pelo p{\'a}\i{}s.},
  language = {English},
  booktitle = {A {{Conquista}} Do {{Espa{\c c}o}}: Do {{Sputnik}} {\`a} {{Miss{\~a}o Centen{\'a}rio}}},
  publisher = {{Editora Livraria da F\'\i{}sica}},
  author = {Orlando, Valcir and Kuga, H{\'e}lio Kotti},
  editor = {{Othon Cabo Winter} and Prado, Antonio Fernando Bertachini de Almeida},
  year = {2007},
  keywords = {\#nosource},
  pages = {.},
  affiliation = {Instituto Nacional de Pesquisas Espaciais (INPE)},
  targetfile = {Capitulo-5.pdf}
}

@incollection{OrlandoKuga:2007:RaCoSa,
  address = {{S{\~a}o Paulo}},
  edition = {cap. 6},
  title = {Rastreio e Controle de Sat{\'e}lites Do {{INPE}}},
  isbn = {978-85-88325-89-0},
  abstract = {Neste ca{\'p}\i{}tulo ser{\~a}o focalizadas as atividades de  rastreio e controle de sat{\'e}lites do INPE. Ser{\~a}o  apresentadas ainda a organiza{\c c}{\~a}o formal adotada e a  infra- estrutura de solo desenvolvida para a  realiza{\c c}{\~a}o dessas atividades, tanto em termos de  hardware e software, quanto de recursos humanos.},
  language = {English},
  booktitle = {A {{Conquista}} Do {{Espa{\c c}o}}: Do {{Sputnik}} {\`a} {{Miss{\~a}o Centen{\'a}rio}}},
  publisher = {{Editora Livraria da F\'\i{}sica}},
  author = {Orlando, Valcir and Kuga, H{\'e}lio Kotti},
  editor = {{Othon Cabo Winter} and Prado, Antonio Fernando Bertachini de Almeida},
  year = {2007},
  keywords = {\#nosource},
  pages = {.},
  affiliation = {Instituto Nacional de Pesquisas Espaciais (INPE)},
  targetfile = {Capitulo-6.pdf}
}

@inproceedings{yilongResearchForestVisualization2012,
  title = {Research on Forest Visualization System Based on Data Cube},
  volume = {2},
  abstract = {This paper analyzes the Forest visual modelling content and basic technical characteristics. For people having higher requirements of forest visualization, we proposed a visualization prototype system of forestry based on the concept of data cube. The prototype system provides forestry operator with the service of browsing forestry data and various management plans. Based on the concept of data cube, the entire forest is a data cube, each individual tree as a basic data cube. By using the data cube design by us, we could construct the virtual forest on different scale using less computer memory. With the help of the visualization system, we can compute the spatial construction information of the forest and adjust the spatial structure, providing a basic platform and virtual environment for other researchers to study spatial structure of forest.},
  booktitle = {2012 {{IEEE International Conference}} on {{Computer Science}} and {{Automation Engineering}} ({{CSAE}})},
  doi = {10.1109/CSAE.2012.6272879},
  author = {Yilong, K. and Junsnan, T. and Huaiqmg, Z. and Xian, J. and Kangnmg, L. and Ning, Z.},
  month = may,
  year = {2012},
  keywords = {virtual reality,Data models,Biological system modeling,data cube,data cube design,data visualisation,Data visualization,forest management,forest management plan,forest visual modelling,forest visualization,forestry,Forestry,forestry data browsing,forestry operator,Load modeling,spatial structure,tree,tree model based on branch structure,Vegetation,virtual forest,Visualization,visualization prototype system},
  pages = {769-773},
  file = {/home/yuri/Zotero/storage/7MCDKGID/Yilong et al. - 2012 - Research on forest visualization system based on d.pdf}
}
% == BibTeX quality report for yilongResearchForestVisualization2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{leeMRDataCubeDataCube2015a,
  title = {{{MRDataCube}}: {{Data}} Cube Computation Using {{MapReduce}}},
  shorttitle = {{{MRDataCube}}},
  abstract = {Data cube is used as an OLAP (On-Line Analytical Processing) model to implement multidimensional analyses in many fields of application. Computing a data cube requires a long sequence of basic operations and storage costs. Exponentially accumulating amounts of data have reached a magnitude that overwhelms the processing capacities of single computers. In this paper, we implement a large-scale data cube computation based on distributed parallel computing using the MapReduce (MR) computational framework. For this purpose, we developed a new algorithm, MRDataCube, which incorporates the MR mechanism into data cube computations such that effective data cube computations are enabled even when using the same computing resources. The proposed MRDataCube consists of two-level MR phases, namely, MRSpread and MRAssemble. The main feature of this algorithm is a continuous data reduction through the combination of partial cuboids and partial cells that are emitted when the computation undergoes these two phases. From the experimental results we revealed that MRDataCube outperforms all other algorithms.},
  booktitle = {2015 {{International Conference}} on {{Big Data}} and {{Smart Computing}} ({{BIGCOMP}})},
  doi = {10.1109/35021BIGCOMP.2015.7072817},
  author = {Lee, S. and Jo, S. and Kim, J.},
  month = feb,
  year = {2015},
  keywords = {Data models,data mining,OLAP,MapReduce,Aggregates,Arrays,continuous data reduction,cube,data cube computation,data reduction,Distributed databases,distributed parallel algorithm,distributed parallel computing,Hadoop,large-scale data cube computation,Manganese,MapReduce computational framework,MRAssemble,MRDataCube,MRSpread,multi-dimensional analysis,multidimensional analyses,online analytical processing,parallel processing,Parallel processing,partial cells,partial cuboids,two-level MR phases},
  pages = {95-102},
  file = {/home/yuri/Zotero/storage/PP2DWR68/Lee et al. - MRDataCube Data Cube Computation Using MapReduce.pdf;/home/yuri/Zotero/storage/R2GILE9E/7072817.html}
}
% == BibTeX quality report for leeMRDataCubeDataCube2015a:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{abouzeidHadoopDBArchitecturalHybrid2009,
  title = {{{HadoopDB}}: An Architectural Hybrid of {{MapReduce}} and {{DBMS}} Technologies for Analytical Workloads},
  volume = {2},
  issn = {21508097},
  shorttitle = {{{HadoopDB}}},
  abstract = {The production environment for analytical data management applications is rapidly changing. Many enterprises are shifting away from deploying their analytical databases on high-end proprietary machines, and moving towards cheaper, lower-end, commodity hardware, typically arranged in a shared-nothing MPP architecture, often in a virtualized environment inside public or private ``clouds''. At the same time, the amount of data that needs to be analyzed is exploding, requiring hundreds to thousands of machines to work in parallel to perform the analysis.},
  language = {en},
  number = {1},
  journal = {Proceedings of the VLDB Endowment},
  doi = {10/gdxrgg},
  author = {Abouzeid, Azza and {Bajda-Pawlikowski}, Kamil and Abadi, Daniel and Silberschatz, Avi and Rasin, Alexander},
  month = aug,
  year = {2009},
  pages = {922-933},
  file = {/home/yuri/Zotero/storage/ZACMF523/Abouzeid et al. - 2009 - HadoopDB an architectural hybrid of MapReduce and.pdf}
}

@article{gusevModelingLowaltitudeQuasitrapped2003,
  title = {Modeling of {{Low}}-Altitude {{Quasi}}-Trapped {{Proton Fluxes}} at the {{Equatorial Inner Magnetosphere}}},
  volume = {33},
  language = {en},
  number = {4},
  journal = {Brazilian Journal of Physics},
  doi = {10/bcv7qh},
  author = {Gusev, A A and Pugacheva, G I and Jayanthi, U B and Schuch, N},
  year = {2003},
  pages = {7},
  file = {/home/yuri/Zotero/storage/38WLEK2P/Gusev et al. - 2003 - Modeling of Low-altitude Quasi-trapped Proton Flux.pdf}
}

@inproceedings{pattersonRadiationEffectsMicroelectronics2003,
  address = {{San Diego, CA, United States}},
  title = {Radiation Effects on Microelectronics and Future Space Missions},
  abstract = {This paper briefly reviews the three basic radiation effect mechanisms, and how they interrupt the functionality of currently available non-volatile memory technologies. This paper also  presents a very general overview of the radiation environments expected in future space exploration missions. Unfortunately, these environments will be very harsh, from a radiation  standpoint, and thus a significant effort is required to develop non-volatile technologies that will meet future mission requirements.},
  booktitle = {2003 {{Non}}-Volatile {{Memory Technology Symposium}}},
  author = {Patterson, Jeffrey D.},
  month = nov,
  year = {2003},
  keywords = {space missions,mission planning,general overviews,microelectronics,radiation effects,space exploration},
  file = {/home/yuri/Zotero/storage/V62P59FV/Patterson_2003_Radiation effects on microelectronics and future space missions.pdf;/home/yuri/Zotero/storage/BMYPWIGD/search.html}
}
% == BibTeX quality report for pattersonRadiationEffectsMicroelectronics2003:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@misc{toddRadiationRisksMitigation2015,
  title = {Radiation {{Risks}} and {{Mitigation}} in {{Electronic Systems}}},
  abstract = {Electrical and electronic systems can be disturbed by radiation-induced effects. In some cases, radiation-induced effects are of a low probability and can be ignored; however, radiation effects must be considered when designing systems that have a high mean time to failure requirement, an impact on protection, and/or higher exposure to radiation. High-energy physics power systems suffer from a combination of these effects: a high mean time to failure is required, failure can impact on protection, and the proximity of systems to accelerators increases the likelihood of radiationinduced events. This paper presents the principal radiation-induced effects, and radiation environments typical to high-energy physics. It outlines a procedure for designing and validating radiation-tolerant systems using commercial off-the-shelf components. The paper ends with a worked example of radiation-tolerant power converter controls that are being developed for the Large Hadron Collider and High Luminosity-Large Hadron Collider at CERN.},
  language = {en},
  publisher = {{CERN}},
  author = {Todd, B and Uznanski, S},
  year = {2015},
  file = {/home/yuri/Zotero/storage/RWBBDLJI/Todd and Uznanski - 2015 - Radiation Risks and Mitigation in Electronic Syste.pdf},
  doi = {10.5170/cern-2015-003.245}
}
% == BibTeX quality report for toddRadiationRisksMitigation2015:
% Missing required field 'howpublished'
% ? Title looks like it was stored in title-case in Zotero

@article{srourRadiationEffectsMicroelectronics1988,
  title = {Radiation Effects on Microelectronics in Space},
  volume = {76},
  issn = {0018-9219},
  abstract = {The basic mechanisms of space radiation effects on microelectronics are reviewed. Topics discussed include the effects of displacement damage and ionizing radiation on devices and circuits, single-event phenomena, dose enhancement, radiation effects on optoelectronic devices and passive components, hardening approaches, and simulation of the space radiation environment. A summary of damage mechanisms that can cause temporary or permanent failure of devices and circuits operating in space is presented.{$<$}{$>$}},
  number = {11},
  journal = {Proceedings of the IEEE},
  doi = {10/c7wvn8},
  author = {Srour, J. R. and McGarrity, J. M.},
  month = nov,
  year = {1988},
  keywords = {Radiation effects,radiation hardening (electronics),failure analysis,microelectronics,radiation effects,Atomic measurements,basic mechanisms,Circuit simulation,Circuit testing,circuits,damage mechanisms,devices,Discrete event simulation,dose enhancement,effects of displacement damage,Extraterrestrial phenomena,hardening approaches,integrated circuit technology,ionizing radiation,Ionizing radiation,Microelectronics,optoelectronic devices,Optoelectronic devices,passive components,permanent failure of devices,Radiation hardening,semiconductor technology,simulation,single-event phenomena,space radiation effects,space radiation environment,temporary failure of devices},
  pages = {1443-1469},
  file = {/home/yuri/Zotero/storage/T2MLZBZN/90114.html}
}

@phdthesis{silva:2015:abordagensParaCubo,
  address = {{S{\~a}o Jos{\'e} dos Campos}},
  title = {{Abordagens para Cubo de Dados Massivos com Alta Dimensionalidade Baseadas em Mem{\'o}ria Principal e Mem{\'o}ria Externa: HIC e BCubing}},
  language = {pt},
  school = {Instituto Tecnol{\'o}gico de Aeron{\'a}utica},
  author = {Silva, Rodrigo Rocha},
  year = {2015},
  file = {/home/yuri/Zotero/storage/C8VGPT8B/Silva_2015_Abordagens para Cubo de Dados Massivos com Alta Dimensionalidade Baseadas em.pdf}
}

@inproceedings{JulioFoAmbrFerrLour:2017:ChImSp,
  title = {The {{Amazonia}}-1 Satellite's Ground Segment - Challenges for Implementation of the Space Link Extension Protocol Services},
  abstract = {Amazonia-1 is the first Remote Sensing Satellite entirely  developed at Brazil by National Institute for Space Research  (INPE) and it is expected to be concluded in 2018. Amazonia-1 is a  polar orbit satellite that will generate images with a 5 days  revisit period. To do this, has a wide sight optical imager,  called Wide Field Imager (WFI), able to observe a range of 700 km  with 70 meters of spatial resolution. Its rapid revisit feature  will enable Amazon deforestation alert data to improve in real  time by maximizing the acquisition of useful images in the face of  cloud cover in the region. The Amazonia-1 will also provide  frequent images of Brazilian areas, and may be useful in other  environmental monitoring applications, such as the coastal zone,  water reservoirs, forests of other biomes and natural disasters.  The Amazonia-1 satellite is based on the Multi-Mission Platform  (MMP), which was also developed by INPE and other Brazilian  industries as a part of the National Program of Space Activities  (PNAE), coordinated by the Brazilian Space Agency (AEB). The MMP  is generic platform to 500 kg class satellites. With 250 kg mass,  it provides the necessary resources, in terms of power, control,  communication and others to operate, in orbit, a payload of up to  280 kg. The requirement of high rate of revisits and the need of  controlling and data reception of other remote sensing satellites  available in Brazil, for example, the CBERS (26 days to revisit)  impose new challenges for the ground segment, related to control  system of orbit and attitude and consequently in the reception,  processing and distribution of data through the ground segment  using the Space Link Extension (SLE) Protocol Services. The SLE  protocol services establish activities, based on the Consultative  Committee for Space Data Systems (CCSDS) for cross support  recommendations, including Management Services for Data Transfer  and SLE protocol services related to Telemetry and Telecommand.  These services standards have been adopted by the different space  agencies, such as: ESA, NASA, CNES, DLR, ASI, JAXA, INPE, to  performing tracking and controlling of the spacecrafts. This paper  presents an overview of the Amazonia-1 satellites ground segment,  its objectives, the satellite design based in the Multi-Mission  Platform (MMP) and also the preparation of the ground segment for  the operation with the SLE Protocol and allow for efficient  operations with cross support.},
  language = {English},
  booktitle = {Proceedings...},
  author = {Julio Filho, Antonio Cassiano and Ambr{\'o}sio, Ana Maria and Ferreira, Mau{\'r}\i{}cio Gon{\c c}alves Vieira and Loureiro, Geilson},
  year = {2017},
  keywords = {Amazonia-1,CCSDS.,Remote Sensing Satellite,Satellite Control System,Space Link Extension (SLE)   Protocol,Space Mission Cost,â No DOI found},
  pages = {1-12},
  file = {/home/yuri/Zotero/storage/S98RUTW9/julio filho_Amazonia.pdf},
  organization = {{International Astronomical Congress, 68. (IAC)}},
  affiliation = {Instituto Nacional de Pesquisas Espaciais (INPE) and Instituto Nacional de Pesquisas Espaciais (INPE) and Instituto Nacional de Pesquisas Espaciais (INPE) and Instituto Nacional de Pesquisas Espaciais (INPE)},
  conference-location = {Adelaide, Australia},
  conference-year = {25-29 Sept.},
  targetfile = {julio filho<sub>A</sub>mazonia.pdf}
}
% == BibTeX quality report for JulioFoAmbrFerrLour:2017:ChImSp:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@misc{SCD1Completa20,
  title = {{SCD-1 completa 20 anos. Primeiro sat{\'e}lite brasileiro comprova o {\^e}xito da engenharia espacial no pa{\'i}s}},
  language = {pt-br},
  howpublished = {http://www.inpe.br/noticias/noticia.php?Cod\_Noticia=3198},
  file = {/home/yuri/Zotero/storage/FY6F4GVX/noticia.html}
}
% == BibTeX quality report for SCD1Completa20:
% Missing required field 'author'
% Missing required field 'year'

@misc{INPECBERS,
  title = {{{INPE}}/{{CBERS}}},
  howpublished = {http://www.cbers.inpe.br/sobre/cbers04a.php},
  file = {/home/yuri/Zotero/storage/T9PSGNZC/cbers04a.html}
}
% == BibTeX quality report for INPECBERS:
% Missing required field 'author'
% Missing required field 'year'

@misc{INPECentroRastreio,
  title = {{{INPE}}/{{Centro}} de {{Rastreio}} e {{Controle}} de {{Sat{\'e}lites}}},
  howpublished = {http://www.inpe.br/crc/satelites/},
  file = {/home/yuri/Zotero/storage/IYRPCIFN/satelites.html}
}
% == BibTeX quality report for INPECentroRastreio:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@misc{INPEMissaoAmazonia,
  title = {{{INPE}} / {{Miss{\~a}o Amazonia}} 1},
  howpublished = {http://www3.inpe.br/amazonia-1/},
  file = {/home/yuri/Zotero/storage/PARHWDHZ/amazonia-1.html}
}
% == BibTeX quality report for INPEMissaoAmazonia:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@book{rcoreteamLanguageEnvironmentStatistical2018,
  address = {{Vienna, Austria}},
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {{R Core Team}},
  year = {2018},
  keywords = {\#nosource},
  organization = {{R Foundation for Statistical Computing}}
}
% == BibTeX quality report for rcoreteamLanguageEnvironmentStatistical2018:
% Missing required field 'publisher'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{mateikUsingBigData2017,
  title = {Using {{Big Data Technologies}} for {{Satellite Data Analytics}}},
  isbn = {978-1-62410-483-1},
  language = {en},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2017-5161},
  author = {Mateik, Dennis and Mital, Rohit and Buonaiuto, Nicholas L. and Louie, Mark and Kief, Craig and Aarestad, Jim},
  month = sep,
  year = {2017},
  keywords = {\#nosource},
  file = {/home/yuri/Zotero/storage/TPFTF9YY/Mateik et al. - 2017 - Using Big Data Technologies for Satellite Data Ana.pdf}
}
% == BibTeX quality report for mateikUsingBigData2017:
% Missing required field 'booktitle'
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@article{vieiraCONCURRENTSYSTEMSENGINEERING,
  title = {{{CONCURRENT SYSTEMS ENGINEERING}}: {{CUBESAT SYSTEM}} \textendash{} {{CASE STUDY}}},
  language = {en},
  author = {Vieira, Fernando Putarov},
  keywords = {ðNo DOI found},
  pages = {17},
  file = {/home/yuri/Zotero/storage/MC86TTX8/Vieira - CONCURRENT SYSTEMS ENGINEERING CUBESAT SYSTEM â C.pdf;/home/yuri/Zotero/storage/Z8WAJB3Q/concurrent-systems-engineering-cubesat-system---case-study.pdf}
}
% == BibTeX quality report for vieiraCONCURRENTSYSTEMSENGINEERING:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@phdthesis{Magalhaes:2012:EsAvTe,
  address = {{S{\~a}o Jos{\'e} dos Campos}},
  title = {{Estudo de avalanche t{\'e}rmica em um sistema de carga e descarga de bateria em sat{\'e}lites artificiais}},
  abstract = {Este trabalho apresenta um estudo de avalanche t{\'e}rmica em um  sistema de carga e descarga de baterias em sat{\'e}lites  artificiais. Para tanto, desenvolve-se um modelo baseado em  prin{\'c}\i{}pios macrosc{\'o}picos, o qual pode ser  generalizado para uma ampla variedade de topologias de suprimento  de energia e tecnologias de bateria. A partir do modelo obtido,  identifica-se a propriedade emergente de avalanche t{\'e}rmica, a  qual ocorre no sistema quando todos os equipamentos est{\~a}o  acoplados e interagindo uns com os outros. Mostra-se que esta  propriedade {\'e} decorrente da intera{\c c}{\~a}o dos efeitos  da efici{\^e}ncia de carga da bateria, degrada{\c c}{\~a}o de  par{\^a}metros do sistema e modo de opera{\c c}{\~a}o do  sat{\'e}lite. Uma vez identificadas as causas dessa instabilidade  t{\'e}rmica, desenvolve-se uma nova m{\'e}trica, baseada em  m{\'e}todos gr{\'a}ficos, obtida a partir do mapa de  Poincar{\'e}, permitindo estabelecer a margem de estabilidade do  sistema e delimitar as regi{\~o}es seguras daquelas com  possibilidade de desencadear avalanche t{\'e}rmica. ABSTRACT: In  this work, we present a study of thermal avalanche on an  artificial satellite battery charging and discharging system. To  do so, we develop a model based on macroscopic principles which  can be generalized to a wide variety of topologies, power supply  and battery technologies. From the model obtained, we identify the  emergent property of thermal avalanche in the system which arises  when we consider the interactions between equipment. We show that  this avalanche effect is due to the interaction of battery  parameters such as efficiency or double-layer capacitance,  degradation of system parameters and the operational modes of the  system. Having identified the causes of thermal instability, we  develop a new metric, based on graphical methods, obtained from  the Poincar{\'e} map, enabling the establishment of a stability  margin, as well as the identification of the threshold for  triggering a thermal avalanche.},
  language = {pt},
  school = {Instituto Nacional de Pesquisas Espaciais},
  author = {de Magalh{\~a}es, Renato Oliveira},
  month = feb,
  year = {2012},
  keywords = {\#nosource,avalanche tÃ©rmica,baterias,batteries,mapa de PoincarÃ©,PoincarÃ© map,satellite power supply.,suprimento de energia de satÃ©lites,thermal   avalanche,â No DOI found},
  file = {/home/yuri/Zotero/storage/F9EMP272/de Magalhaes - ESTUDO DE AVALANCHE TEÂ´RMICA EM UM SISTEMA DE CARG.pdf},
  committee = {Ricci, MÃ¡rio CÃ©sar (presidente) and Souza, Marcelo Lopes de Oliveira e (orientador) and Milani, Paulo GiÃ¡como and Leite, RosÃ¢ngela Meireles Gomes de and Trivelato, Gilberto da Cunha and Ferreira, Leonardo de OlivÃ©},
  copyholder = {SID/SCD},
  englishtitle = {A study of thermal avalanche on an artificial satellite battery charging and discharging system},
  targetfile = {publicacao.pdf}
}

@book{ArtificialPlanningScheduling,
  title = {Artificial {{Planning}} and {{Scheduling}}},
  keywords = {\#nosource}
}
% == BibTeX quality report for ArtificialPlanningScheduling:
% Missing required field 'author'
% Missing required field 'publisher'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{ReferenceModelOpen2012,
  title = {Reference {{Model}} for an {{Open Archival Information System}} ({{OAIS}})},
  language = {en},
  year = {2012},
  keywords = {\#nosource},
  pages = {135}
}
% == BibTeX quality report for ReferenceModelOpen2012:
% Missing required field 'author'
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@article{boniolModularCertifiedAvionics2014,
  title = {Towards {{Modular}} and {{Certified Avionics}} for {{UAV Aerial Robotics}}},
  abstract = {This paper proposes a review of the current state and forthcoming evolutions for UAV avionics architecture and software. It provides an outlook of the specific technical issues arising in the design of embedded systems for UAV.},
  number = {8},
  journal = {AerospaceLab},
  author = {BONIOL, F. and Wiels, V.},
  year = {2014},
  keywords = {\#nosource,AVIONICS,UAV},
  pages = {1-8}
}
% == BibTeX quality report for boniolModularCertifiedAvionics2014:
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@book{russellArtificialIntelligenceModern2003,
  address = {{Upper Saddle River, N.J}},
  edition = {2nd ed},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  title = {Artificial Intelligence: A Modern Approach},
  isbn = {978-0-13-790395-5},
  lccn = {Q335 .R86 2003},
  shorttitle = {Artificial Intelligence},
  publisher = {{Prentice Hall/Pearson Education}},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2003},
  keywords = {\#nosource,Artificial intelligence}
}

@article{senaUmaSolucaoPara2016,
  title = {Uma Solu{\c c}{\~a}o Para Qualidade de Contexto Baseada Em Ontologia e L{\'o}gica Nebulosa Com Aplica{\c c}{\~a}o Em Monitoramento de Sinais Vitais Em Uti},
  author = {Sena, M{\'a}rcio Vin{\'i}cius Oliveira and others},
  year = {2016},
  keywords = {\#nosource}
}
% == BibTeX quality report for senaUmaSolucaoPara2016:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@misc{chenStudySatelliteTelemetry2015,
  title = {The {{Study}} of the {{Satellite Telemetry Parameters Prediction Method Based}}-on x-11 {{Model}}},
  abstract = {During the long-term operation process of the satellites in orbit, because of
the design life and a variety of environmental factors, the telemetry paramet...},
  language = {en},
  journal = {Recent Advances in Electrical \& Electronic Engineering},
  howpublished = {http://www.eurekaselect.com/130439/article},
  author = {Chen, Bing and Fang, Hong-Zheng and Fan, Hao-dong Ma {and} Huan-Zhen},
  month = jul,
  year = {2015},
  keywords = {\#nosource}
}
% == BibTeX quality report for chenStudySatelliteTelemetry2015:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{2009ESASP.673E...8S,
  series = {{{ESA Special Publication}}},
  title = {Advanced {{Planning}} and {{Scheduling Initiative}}- {{MrSpock Aims}} for {{Xmas}}},
  volume = {673},
  booktitle = {{{ESA Special Publication}}},
  author = {Steel, R. and Niezette, M. and Cesta, A. and Fratini, S. and Oddi, A. and Cortellessa, G. and Rasconi, R. and Verfaillie, G. and Pralet, C. and Lavagna, M. and Brambilla, A. and Castellini, F. and Donati, A. and Policella, N.},
  month = sep,
  year = {2009},
  keywords = {\#nosource},
  pages = {8},
  eid = {8},
  adsurl = {http://adsabs.harvard.edu/abs/2009ESASP.673E...8S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
% == BibTeX quality report for 2009ESASP.673E...8S:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{menonConcurrentEngineeringEffective1996,
  title = {Concurrent Engineering: Effective Deployment Strategies},
  volume = {6},
  issn = {0103-6513},
  shorttitle = {Concurrent Engineering},
  number = {2},
  journal = {Production},
  doi = {10/gdwr2z},
  author = {Menon, Unny and Graham, Michael},
  month = dec,
  year = {1996},
  pages = {165-181},
  file = {/home/yuri/Zotero/storage/64K9GGBY/Menon and Graham - 1996 - Concurrent engineering effective deployment strat.pdf;/home/yuri/Zotero/storage/ZKIWWMY8/scielo.html}
}

@book{armySystemsEngineeringFundamentals2013,
  title = {Systems {{Engineering Fundamentals}}},
  isbn = {978-1-4841-2083-5},
  abstract = {This book provides a basic, conceptual-level description of engineering management disciplines that relate to the development and life cycle management of a system. For the non-engineer it provides an overview of how a system is developed. For the engineer and project manager it provides a basic framework for planning and assessing system development. Information in the book is from various sources, but a good portion is taken from lecture material developed for the two Systems Planning, Research, Development, and Engineering courses offered by the Defense Acquisition University. The book is divided into four parts: Introduction; Systems Engineering Process; Systems Analysis and Control; and Planning, Organizing, and Managing. The first part introduces the basic concepts that govern the systems engineering process and how those concepts fit the Department of Defense acquisition process. Chapter 1 establishes the basic concept and introduces terms that will be used throughout the book. The second chapter goes through a typical acquisition life cycle showing how systems engineering supports acquisition decision making. The second part introduces the systems engineering problem-solving process, and discusses in basic terms some traditional techniques used in the process. An overview is given, and then the process of requirements analysis, functional analysis and allocation, design synthesis, and verification is explained in some detail. This part ends with a discussion of the documentation developed as the finished output of the systems engineering process. Part three discusses analysis and control tools that provide balance to the process. Key activities (such as risk management, configuration management, and trade studies) that support and run parallel to the system engineering process are identified and explained. Part four discusses issues integral to the conduct of a systems engineering effort, from planning to consideration of broader management issues. In some chapters supplementary sections provide related material that shows common techniques or policy-driven processes. These expand the basic conceptual discussion, but give the student a clearer picture of what systems engineering means in a real acquisition environment.},
  language = {English},
  publisher = {{CreateSpace Independent Publishing Platform}},
  author = {Army, United States Government US},
  month = apr,
  year = {2013}
}
% == BibTeX quality report for armySystemsEngineeringFundamentals2013:
% ? Title looks like it was stored in title-case in Zotero

@article{viswanathanUsercentricSpatialData2014,
  title = {User-Centric Spatial Data Warehousing: A Survey of Requirements and Approaches},
  volume = {6},
  issn = {1759-1163, 1759-1171},
  shorttitle = {User-Centric Spatial Data Warehousing},
  language = {en},
  number = {4},
  journal = {International Journal of Data Mining, Modelling and Management},
  doi = {10.1504/IJDMMM.2014.066764},
  author = {Viswanathan, Ganesh and Schneider, Markus},
  year = {2014},
  pages = {369},
  file = {/home/yuri/Zotero/storage/8BL5P7DC/Viswanathan and Schneider - User-centric spatial data warehousing a survey of.pdf}
}

@inproceedings{boubrahimiPredictionX003E1002017,
  title = {On the Prediction of \#{{x003E}};100 {{MeV}} Solar Energetic Particle Events Using {{GOES}} Satellite Data},
  abstract = {Solar energetic particles are a result of intense solar events such as solar flares and Coronal Mass Ejections (CMEs). These latter events all together can cause major disruptions to spacecraft that are in Earth's orbit and outside of the magnetosphere. In this work we are interested in establishing the necessary conditions for a major geo-effective solar particle storm immediately after a major flare, namely the existence of a direct magnetic connection. To our knowledge, this is the first work that explores not only the correlations of GOES X-ray and proton channels, but also the correlations that happen across all the proton channels. We found that proton channels autocorrelations and cross-correlations may also be precursors to the occurrence of an SEP event. In this paper, we tackle the problem of predicting {$>$}100 MeV SEP events from a multivariate time series perspective using easily interpretable decision tree models.},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2017.8258212},
  author = {Boubrahimi, S. F. and Aydin, B. and Martens, P. and Angryk, R.},
  month = dec,
  year = {2017},
  keywords = {Satellites,Earth,Predictive models,solar energetic particles,solar wind,100 MeV SEP,CART decision tree,Coronal Mass Ejections,Correlation,cosmic ray protons,direct magnetic connection,Earth's orbit,geo-effective solar particle storm,GOES satellite data,GOES X-ray and Proton correlation,GOES X-ray channels,intense solar events,Magnetosphere,proton channels autocorrelations,Protons,SEP event,SEP Events Prediction,solar corona,solar coronal mass ejections,solar cosmic ray particles,solar flares,solar prominences,time series,Time series analysis,Vector autoregression},
  pages = {2533-2542},
  file = {/home/yuri/Zotero/storage/4G8RKQEJ/Boubrahimi et al. - 2017 - On the prediction of #x003E\;100 MeV solar energeti.pdf;/home/yuri/Zotero/storage/DA22ZHUV/8258212.html}
}
% == BibTeX quality report for boubrahimiPredictionX003E1002017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{wangStudyPerformanceComparison2018,
  title = {Study of Performance Comparison of Satellite Error Correction Codes for Correcting Big Burst Data Errors},
  abstract = {Big burst errors often occur in satellite communications, leading to multiple consecutive bit errors. In the paper, we make a comparison among the CCSDS codes, such as RS codes, convolutional codes, Turbo codes and LDPC codes. In addition, the performance analysis of correcting big burst errors of these codes is conducted. The experimental results show that the burst error correction performance of RS codes is the best under CCSDS standard.},
  booktitle = {2018 {{IEEE}} 3rd {{International Conference}} on {{Big Data Analysis}} ({{ICBDA}})},
  doi = {10.1109/ICBDA.2018.8367687},
  author = {Wang, B. and Zhang, Q.},
  month = mar,
  year = {2018},
  keywords = {error correction,error correction codes,satellite communication,Big Data,big burst data errors,Burst error,burst error correction performance,CCSDS,CCSDS codes,Conferences,convolutional codes,error correction code,LDPC codes,multiple consecutive bit errors,parity check codes,performance analysis,performance comparison,Reed-Solomon codes,RS codes,satellite communications,satellite error correction codes,turbo codes,Turbo codes},
  pages = {254-258},
  file = {/home/yuri/Zotero/storage/7JHSKBNH/Wang and Zhang - 2018 - Study of performance comparison of satellite error.pdf;/home/yuri/Zotero/storage/75XJYSPS/8367687.html}
}
% == BibTeX quality report for wangStudyPerformanceComparison2018:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{zengMultidimensionalDataVisualization2018,
  title = {Multidimensional Data Visualization Technology of Radiation Effects in Space Environment},
  abstract = {Conventional space environment information display exists some problems such as single display mode, abstract display content, and poor display effect. It is difficult to understand the information status of space environment as a whole. Therefore, it is urgent to establish a unified platform for the integration of space object and environment, which can make multidimensional data into two-dimensional and three-dimensional space and vector expression. The platform can realize dynamic display space environment and the historical data query visualization processing. In this paper, the space environment data integrated display system will be developed by realizing the multi-dimensional and visual integration of space natural environment and the space object environment. It can also grasp clearly and intuitively the state and laws of the natural environment of space, the effect of space natural environment on spacecraft, the flight state and characteristic parameters of a space object. It can accomplish the integration of a variety of data interactive display to fulfill the visualization and simplification of space environment information.},
  booktitle = {2018 {{IEEE}} 3rd {{International Conference}} on {{Cloud Computing}} and {{Big Data Analysis}} ({{ICCCBDA}})},
  doi = {10.1109/ICCCBDA.2018.8386524},
  author = {Zeng, D. and Wang, Y.},
  month = apr,
  year = {2018},
  keywords = {Space vehicles,aerospace computing,Earth,Orbits,data visualisation,radiation effects,abstract display content,Aircraft manufacture,Belts,conventional space environment information display,Cosmic rays,data interactive display,dynamic display space environment,environment effect,historical data query visualization processing,information status,Magnetic fields,multidimensional data visualization technology,poor display effect,query processing,single display mode,space environment,space environment data integrated display system,space natural environment,space object,space object environment,three-dimensional displays,three-dimensional space,vector expression,visual integration,visualization},
  pages = {266-270},
  file = {/home/yuri/Zotero/storage/AYM42K3Z/Zeng and Wang - 2018 - Multidimensional data visualization technology of .pdf;/home/yuri/Zotero/storage/BVI2M4EA/8386524.html}
}
% == BibTeX quality report for zengMultidimensionalDataVisualization2018:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{TominagaFerrAmbr:2017:CoSaTe,
  address = {{S{\~a}o Jos{\'e} dos Campos}},
  title = {Comparing Satellite Telemetry against Simulation Parameters in a Simulator Model Reconfiguration Tool},
  abstract = {Operational satellite simulator models require updates in order to  reflect the behavior of the actual satellite, especially after  several years of flight operations. Because it can be extremely  costly to modify them manually, a software tool for autonomous  reconfiguration of operational simulator behavior models is  proposed. To implement such a feature, behavior models must be  reassessed and reevaluated to identify the need of model updates  and to trigger them. This can be achieved by periodically  monitoring discrepancies between the satellite telemetry and the  simulator parameters. This paper describes a procedure for  comparative analysis between telemetry and simulation parameters  for an autonomous reconfiguration tool for operational simulator  behavior models for synchronization.},
  language = {English},
  booktitle = {Anais...},
  publisher = {{Instituto Nacional de Pesquisas Espaciais (INPE)}},
  author = {Tominaga, Jun and Ferreira, Mau{\'r}\i{}cio Gon{\c c}alves Vieira and Ambr{\'o}sio, Ana Maria},
  editor = {Cerqueira, Christopher Shneider and B{\"u}rger, Eduardo Escobar and Yassuda, Irineu dos Santos and Rodrigues, Italo Pinto and Lima, Jeanne Samara dos Santos and de Oliveira, M{\^o}nica Elizabeth Rocha and Ten{\'o}rio, P{\'l}\i{}nio Ivo Gama},
  year = {2017},
  keywords = {Telemetry,Ground Control Software,Software Simulation.,Spacecraft Operations,Tracking and Commanding},
  file = {/home/yuri/Zotero/storage/A6MD8T2F/9_[ARTIGO] Jun Tominaga.pdf},
  issn = {2177-3114},
  organization = {{Workshop em Engenharia e Tecnologias Espaciais, 8. (WETE)}},
  type = {Engenharia e Gerenciamento de Sistemas Espaciais},
  affiliation = {Instituto Nacional de Pesquisas Espaciais (INPE) and Instituto Nacional de Pesquisas Espaciais (INPE)},
  conference-location = {SÃ£o JosÃ© dos Campos},
  conference-year = {9-10 ago. 2017},
  organisation = {Instituto Nacional de Pesquisas Espaciais (INPE)},
  targetfile = {9<sub>[</sub>ARTIGO] Jun Tominaga.pdf}
}
% == BibTeX quality report for TominagaFerrAmbr:2017:CoSaTe:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle

@inproceedings{AzevedoAmbr:2010:ArSaTe,
  address = {{S{\~a}o Jos{\'e} dos Campos}},
  title = {Dependability in {{Satellite Systems}}: {{An Architecture}} for {{Satellite Telemetry Analysis}}},
  volume = {IWETE2010-1065},
  language = {English},
  booktitle = {Anais...},
  publisher = {{INPE}},
  author = {Azevedo, Denise Nunes Rotondi and Ambr{\'o}sio, Ana Maria},
  year = {2010},
  keywords = {â No DOI found},
  pages = {6},
  file = {/home/yuri/Zotero/storage/J4BSWGDA/CSE_ssme_1065-eng.pdf},
  issn = {2177-3114},
  organization = {{Workshop em Engenharia e Tecnologia Espaciais, 1. (WETE).}},
  affiliation = {Instituto Nacional de Pesquisas Espaciais (INPE) and Instituto Nacional de Pesquisas Espaciais (INPE)},
  conference-location = {SÃ£o JosÃ© dos Campos},
  conference-year = {30 mar. - 1 abr. 2010},
  targetfile = {CSEâsmeâ065-eng.pdf}
}
% == BibTeX quality report for AzevedoAmbr:2010:ArSaTe:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{euclidesAPUAMASoftwareTool2017,
  title = {{{APUAMA}}: A Software Tool for Reaction Rate Calculations},
  volume = {23},
  issn = {1610-2940, 0948-5023},
  shorttitle = {{{APUAMA}}},
  abstract = {APUAMA is a free software designed to determine the reaction rate and thermodynamic properties of chemical species of a reagent system. With data from electronic structure calculations, the APUAMA determine the rate constant with tunneling correction, such as Wigner, Eckart and small curvature, and also, include the rovibrational level of diatomic molecules. The results are presented in the form of Arrhenius form, for the reaction rate, and the thermodynamic properties are written down in the polynomial form. The word APUAMA means ``fast'' in Tupi-Guarani Brazilian language, then the code calculates the reaction rate on a simple, intuitive graphic interface, the form fast and practical. As program output, there are several text files with tabulated information for rate constant, rovibrational levels, energy barriers and enthalpy of reaction, Arrhenius coefficient, and also, the option to the User save all graphics in BMP format.},
  language = {en},
  number = {6},
  journal = {Journal of Molecular Modeling},
  doi = {10/gbkmx4},
  author = {Euclides, Henrique O. and P. Barreto, Patricia R.},
  month = jun,
  year = {2017},
  file = {/home/yuri/Zotero/storage/UNJZ3BWY/Euclides and P. Barreto - 2017 - APUAMA a software tool for reaction rate calculat.pdf}
}
% == BibTeX quality report for euclidesAPUAMASoftwareTool2017:
% Missing required field 'pages'

@article{loureiroIAC10D1LESSONSLEARNED2010,
  title = {{{IAC}}-10-{{D1}}.5.2 {{LESSONS LEARNED IN}} 12 {{YEARS OF SPACE SYSTEMS CONCURRENT ENGINEERING}}},
  language = {en},
  author = {Loureiro, G},
  year = {2010},
  pages = {8},
  file = {/home/yuri/Zotero/storage/5226B3Q9/Loureiro - IAC-10-D1.5.2 LESSONS LEARNED IN 12 YEARS OF SPACE.pdf}
}
% == BibTeX quality report for loureiroIAC10D1LESSONSLEARNED2010:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@incollection{montroniBigDataMission2016,
  title = {Big {{Data}} in {{Mission Operations}}, the {{ExoMars}} 2016 {{Experience}}},
  booktitle = {{{AIAA SPACE}} 2016},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Montroni, Gianluca and Pantoquilho, Marta and Santos, Rui},
  year = {2016},
  file = {/home/yuri/Zotero/storage/T8KNITV5/6.html;/home/yuri/Zotero/storage/V7JD8SZS/6.html},
  doi = {10.2514/6.2016-5403}
}
% == BibTeX quality report for montroniBigDataMission2016:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@article{blakeFlavoursPhysicsMachine2018,
  title = {Flavours of {{Physics}}: The Machine Learning Challenge for the Search of {$\tau$} - {$\rightarrow$} {$M-\mathrm{\mu}-\mathrm{\mu}$}+ Decays at {{LHCb}}},
  abstract = {The Flavours of Physics machine learning challenge has been proposed to enhance the exchange of knowledge between the particle physics and data science communities. The LHCb collaboration provides simulated and real data used in the search for lepton flavour violationg decay {$\tau$} - {$\rightarrow$} {$\mathrm{\mu}-\mathrm{\mu}-\mathrm{\mu}$}+. The challenge will be hosted by Kaggle (https://www.kaggle.com). This document describes in details the challenge with physics motivation for studying the {$\tau$} - {$\rightarrow$} {$\mathrm{\mu}-\mathrm{\mu}-\mathrm{\mu}$}+ decay and analysis steps. No prior knowledge of high energy physics is required. All information on how to participate in the Challenge is available on the Kaggle web site.},
  language = {en},
  author = {Blake, Thomas and Bettler, Marc-Olivier and Chrz{\k{a}}szcz, Marcin and Dettori, Francesco and Ustyuzhanin, Andrey},
  year = {2018},
  keywords = {ðNo DOI found},
  pages = {14},
  file = {/home/yuri/Zotero/storage/IX8LU56Z/Blake et al. - Flavours of Physics the machine learning challeng.pdf;/home/yuri/Zotero/storage/ZQLEWQQM/Blake et al. - Flavours of Physics the machine learning challeng.pdf}
}
% == BibTeX quality report for blakeFlavoursPhysicsMachine2018:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'

@inproceedings{qianQuantitativeEvaluationHyperspectral2002,
  title = {Quantitative Evaluation of Hyperspectral Data Compressed by near Lossless Onboard Compression Techniques},
  volume = {3},
  abstract = {The Canadian Space Agency is investigating an onboard compressor for a hyperspectral satellite using its two innovative data compression techniques. It is essential to verify the quality of the compressed data and users' acceptability in terms of their remote sensing applications. Hyperspectral data cubes acquired by hyperspectral sensors such as casi, AVIRIS, Probe-1 and Hyperion were tested. Statistical hypothesis tests were used to assess if the means and variances in each spectral band of specified zones calculated from the reconstructed data cubes are significantly different from those calculated from the original data cube. Remote sensing end products, such as red edge, chlorophyll content and spectral unmixing were used to evaluate the compressed data. Preliminary test results show that hyperspectral data compressed using the two compression techniques at compression ratio up to 30:1 are acceptable in terms of statistical tests and remote sensing end products.},
  booktitle = {{{IEEE International Geoscience}} and {{Remote Sensing Symposium}}},
  doi = {10.1109/IGARSS.2002.1026137},
  author = {Qian, Shen-En and Hu, Baoxin and Bergeron, M. and Hollinger, A. and Oswald, P.},
  month = jun,
  year = {2002},
  keywords = {Satellites,Testing,Calibration,image processing,data cube,chlorophyll,data compression,Data compression,geophysical measurement technique,geophysical signal processing,geophysical techniques,Hyperspectral imaging,hyperspectral remote sensing,Hyperspectral sensors,Image coding,image compression,infrared,IR,land surface,lossless method,multidimensional signal processing,multispectral remote sensing,onboard method,quantitative evaluation,red edge,Reflectivity,remote sensing,Remote sensing,satellite remote sensing,spectral band,statistical hypothesis test,terrain mapping,Vector quantization,vegetation mapping,visible},
  pages = {1425-1427 vol.3},
  file = {/home/yuri/Zotero/storage/ALWQQVAA/Qian et al. - 2002 - Quantitative evaluation of hyperspectral data comp.pdf;/home/yuri/Zotero/storage/KQFMMAJZ/Qian et al. - 2002 - Quantitative evaluation of hyperspectral data comp.pdf;/home/yuri/Zotero/storage/8MVP74RF/1026137.html;/home/yuri/Zotero/storage/PMEY3T23/1026137.html}
}
% == BibTeX quality report for qianQuantitativeEvaluationHyperspectral2002:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{helmichVisualizingRDFData2014,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Visualizing {{RDF Data Cubes Using}} the {{Linked Data Visualization Model}}},
  isbn = {978-3-319-11954-0 978-3-319-11955-7},
  abstract = {Data Cube represents one of the basic means for storing, processing and analyzing statistical data. Recently, the RDF Data Cube Vocabulary became a W3C recommendation and at the same time interesting datasets using it started to appear. Along with them appeared the need for compatible visualization tools. The Linked Data Visualisation Model is a formalism focused on this area and is implemented by Payola, a framework for analysis and visualization of Linked Data. In this paper, we present capabilities of LDVM and Payola to visualize RDF Data Cubes as well as other statistical datasets not yet compatible with the Data Cube Vocabulary. We also compare our approach to CubeViz, which is a visualization tool specialized on RDF Data Cube visualizations.},
  language = {en},
  booktitle = {The {{Semantic Web}}: {{ESWC}} 2014 {{Satellite Events}}},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-11955-7_50},
  author = {Helmich, Ji{\v r}{\'i} and Kl{\'i}mek, Jakub and Ne{\v c}ask{\'y}, Martin},
  month = may,
  year = {2014},
  pages = {368-373},
  file = {/home/yuri/Zotero/storage/XWGS4S9Z/Helmich et al. - 2014 - Visualizing RDF Data Cubes Using the Linked Data V.pdf;/home/yuri/Zotero/storage/4CWHQZNL/10.html;/home/yuri/Zotero/storage/AEUJFVKE/10.html}
}
% == BibTeX quality report for helmichVisualizingRDFData2014:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{coronelSystemsEngineeringApproach2017,
  address = {{Adelaide, Australia}},
  title = {A {{Systems Engineering}} Approach for Specifying a Combined {{Compact Antenna Test Range}} and {{Near}}-{{Field Scanner}} Facility},
  abstract = {This paper describes the systems engineering approach used at the Systems Engineering Office (LSIS) to specify the detailed requirements of an integrated compact payload test range (CPTR) and near-field scanner (NFS) facility. The Integration and Test Laboratory (LIT) of the Brazilian National Institute for Space Research (INPE) is currently expanding its facilities due to the evolution of Brazilian space programs, which includes larger, complex, and heavier satellites, such as telecommunication satellites. One of the new test facilities that will be part of the LIT is an integrated CPTR and NFS facility for antenna and satellite payload testing. The systems engineering approach includes systems engineering processes, such as stakeholder needs and requirements definition, system concepts definition, system requirements definition, system analysis, and architecture definition. The approach also describes specific methods and tools that were implemented within each process, such as stakeholder interviews, life cycle analysis, context diagrams, and behavioral diagrams. Additionally, this paper presents a review of several existing compact test range and near-field facilities whose characteristics aided in the elaboration of the specifications. The systems engineering approach that was used enabled the establishment of an optimal solution for the combined CPTR and NFS facility that aligns with Brazilian space program projections.},
  language = {en},
  booktitle = {68 Th {{International Astronautical Congress}} ({{IAC}})},
  author = {Coronel, Gabriel G. M. and Burger, Eduardo E. and Siqueira, Renato C. and Raimundi, Lucas R. and Kawassaki, Guilherme N. and Lino, Carlos O. and Loureiro, Geilson},
  year = {2017},
  keywords = {ðNo DOI found},
  pages = {11},
  file = {/home/yuri/Zotero/storage/HTFRBAMZ/SUBSTITUIR_IAC-2017.pdf}
}
% == BibTeX quality report for coronelSystemsEngineeringApproach2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{loureiroLessonsLearned202018,
  title = {Lessons Learned in 20 Years of Application of {{Systems Concurrent Engineering}} to Space Products},
  volume = {151},
  issn = {00945765},
  abstract = {This paper aims to present the lessons learned in 20 years of application of the SCE (Systems Concurrent Engineering) approach that evolved over the last 20 years being applied to the development of more than 200 complex system solutions. SCE is an approach to the integrated development of complex systems that applies the systems engineering process, simultaneously, to the product elements of a system solution as well as for the service elements of the system solution, recursively, at every layer of the system solution breakdown structure. The approach was born as the application of the requirements, functional and physical analysis processes to the simultaneous development of a product, its life cycle processes and their performing organizations, at every layer of the product breakdown structure. The continuous application of the approach up to 2010, showed the need to include a stakeholder analysis step, to acknowledge that the solution was comprised of product and organization elements (processes were, in fact, the functions of products and organizations), that a mission layer should be added at the top of the product breakdown structure and that the notion of circumstances should be added to the traditional notion of scenarios. With the increasing use of the approach for system of systems conception and development such as those involving multi-spacecraft solutions, the mission layer needed to be extended to include other life cycle processes (besides the operations processes) concept of service and system service architecture. This requires the development of a system solution breakdown structure that will guide the development of the overall solution. For multi-spacecraft solutions, for example, it is necessary to conceive and architect testing, launching and decommissioning services as early as operations. Also, going into more detail in the approach, modes can be derived from circumstances, interface states and internal states of the system and not only from circumstances, as initially established in the approach. These lessons to be presented were learned during the development of: 1) the Brazilian Strategic Program for Space Systems (PESE) and; 2) the TIM Project (Telematics International Mission), a satellite formation with contributions from many regions in the world.},
  language = {en},
  journal = {Acta Astronautica},
  doi = {10.1016/J.ACTAASTRO.2018.05.042},
  author = {Loureiro, G. and Panades, W.F. and Silva, A.},
  month = oct,
  year = {2018},
  pages = {44-52},
  file = {/home/yuri/Zotero/storage/4F5MWNRP/Loureiro et al. - 2018 - Lessons learned in 20 years of application of Syst.pdf}
}
% == BibTeX quality report for loureiroLessonsLearned202018:
% Missing required field 'number'

@article{arnautSystemsEngineeringProcess2013,
  title = {Systems {{Engineering Process}} of a {{CubeSat}} from the {{Perspective}} of {{Operations}}},
  volume = {23},
  issn = {23345837},
  abstract = {This paper presents the application of a systems concurrent engineering approach to university CubeSats development. The university CubeSat, object of this paper, is a joint development between the Brazilian Institute for Space Research (INPE) and the Aeronautics Institute of Technology (ITA). The effort consists in developing a Brazilian CubeSat platform that can be industrialized in Brazil and takes advantage of a Systems Engineering Process approach. This Systems Engineering Process applied on the CubeSat includes mission and life-cycle analysis, requirements engineering, functional analysis, architecture design, and uses an operation life-cycle scenario as an example. The approach is supposed to be applied to every life-cycle process. Thus, System Engineering Process gives a wide range view of the problem, guiding engineers to develop product and organization solutions throughout life-cycle. This approach helps to solve technical and management issues that sometimes are not encompassed with just experienced staff and standard methods.},
  language = {en},
  number = {1},
  journal = {INCOSE International Symposium},
  doi = {10.1002/J.2334-5837.2013.TB03031.X},
  author = {Arnaut, Bruno Morato and Viot, Diego Dutra and Costa, Lucas Lopes and Loureiro, Geilson and B{\"u}rger, Eduardo Escobar and Franchitto, Marcelo},
  month = jun,
  year = {2013},
  pages = {458-470},
  file = {/home/yuri/Zotero/storage/HQTJD72T/Arnaut et al. - 2013 - Systems Engineering Process of a CubeSat from the .pdf}
}
% == BibTeX quality report for arnautSystemsEngineeringProcess2013:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{prarthanaUserBehaviourAnomaly2017,
  title = {User {{Behaviour Anomaly Detection}} in {{Multidimensional Data}}},
  abstract = {User Anomaly Detection, an important aspect of User Behaviour Analysis, is used to find anomalous user events from the event and network traffic log data. Traditional security mechanisms are not able to detect new/unknown types of anomalies as they do not incorporate contextual and behavioural aspects of the data for analysis. User Behaviour Anomaly Detection (UBAD) employs user behavioural patterns in context thereby achieving a higher detection rate. Log data employed for UBAD has multiple dimensions. With increase in dimensions (attributes), data gets sparse and the detection of anomalies becomes increasingly complex. For this reason, the single dimensional algorithms proposed for anomaly detection based on clustering, proximity and dimensional reduction do not work well on higher dimension data. OLAP based data analysis techniques provide efficient data slicing and aggregation operations crucial for multidimensional analytics along with multiscale visualisation for exploratory discovery. In this paper, an effective multidimensional process for UBAD is developed to detect anomalies using multi-dimensional statistical tests. An integral part of the process is the development of an OLAP Cube data model for event log data. The statistical efficiency of the UBAD process for different dimensions is investigated. On a real-life event log data, it is shown that the statistical efficiency of detection improves with the increased dimensionality of the tests: the true negative rate and the true positive rate show marked improvement. It is deduced that the computationally more expensive higher dimensional tests need to be employed in order to achieve better anomaly detection.},
  booktitle = {2017 {{IEEE International Conference}} on {{Cloud Computing}} in {{Emerging Markets}} ({{CCEM}})},
  doi = {10.1109/CCEM.2017.19},
  author = {Prarthana, T. S. and Gangadhar, N. D.},
  month = nov,
  year = {2017},
  keywords = {Data models,Anomaly detection,Anomaly Detection,security of data,Security,Data Cube,OLAP,Big Data,data visualisation,aggregation operations,anomalous user events,behavioural aspects,Clustering,Clustering algorithms,Data analysis,Data Analytics,higher detection rate,higher dimension data,incorporate contextual aspects,log data,Multidimensional,multidimensional data,OLAP Cube data model,OLAP-based data analysis techniques,Security Analytics,Statistical Test,UBAD,user anomaly detection,User Behavior Anomaly Detection,user behavioural patterns},
  pages = {3-10},
  file = {/home/yuri/Zotero/storage/9TBTHZSA/Prarthana and Gangadhar - 2017 - User Behaviour Anomaly Detection in Multidimension.pdf;/home/yuri/Zotero/storage/S68M6X94/8332549.html}
}
% == BibTeX quality report for prarthanaUserBehaviourAnomaly2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@misc{RDFDataCube,
  title = {The {{RDF Data Cube Vocabulary}}},
  howpublished = {https://www.w3.org/TR/vocab-data-cube/},
  file = {/home/yuri/Zotero/storage/RVYN5IYA/vocab-data-cube.html}
}
% == BibTeX quality report for RDFDataCube:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{meimarisComputationalMethodsOptimizations2018,
  title = {Computational Methods and Optimizations for Containment and Complementarity in Web Data Cubes},
  volume = {75},
  issn = {0306-4379},
  abstract = {The increasing availability of diverse multidimensional data on the web has led to the creation and adoption of common vocabularies and practices that facilitate sharing, aggregating and reusing data from remote origins. One prominent example in the Web of Data is the RDF Data Cube vocabulary, which has recently attracted great attention from the industrial, government and academic sectors as the de facto representational model for publishing open multidimensional data. As a result, different datasets share terms from common code lists and hierarchies, this way creating an implicit relatedness between independent sources. Identifying and analyzing relationships between disparate data sources is a major prerequisite for enabling traditional business analytics at the web scale. However, discovery of instance-level relationships between datasets becomes a computationally costly procedure, as typically all pairs of records must be compared. In this paper, we define three types of relationships between multidimensional observations, namely full containment, partial containment and complementarity, and we propose four methods for efficient and scalable computation of these relationships. We conduct an extensive experimental evaluation over both real and synthetic datasets, comparing with traditional query-based and inference-based alternatives, and we show how our methods provide efficient and scalable solutions.},
  journal = {Information Systems},
  doi = {10/gdhvnn},
  author = {Meimaris, Marios and Papastefanatos, George and Vassiliadis, Panos and Anagnostopoulos, Ioannis},
  month = jun,
  year = {2018},
  keywords = {Elsevier,LaTeX,Template},
  pages = {56-74},
  file = {/home/yuri/Zotero/storage/3ESXHJJU/Meimaris et al. - 2018 - Computational methods and optimizations for contai.pdf;/home/yuri/Zotero/storage/HWCFJYH9/S030643791730488X.html}
}
% == BibTeX quality report for meimarisComputationalMethodsOptimizations2018:
% Missing required field 'number'

@incollection{lauriniGeographicKnowledgeDiscovery2017,
  title = {9 - {{Geographic Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-78548-243-4},
  abstract = {Where does knowledge come from? This is a very old question. One clue lies perhaps within databases under the assumption that knowledge is hidden in data as presented in Principle \#1. Like miners trying to extract gold nuggets, computer scientists have developed theories and techniques to search databases in order to extract knowledge. Under the name of knowledge discovery in databases (KDD), these issues are now becoming very popular in a branch of IT called big data.},
  booktitle = {Geographic {{Knowledge Infrastructure}}},
  publisher = {{Elsevier}},
  author = {Laurini, Robert},
  editor = {Laurini, Robert},
  month = jan,
  year = {2017},
  keywords = {Association rules,Co-location patterns,Data Mining,Geographic Knowledge Discovery,KDD process,Non-spatial databases,Spatial data mining},
  pages = {183-194},
  file = {/home/yuri/Zotero/storage/T3IY8SNT/B9781785482434500098.html},
  doi = {10.1016/B978-1-78548-243-4.50009-8}
}
% == BibTeX quality report for lauriniGeographicKnowledgeDiscovery2017:
% ? Title looks like it was stored in title-case in Zotero

@incollection{venkateshanChapterInterpolation2014,
  address = {{Boston}},
  title = {Chapter 5 - {{Interpolation}}},
  isbn = {978-0-12-416702-5},
  booktitle = {Computational {{Methods}} in {{Engineering}}},
  publisher = {{Academic Press}},
  author = {Venkateshan, S. P. and Swaminathan, Prasanna},
  editor = {Venkateshan, S. P. and Swaminathan, Prasanna},
  month = jan,
  year = {2014},
  pages = {213-254},
  file = {/home/yuri/Zotero/storage/8CP8NFV7/B9780124167025500053.html},
  doi = {10.1016/B978-0-12-416702-5.50005-3}
}
% == BibTeX quality report for venkateshanChapterInterpolation2014:
% ? Title looks like it was stored in title-case in Zotero

@article{liGeospatialBigData2016,
  series = {Theme Issue '{{State}}-of-the-Art in Photogrammetry, Remote Sensing and Spatial Information Science'},
  title = {Geospatial Big Data Handling Theory and Methods: {{A}} Review and Research Challenges},
  volume = {115},
  issn = {0924-2716},
  shorttitle = {Geospatial Big Data Handling Theory and Methods},
  abstract = {Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  doi = {10/f3pnxf},
  author = {Li, Songnian and Dragicevic, Suzana and Castro, Francesc Ant{\'o}n and Sester, Monika and Winter, Stephan and Coltekin, Arzu and Pettit, Christopher and Jiang, Bin and Haworth, James and Stein, Alfred and Cheng, Tao},
  month = may,
  year = {2016},
  keywords = {Analytics,Big data,Data handling,Geospatial,Review,Spatial modeling},
  pages = {119-133},
  file = {/home/yuri/Zotero/storage/3BCQGJKV/Li et al. - 2016 - Geospatial big data handling theory and methods A.pdf;/home/yuri/Zotero/storage/JCLILQ6L/S0924271615002439.html}
}
% == BibTeX quality report for liGeospatialBigData2016:
% Missing required field 'number'

@article{wangPipsCloudHighPerformance2018,
  title = {{{pipsCloud}}: {{High}} Performance Cloud Computing for Remote Sensing Big Data Management and Processing},
  volume = {78},
  issn = {0167-739X},
  shorttitle = {{{pipsCloud}}},
  abstract = {Massive, large-region coverage, multi-temporal, multi-spectral remote sensing (RS) datasets are employed widely due to the increasing requirements for accurate and up-to-date information about resources and the environment for regional and global monitoring. In general, RS data processing involves a complex multi-stage processing sequence, which comprises several independent processing steps according to the type of RS application. RS data processing for regional environmental and disaster monitoring is recognized as being computationally intensive and data intensive. We propose pipsCloud to address these issues in an efficient manner, which combines recent cloud computing and HPC techniques to obtain a large-scale RS data processing system that is suitable for on-demand real-time services. Due to the ubiquity, elasticity, and high-level transparency of the cloud computing model, massive RS data management and data processing for dynamic environmental monitoring can all be performed on the cloud via Web interfaces. A Hilbert-R+-based data indexing method is employed for the optimal querying and access of RS images, RS data products, and interim data. In the core platform beneath the cloud services, we provide a parallel file system for massive high-dimensional RS data, as well as interfaces for accessing irregular RS data to improve data locality and optimize the I/O performance. Moreover, we use an adaptive RS data analysis workflow management system for on-demand workflow construction and the collaborative processing of a distributed complex chain of RS data, e.g., for forest fire detection, mineral resources detection, and coastline monitoring. Our experimental analysis demonstrated the efficiency of the pipsCloud platform.},
  journal = {Future Generation Computer Systems},
  doi = {10/gdtgxt},
  author = {Wang, Lizhe and Ma, Yan and Yan, Jining and Chang, Victor and Zomaya, Albert Y.},
  month = jan,
  year = {2018},
  keywords = {Remote sensing,Big data,Cloud computing,Data-intensive computing,High performance computing},
  pages = {353-368},
  file = {/home/yuri/Zotero/storage/UGHRZ7EK/Wang et al. - 2018 - pipsCloud High performance cloud computing for re.pdf;/home/yuri/Zotero/storage/XNFVH2Y8/S0167739X16301923.html}
}
% == BibTeX quality report for wangPipsCloudHighPerformance2018:
% Missing required field 'number'

@article{casuBigRemotelySensed2017,
  series = {Big {{Remotely Sensed Data}}: Tools, Applications and Experiences},
  title = {Big {{Remotely Sensed Data}}: Tools, Applications and Experiences},
  volume = {202},
  issn = {0034-4257},
  shorttitle = {Big {{Remotely Sensed Data}}},
  journal = {Remote Sensing of Environment},
  doi = {10/gdtgxs},
  author = {Casu, F. and Manunta, M. and Agram, P. S. and Crippen, R. E.},
  month = dec,
  year = {2017},
  keywords = {Big Data},
  pages = {1-2},
  file = {/home/yuri/Zotero/storage/7ZLWR288/Casu et al. - 2017 - Big Remotely Sensed Data tools, applications and .pdf;/home/yuri/Zotero/storage/XV6IZNGC/S003442571730425X.html}
}
% == BibTeX quality report for casuBigRemotelySensed2017:
% Missing required field 'number'

@inproceedings{boussoufBigDataBased2018,
  title = {Big {{Data Based Operations}} for {{Space Systems}}},
  booktitle = {2018 {{SpaceOps Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Boussouf, Loic and Bergelin, Baptiste and Scudeler, David and Graydon, Henry and Stamminger, Johannes and Rosnet, Patrice and Taillefer, Emilie and Barreyre, Cl{\'e}mentine},
  year = {2018},
  file = {/home/yuri/Zotero/storage/JBVCSHRF/Boussouf et al. - Big Data Based Operations for Space Systems.pdf;/home/yuri/Zotero/storage/92UNK8GG/6.html},
  doi = {10.2514/6.2018-2506}
}
% == BibTeX quality report for boussoufBigDataBased2018:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@incollection{fischerPreparingFutureMission2012,
  title = {Preparing {{Future Mission Data Systems}} for {{Secure Space Communications}}},
  booktitle = {{{SpaceOps}} 2012 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Fischer, Daniel},
  year = {2012},
  file = {/home/yuri/Zotero/storage/7Q4992ES/Fischer - Preparing Future Mission Data Systems for Secure S.pdf;/home/yuri/Zotero/storage/KU8DJV3T/6.html},
  doi = {10.2514/6.2012-1279125}
}
% == BibTeX quality report for fischerPreparingFutureMission2012:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@incollection{friendBigMissionsSmall2016,
  title = {Big {{Missions}}, {{Small Solutions Advances}} and {{Innovation}} in {{Architecture}} and {{Technology}} for {{Small Satellites}}},
  booktitle = {{{AIAA SPACE}} 2016},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Friend, Robert B. and Arroyo, Chris and Hansen, Joseph},
  year = {2016},
  file = {/home/yuri/Zotero/storage/CYK3VI3N/6.html},
  doi = {10.2514/6.2016-5229}
}
% == BibTeX quality report for friendBigMissionsSmall2016:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@incollection{holzhauerProtectingMissionData2012,
  title = {Protecting Mission Data against Loss},
  booktitle = {{{SpaceOps}} 2012 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Holzhauer, Bernd},
  year = {2012},
  file = {/home/yuri/Zotero/storage/DYN7MMAA/Holzhauer - Protecting mission data against loss.pdf;/home/yuri/Zotero/storage/J2AFNFUV/6.html},
  doi = {10.2514/6.2012-1263870}
}
% == BibTeX quality report for holzhauerProtectingMissionData2012:
% Missing required field 'pages'

@incollection{luqueSAGExampleGeneric2014,
  title = {{{SAG}} - {{Example}} of a Generic Data Hosting and Processing Platform for {{Space}} Operations},
  booktitle = {{{SpaceOps}} 2014 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Luque, Santiago Pe{\~n}a},
  year = {2014},
  file = {/home/yuri/Zotero/storage/EHERVG5T/Luque - SAG - Example of a generic data hosting and proces.pdf;/home/yuri/Zotero/storage/EK4JUUKJ/6.html},
  doi = {10.2514/6.2014-1763}
}
% == BibTeX quality report for luqueSAGExampleGeneric2014:
% Missing required field 'pages'

@incollection{martinez-herasDataMiningService2006,
  title = {Data {{Mining}} at the {{Service}} of {{Space Operations}}},
  booktitle = {{{SpaceOps}} 2006 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {{Martinez-Heras}, Jose and Donati, Alessandro},
  year = {2006},
  file = {/home/yuri/Zotero/storage/83XI4LMM/Martinez-Heras and Donati - Data Mining at the Service of Space Operations.pdf;/home/yuri/Zotero/storage/MD5C476I/6.html},
  doi = {10.2514/6.2006-5717}
}
% == BibTeX quality report for martinez-herasDataMiningService2006:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@incollection{mcgregorMethodIntegrationRealtime2017,
  title = {A {{Method}} for the {{Integration}} of {{Real}}-Time {{Probabilistic Approaches}} for {{Astronaut Wellness}} in {{Human}} in the {{Loop Related Missions}} and {{Situations}} with {{Big Data Analytics}}},
  booktitle = {19th {{AIAA Non}}-{{Deterministic Approaches Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {McGregor, Carolyn and Orlov, Oleg and Baevsky, Roman and Chernikova, Anna and Rusanov, Vasiliy},
  year = {2017},
  file = {/home/yuri/Zotero/storage/MNIAN5BK/6.html},
  doi = {10.2514/6.2017-1096}
}
% == BibTeX quality report for mcgregorMethodIntegrationRealtime2017:
% Missing required field 'pages'

@incollection{picartAutomaticCheckingPROTEUS2010,
  title = {Automatic {{Checking}} of the {{PROTEUS Space Fleet Data Bases}} with {{Respect}} to {{Their Specifications}}},
  booktitle = {{{SpaceOps}} 2010 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Picart, Gilles and Smith, Martyn},
  year = {2010},
  file = {/home/yuri/Zotero/storage/PESX4KF7/Picart and Smith - Automatic Checking of the PROTEUS Space Fleet Data.pdf;/home/yuri/Zotero/storage/ZD4Z4H29/6.html},
  doi = {10.2514/6.2010-2001}
}
% == BibTeX quality report for picartAutomaticCheckingPROTEUS2010:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@incollection{quSpaceMissionData2018,
  title = {Space {{Mission Data Provenance Traceability}}},
  booktitle = {2018 {{SpaceOps Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Qu, Yi and Wu, Haitao and Liu, Ting and Zhao, Yue},
  year = {2018},
  file = {/home/yuri/Zotero/storage/SM22GKSP/Qu et al. - Space Mission Data Provenance Traceability.pdf;/home/yuri/Zotero/storage/GNBJRYAV/6.html},
  doi = {10.2514/6.2018-2482}
}
% == BibTeX quality report for quSpaceMissionData2018:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@incollection{santosHowUseBig2014,
  title = {How the Use of ``{{Big Data}}'' Clusters Improves off-Line Data Analysis and Operations},
  booktitle = {{{SpaceOps}} 2014 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Santos, Rui},
  year = {2014},
  file = {/home/yuri/Zotero/storage/WFKA69LE/Santos - How the use of âBig Dataâ clusters improves off-li.pdf;/home/yuri/Zotero/storage/73XZEPY3/6.html},
  doi = {10.2514/6.2014-1735}
}
% == BibTeX quality report for santosHowUseBig2014:
% Missing required field 'pages'

@incollection{sarkaratiWhenSpaceGets2012,
  title = {When the {{Space Gets Cloudy}}: {{A Systematic Assessment}} of {{Cloud Computing Application Domains}} for {{ESA Ground Data Systems}}},
  shorttitle = {When the {{Space Gets Cloudy}}},
  booktitle = {{{SpaceOps}} 2012 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Sarkarati, Mehran},
  year = {2012},
  file = {/home/yuri/Zotero/storage/HA9A6W5S/Sarkarati - When the Space Gets Cloudy A Systematic Assessmen.pdf;/home/yuri/Zotero/storage/T2SAHEUX/6.html},
  doi = {10.2514/6.2012-1289207}
}
% == BibTeX quality report for sarkaratiWhenSpaceGets2012:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{delpeyratStudentRocketSERA32018,
  title = {Student {{Rocket SERA}}-3 {{Flight Data Analysis}}},
  booktitle = {2018 {{SpaceOps Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Delpeyrat, Julien and Kassarian, Ervan},
  year = {2018},
  file = {/home/yuri/Zotero/storage/7374D7WK/Delpeyrat and Kassarian - Student Rocket SERA-3 Flight Data Analysis.pdf;/home/yuri/Zotero/storage/54XX4B9I/6.html},
  doi = {10.2514/6.2018-2382}
}
% == BibTeX quality report for delpeyratStudentRocketSERA32018:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@incollection{ImportanceInternationalCooperation,
  title = {The Importance of International Cooperation in Building National Space Data Infrastructure in All Countries},
  booktitle = {57th {{International Astronautical Congress}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  file = {/home/yuri/Zotero/storage/BGDNSPMW/6.IAC-06-E6.3.html},
  doi = {10.2514/6.IAC-06-E6.3.06}
}
% == BibTeX quality report for ImportanceInternationalCooperation:
% Missing required field 'author'
% Missing required field 'pages'
% Missing required field 'year'

@incollection{danielsCollectionAnalysesSpace,
  title = {Collection and Analyses of {{Space Shuttle Orbiter}} Historical Data},
  booktitle = {4th {{Space Logistics Symposium}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {DANIELS, M. and BROOKS, L.},
  file = {/home/yuri/Zotero/storage/ILMW4BUG/6.html},
  doi = {10.2514/6.1991-4122}
}
% == BibTeX quality report for danielsCollectionAnalysesSpace:
% Missing required field 'pages'
% Missing required field 'year'

@incollection{EarthStationsVery2008,
  title = {Earth {{Stations}}: {{From Very Big}} to {{Very Small}}},
  isbn = {978-1-56347-966-3},
  shorttitle = {Earth {{Stations}}},
  booktitle = {Success {{Stories}} in {{Satellite Systems}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  year = {2008},
  pages = {329-351},
  file = {/home/yuri/Zotero/storage/JJBT9BE7/5.9781563479670.0329.html},
  doi = {10.2514/5.9781563479670.0329.0351}
}
% == BibTeX quality report for EarthStationsVery2008:
% Missing required field 'author'
% ? Title looks like it was stored in title-case in Zotero

@article{kuijpersAlgebraOLAP2017,
  title = {An Algebra for {{OLAP}}},
  volume = {21},
  issn = {1088-467X},
  abstract = {Online Analytical Processing (OLAP) comprises tools and algorithms that allow querying multidimensional databases. It is based on the multidimensional model, where data can be seen as a cube, where each cell contains one or more measures can be aggre},
  language = {en},
  number = {5},
  journal = {Intelligent Data Analysis},
  doi = {10/gcnqms},
  author = {Kuijpers, Bart and Vaisman, Alejandro},
  month = jan,
  year = {2017},
  pages = {1267-1300},
  file = {/home/yuri/Zotero/storage/BV9ZCKE5/ida163161.html}
}

@article{pahinsHashedcubesSimpleLow2017,
  title = {Hashedcubes: {{Simple}}, {{Low Memory}}, {{Real}}-{{Time Visual Exploration}} of {{Big Data}}},
  volume = {23},
  issn = {1077-2626},
  shorttitle = {Hashedcubes},
  abstract = {We propose Hashedcubes, a data structure that enables real-time visual exploration of large datasets that improves the state of the art by virtue of its low memory requirements, low query latencies, and implementation simplicity. In some instances, Hashedcubes notably requires two orders of magnitude less space than recent data cube visualization proposals. In this paper, we describe the algorithms to build and query Hashedcubes, and how it can drive well-known interactive visualizations such as binned scatterplots, linked histograms and heatmaps. We report memory usage, build time and query latencies for a variety of synthetic and real-world datasets, and find that although sometimes Hashedcubes offers slightly slower querying times to the state of the art, the typical query is answered fast enough to easily sustain a interaction. In datasets with hundreds of millions of elements, only about 2\% of the queries take longer than 40ms. Finally, we discuss the limitations of data structure, potential spacetime tradeoffs, and future research directions.},
  number = {1},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10/gdrbsx},
  author = {Pahins, C. A. L. and Stephens, S. A. and Scheidegger, C. and Comba, J. L. D.},
  month = jan,
  year = {2017},
  keywords = {data cube,data structures,Big Data,Arrays,data visualisation,Data visualization,Visualization,query processing,multidimensional data,Big data,binned scatterplots,data cube visualization,data structure,Hashedcube query,heatmaps,interactive exploration,interactive systems,interactive visualizations,linked histograms,low memory real-time visual exploration,Memory management,memory requirements,query latencies,querying times,real-world datasets,Scalability,Sorting,synthetic datasets},
  pages = {671-680},
  file = {/home/yuri/Zotero/storage/6FK6ITWP/Pahins et al. - 2017 - Hashedcubes Simple, Low Memory, Real-Time Visual .pdf;/home/yuri/Zotero/storage/4HMP79WU/7539326.html}
}
% == BibTeX quality report for pahinsHashedcubesSimpleLow2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{djedainiDetectingUserFocus2017,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Detecting {{User Focus}} in {{OLAP Analyses}}},
  isbn = {978-3-319-66916-8 978-3-319-66917-5},
  abstract = {In this paper, we propose an approach to automatically detect focused portions of data cube explorations by using different features of OLAP queries. While such a concept of focused interaction is relevant to many domains besides OLAP explorations, like web search or interactive database exploration, there is currently no precise formal, commonly agreed definition. This concept of focus phase is drawn from Exploratory Search, which is a paradigm that theorized search as a complex interaction between a user and a system. The interaction consists of two different phases: an exploratory phase where the user is progressively defining her information need, and a focused phase where she investigates in details precise facts, and learn from this investigation. Following this model, our work is, to the best of our knowledge, the first to propose a formal feature-based description of a focused query in the context of interactive data exploration. Our experiments show that we manage to identify focused queries in real navigations, and that our model is sufficiently robust and general to be applied to different OLAP navigations datasets.},
  language = {en},
  booktitle = {Advances in {{Databases}} and {{Information Systems}}},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-66917-5_8},
  author = {Djedaini, Mahfoud and Labroche, Nicolas and Marcel, Patrick and Peralta, Ver{\'o}nika},
  month = sep,
  year = {2017},
  pages = {105-119},
  file = {/home/yuri/Zotero/storage/YX7TKRM2/10.html}
}
% == BibTeX quality report for djedainiDetectingUserFocus2017:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{campbellMultidimensionalBenchmarkingData2017,
  title = {Multidimensional Benchmarking in Data Warehouses},
  volume = {21},
  issn = {1088-467X},
  abstract = {Benchmarking is among the most widely adopted practices in business today. However, to the best of our knowledge, conducting multidimensional benchmarking in data warehouses has not been explored from a technical efficiency perspective. In this paper},
  language = {en},
  number = {4},
  journal = {Intelligent Data Analysis},
  doi = {10/gcgg9w},
  author = {Campbell, Akiko and Mao, Xiangbo and Pei, Jian and {Al-Barakati}, Abdullah},
  month = jan,
  year = {2017},
  pages = {781-801},
  file = {/home/yuri/Zotero/storage/589UMQUX/ida160035.html}
}

@incollection{hennionBigdataSatelliteYearly2018,
  title = {Big-Data for Satellite Yearly Reports Generation},
  booktitle = {2018 {{SpaceOps Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Hennion, Nicolas},
  year = {2018},
  file = {/home/yuri/Zotero/storage/XYGPYA9E/Hennion_2018_Big-data for satellite yearly reports generation.pdf},
  doi = {10.2514/6.2018-2368}
}
% == BibTeX quality report for hennionBigdataSatelliteYearly2018:
% Missing required field 'pages'

@inproceedings{oliveiraDataCubeTyped2017,
  address = {{New York, NY, USA}},
  series = {{{DBPL}} '17},
  title = {The {{Data Cube As}} a {{Typed Linear Algebra Operator}}},
  isbn = {978-1-4503-5354-0},
  abstract = {There is a need for a typed notation for linear algebra applicable to the fields of econometrics and data mining. In this paper we show that such a notation exists and can be useful in formalizing and reasoning about data aggregation operations. One such operation - the construction of a data cube - is shown to be easily expressible as a linear algebra operator. The construction is shown to be type-generic and some of its properties are derived from its typed definition and proved using matrix algebra. Other forms of data aggregation such as eg. rollup and cross tabulation are shown to be algebraically derivable from data cubes.},
  booktitle = {Proceedings of {{The}} 16th {{International Symposium}} on {{Database Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/3122831.3122834},
  author = {Oliveira, J. N. and Macedo, H. D.},
  year = {2017},
  pages = {6:1--6:11},
  file = {/home/yuri/Zotero/storage/M56KRJYI/Oliveira and Macedo - 2017 - The Data Cube As a Typed Linear Algebra Operator.pdf}
}
% == BibTeX quality report for oliveiraDataCubeTyped2017:
% ? Title looks like it was stored in title-case in Zotero

@article{songMultidimensionalVisualizationTransit2018,
  title = {Multidimensional Visualization of Transit Smartcard Data Using Space\textendash{}Time Plots and Data Cubes},
  volume = {45},
  issn = {0049-4488, 1572-9435},
  abstract = {Given the wide application of automatic fare collection systems in transit systems across the globe, smartcard data with on- and/or off-boarding information has become a new source of data to understand passenger flow patterns. This paper uses Nanjing, China as a case study and examines the possibility of using the data cube technique in data mining to understand space\textendash{}time travel patterns of Nanjing rail transit users. One month of smartcard data in October, 2013 was obtained from Nanjing rail transit system, with a total of over 22 million transaction records. We define the original data cube for the smartcard data based on four dimensions\textemdash{}Space, Date, Time, and User, design a hierarchy for each dimension, and use the total number of transactions as the quantitative measure. We develop modules using the programming language Python and share them as open-source on GitHub to enable peer production and advancement in the field. The visualizations of two-dimensional slices of the data cube show some interesting patterns such as different travel behaviors across user groups (e.g. students vs. elders), and irregular peak hours during National Holiday (October 1st\textendash{}7th) compared to regular morning and afternoon peak hours during regular working weeks. Spatially, multidimensional visualizations show concentrations of various activity opportunities near metro rail stations and the changing popularities of rail stations through time accordingly. These findings support the feasibility and efficiency of the data cube technique as a mean of visual exploratory analysis for massive smart-card data, and can contribute to the evaluation and planning of public transit systems.},
  language = {en},
  number = {2},
  journal = {Transportation},
  doi = {10/gddgv6},
  author = {Song, Ying and Fan, Yingling and Li, Xin and Ji, Yanjie},
  month = mar,
  year = {2018},
  pages = {311-333},
  file = {/home/yuri/Zotero/storage/HQ5HAAP9/Song et al. - 2018 - Multidimensional visualization of transit smartcar.pdf}
}

@inproceedings{djirounDataCubeDesign2016,
  title = {A Data Cube Design and Construction Methodology Based on {{OLAP}} Queries},
  abstract = {Business Intelligence systems provide an effective solution for multidimensional online computing and analysis from large volumes of data. In the decision making process, analyzed data are typically stored in a set of cubes often heterogeneous. Most of the time, the structure of these cubes is unknown by decision makers. Our goal is to enable decision makers to express their need via a query in natural language. In this paper, we deal with the problem of data cube design and construction according to a user query where the expressed need is dispersed on more than one cube. We propose a cubes design and construction approach based on fusion of cubes containing a part of the user's need. We validate our approach via a tool, called ``Design-Cubes-Query'', that implements our approach and we show its use through a case study.},
  booktitle = {2016 {{IEEE}}/{{ACS}} 13th {{International Conference}} of {{Computer Systems}} and {{Applications}} ({{AICCSA}})},
  doi = {10.1109/AICCSA.2016.7945795},
  author = {Djiroun, R. and Boukhalfa, K. and Alimazighi, Z. and Atigui, F. and Bimonte, S.},
  month = nov,
  year = {2016},
  keywords = {Meteorology,data mining,OLAP,Aggregates,data cube design,query processing,Business intelligence,Business Intelligence,business intelligence systems,competitive intelligence,construction methodology,Cube Design,decision making,decision making process,design-cubes-query,Drill-Across,Fusion,Irrigation,Large scale integration,multidimensional online computing,Multidimentional Data Cubes,natural language,natural language processing,OLAP queries,Production,user query},
  pages = {1-8},
  file = {/home/yuri/Zotero/storage/MWN9QJH6/Djiroun et al. - 2016 - A data cube design and construction methodology ba.pdf;/home/yuri/Zotero/storage/CT5II9HJ/7945795.html}
}
% == BibTeX quality report for djirounDataCubeDesign2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{yangHolisticAlgebraicData2017,
  title = {Holistic and {{Algebraic Data Cube Computation Using MapReduce}}},
  volume = {2},
  abstract = {As an important method of accelerating large data analysis, data cube computing has been a hot area. At present, the researches based on MapReduce mainly focus on algebraic data cube. In this paper, we use MapReduce as the basis of the algorithm. For the algebraic data cube, data cube lattice is divided into several regions, according to different types of regions, different calculation algorithms are adopted. In this paper, for holistic measures, the data cube computation of Count Distinct measure by using Bitmap is proposed, and then the applicability of the algorithm to incremental computation and the combination with algebraic metric are discussed. The final experimental results show that the methods are more efficient and show good practicability.},
  booktitle = {2017 9th {{International Conference}} on {{Intelligent Human}}-{{Machine Systems}} and {{Cybernetics}} ({{IHMSC}})},
  doi = {10.1109/IHMSC.2017.126},
  author = {Yang, H. and Han, C.},
  month = aug,
  year = {2017},
  keywords = {Algorithm design and analysis,data analysis,data mining,OLAP,algebraic data cube computation,Atmospheric measurements,Big Data,Bitmap,count distinct measure,Current measurement,Data Cube Computation,data cube computing,data cube lattice,data handling,Distributed Computing,incremental computation,Lattices,MapReduce,mathematics computing,parallel programming,Particle measurements},
  pages = {47-50},
  file = {/home/yuri/Zotero/storage/A9KMPR3U/Yang and Han - 2017 - Holistic and Algebraic Data Cube Computation Using.pdf;/home/yuri/Zotero/storage/CXZCTVF7/Yang and Han - 2017 - Holistic and Algebraic Data Cube Computation Using.pdf;/home/yuri/Zotero/storage/JUJUZZPY/8048110.html}
}
% == BibTeX quality report for yangHolisticAlgebraicData2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{nataliEodataserviceOrgBig2017,
  title = {Eodataservice.Org: {{Big Data}} Platform to Enable Multi-Disciplinary Intelligent Planet Applications via Informatics},
  abstract = {We are currently living in a digital geospatial data world: everything is digitalized from the route we take to move, to the way we connect to the internet, weather information and so on. In this ``digitalization of the living environment'' artificial satellites have played a crucial role, and will play more in future: positioning, telecommunication, Earth observation \textendash{}based services are directly or indirectly involved in all steps of our daily life.},
  language = {en},
  author = {NATALI, Stefano and Srl, MEEO},
  year = {2017},
  keywords = {ðNo DOI found},
  pages = {5},
  file = {/home/yuri/Zotero/storage/6QU9VHB7/NATALI and Srl - 2017 - eodataservice.org Big Data platform to enable mul.pdf}
}
% == BibTeX quality report for nataliEodataserviceOrgBig2017:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'

@inproceedings{baumannStandardizingBigEarth2017,
  title = {Standardizing Big Earth Datacubes},
  abstract = {Geo data like satellite imagery and climate simulation output constitute major contributors to today's Big Data deluge. The datacube paradigm is currently being considered not only for getting data analysis ready, but also to offer advanced functionality in an easy-to-use way while allowing for highly effective server-side optimizations. Datacube standards can help substantially in unifying access protocols, thereby easing access for users and also paving the way for cross-data-center datacube fusion. We present OGC datacube model which is based on the coverage data and service model standardized by OGC, ISO, and INSPIRE. This paper aims at contributing to a cross-disciplinary exchange on modeling challenges, successes, and possible overlaps amenable to generalization and harmonization.},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10.1109/BigData.2017.8257912},
  author = {Baumann, P.},
  month = dec,
  year = {2017},
  keywords = {Meteorology,Internet,Earth,cloud computing,data analysis,standards,data mining,Big Data,access protocols,Big Data deluge,Big Earth Data,big earth datacubes standardization,climate,climate simulation output,coverage,coverage data,cross-data-center datacube fusion,cross-disciplinary exchange,datacube,datacube paradigm,geo data,geographic information systems,INSPIRE,ISO,ISO Standards,Metadata,modeling challenges,OGC,OGC datacube model,rasdaman,satellite imagery,server-side optimizations,service model,Solid modeling,WCS},
  pages = {67-73},
  file = {/home/yuri/Zotero/storage/H6Z724YT/Baumann - 2017 - Standardizing big earth datacubes.pdf;/home/yuri/Zotero/storage/MSH8ZBES/8257912.html}
}
% == BibTeX quality report for baumannStandardizingBigEarth2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{kantojarviImagingSpectralSignature2007,
  title = {Imaging Spectral Signature Satellite Instrument for the Real-Time Identification of Ground Scenes with a Dedicated Spectral Signature},
  volume = {6585},
  abstract = {With hyperspectral pushbroom imaging spectrometers on Earth observation satellites it is possible to detect and identify dedicated ground pixels by their spectral signature. Conventional time consuming on-ground processing performs this selection by processing the measured hyperspectral data cube of the image. The Imaging Spectral Signature Instrument (ISSI) concept combines an optical on-board processing of the hyperspectral data cube with a thresholding algorithm, to identify pixels with a pre-defined and programmable spectral signature, such as water, forest and minerals, in the ground swath. The Imaging Spectral Signature Instrument consists of an imaging telescope, which images an object line on the entrance slit of a first imaging spectrometer, which disperses each pixel of the object line into its spectral content and images the hyperspectral image on the spatial light modulator. This spatial light modulator will be programmed with a spatial transmission or reflection behavior, which is constant along the spatial pixels and along the spectral pixels identical to a filter vector that corresponds to the spectral signature of the searched specific feature. A second inverted spectrometer reimages the by the first spectrometer dispersed and by the spatial light modulator transmitted or reflected flux into a line of pixels. In case the spectral content of the ground scene is identical to the searched signature, the flux traversing or reflecting the spatial light modulator will be maximum. The related pixel can be identified in the final image as a high signal by a threshold discriminator. A component test setup consists of an imaging lens, two Imspector\texttrademark{} spectrographs, a spatial light modulator, which is a programmable transmissible liquid crystal display and a CCD sensor as a detector. A mathematical model was developed for the instrument and its performance was evaluated in order to compare different concept variations. All components were measured and characterized individually, and the results were used in the simulations. Performance was then analyzed by means of radiometric throughput and spatial and spectral resolutions. The simulations were performed at wavelengths of 450 nm to 900 nm. The throughput was found to be between 1\% and 4.5\%.},
  booktitle = {Optical {{Sensing Technology}} and {{Applications}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.723557},
  author = {Kantoj{\"a}rvi, Uula and Saari, Heikki and Viherkanto, Kai and Herrala, Esko and Harnisch, Bernd},
  month = may,
  year = {2007},
  pages = {65850S},
  file = {/home/yuri/Zotero/storage/UCA3G7N2/KantojÃ¤rvi et al. - 2007 - Imaging spectral signature satellite instrument fo.pdf;/home/yuri/Zotero/storage/HITGQ6DY/12.723557.html}
}
% == BibTeX quality report for kantojarviImagingSpectralSignature2007:
% ? Unsure about the formatting of the booktitle

@inproceedings{jungicDeployingEfficientBusiness2003,
  title = {Deploying Efficient Business Processes the Way for Small Telecom Operators to Survive},
  volume = {1},
  abstract = {Complex and dynamic environment of telecom operators and their efforts to strive for in the fierce competition foster small operators to change their business strategy and to enhance their processes to be more efficient then their competitors. The paper proposes deployment of e-TOM processes as guidelines for planning reengineering of business processes. The new e-TOM provides more details on fulfillments, assurance and billing processes used as basic processes for many operators and service providers. The concept of customer relationship management (CRM), as strategic issues, facilitates introduction of new business process supporting operator's holistic approach to manage relationships with customers. Support of suitable data warehouse with the use of online analytical processing - OLAP and data mining techniques is required and discussed in this paper.},
  booktitle = {6th {{International Conference}} on {{Telecommunications}} in {{Modern Satellite}}, {{Cable}} and {{Broadcasting Service}}, 2003. TEL{{SIKS}} 2003.},
  doi = {10.1109/\%0021SKS.2003.1246175},
  author = {Jungic, Z. and Gospic, N.},
  month = oct,
  year = {2003},
  keywords = {Data mining,data mining,data warehouses,OLAP,online analytical processing,Production,business process deployment,business process re-engineering,Business process re-engineering,Companies,CRM,customer relationship management,Customer relationship management,data mining techniques,data warehouse,Data warehouses,e-TOM process,Guidelines,Process planning,Shape,telecom operators,telecommunication,Telecommunications},
  pages = {18-21 vol.1},
  file = {/home/yuri/Zotero/storage/84FU9GMF/Jungic and Gospic - 2003 - Deploying efficient business processes the way for.pdf;/home/yuri/Zotero/storage/LYUFV8Q9/1246175.html}
}
% == BibTeX quality report for jungicDeployingEfficientBusiness2003:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{lewisAustralianGeoscienceData2017,
  series = {Big {{Remotely Sensed Data}}: Tools, Applications and Experiences},
  title = {The {{Australian Geoscience Data Cube}} \textemdash{} {{Foundations}} and Lessons Learned},
  volume = {202},
  issn = {0034-4257},
  abstract = {The Australian Geoscience Data Cube (AGDC) aims to realise the full potential of Earth observation data holdings by addressing the Big Data challenges of volume, velocity, and variety that otherwise limit the usefulness of Earth observation data. There have been several iterations and AGDC version 2 is a major advance on previous work. The foundations and core components of the AGDC are: (1) data preparation, including geometric and radiometric corrections to Earth observation data to produce standardised surface reflectance measurements that support time-series analysis, and collection management systems which track the provenance of each Data Cube product and formalise re-processing decisions; (2) the software environment used to manage and interact with the data; and (3) the supporting high performance computing environment provided by the Australian National Computational Infrastructure (NCI). A growing number of examples demonstrate that our data cube approach allows analysts to extract rich new information from Earth observation time series, including through new methods that draw on the full spatial and temporal coverage of the Earth observation archives. To enable easy-uptake of the AGDC, and to facilitate future cooperative development, our code is developed under an open-source, Apache License, Version 2.0. This open-source approach is enabling other organisations, including the Committee on Earth Observing Satellites (CEOS), to explore the use of similar data cubes in developing countries.},
  journal = {Remote Sensing of Environment},
  doi = {10/gcr3rp},
  author = {Lewis, Adam and Oliver, Simon and Lymburner, Leo and Evans, Ben and Wyborn, Lesley and Mueller, Norman and Raevksi, Gregory and Hooke, Jeremy and Woodcock, Rob and Sixsmith, Joshua and Wu, Wenjun and Tan, Peter and Li, Fuqin and Killough, Brian and Minchin, Stuart and Roberts, Dale and Ayers, Damien and Bala, Biswajit and Dwyer, John and Dekker, Arnold and Dhu, Trevor and Hicks, Andrew and Ip, Alex and Purss, Matt and Richards, Clare and Sagar, Stephen and Trenham, Claire and Wang, Peter and Wang, Lan-Wei},
  month = dec,
  year = {2017},
  keywords = {Landsat,Big data,High performance computing,Australian Geoscience Data Cube,Collection management,Data cube,Geometric correction,High performance data,Pixel quality,Time-series},
  pages = {276-292},
  file = {/home/yuri/Zotero/storage/VK8I3FVH/Lewis et al. - 2017 - The Australian Geoscience Data Cube â Foundations .pdf;/home/yuri/Zotero/storage/8CKMNGTN/S0034425717301086.html}
}
% == BibTeX quality report for lewisAustralianGeoscienceData2017:
% Missing required field 'number'

@inproceedings{zhouPreparationAnalysisReady2017,
  title = {Preparation of Analysis Ready Polsar Data for the {{Australian Geoscience Data Cube}}},
  abstract = {This paper describes a CSIRO investigation into the potential use of Sentinel-1 (S1) radar data with the Australian Geosciences Data Cube (AGDC) and in particular the necessity of polarimetric analysis of S1 dual-polarization data within AGDC. As a collaboration between Geoscience Australia, CSIRO and the National Computational Infrastructure, the AGDC provides a new collaborative information technology environment which combines an image database, high performance computing resources, software Application Programming Interfaces (API) to popular analysis tools and libraries, as well as an environment for custom algorithm development. It was created with the view to analysing the increasingly vast quantities of satellite imagery and other Earth observations. With the increasing availability of Earth observation data, especially freely accessed, globally covered and frequently revisited Copernicus Program data, the AGDC provides a potential resource for the analysis and processing of these data, especially where joint analysis with other archives is required. Here we investigate the preparation and necessity of including Sentinal-1 dual-pol SAR data polarimetric processing for the purpose of ingestion into the AGDC.},
  booktitle = {2017 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  doi = {10.1109/IGARSS.2017.8128180},
  author = {Zhou, Z. S. and Caccetta, P. and Devereux, D. and Caccetta, M. and Woodcock, R. and Paget, M. and Held, A.},
  month = jul,
  year = {2017},
  keywords = {Monitoring,Indexes,Irrigation,Australian Geoscience Data Cube,AGDC,analysis ready polsar data,application program interfaces,ARD,collaborative information technology environment,Copernicus Program data,CSIRO investigation,CVA,Decomposition,dual-pol SAR data polarimetric processing,Earth observation data,Geology,geophysical image processing,high performance computing resources,National Computational Infrastructure,polarimetric analysis,POLSAR,popular analysis tools,radar imaging,radar polarimetry,remote sensing by radar,S1 dual-polarization data,Sentinel-1,Sentinel-1 radar data,software Application Programming Interfaces,synthetic aperture radar},
  pages = {5229-5232},
  file = {/home/yuri/Zotero/storage/BPAVN3EZ/Zhou et al. - 2017 - Preparation of analysis ready polsar data for the .pdf;/home/yuri/Zotero/storage/6ZENERYA/8128180.html}
}
% == BibTeX quality report for zhouPreparationAnalysisReady2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{maRemoteSensingBig2015,
  series = {Special {{Section}}: {{A Note}} on {{New Trends}} in {{Data}}-{{Aware Scheduling}} and {{Resource Provisioning}} in {{Modern HPC Systems}}},
  title = {Remote Sensing Big Data Computing: {{Challenges}} and Opportunities},
  volume = {51},
  issn = {0167-739X},
  shorttitle = {Remote Sensing Big Data Computing},
  abstract = {As we have entered an era of high resolution earth observation, the RS data are undergoing an explosive growth. The proliferation of data also give rise to the increasing complexity of RS data, like the diversity and higher dimensionality characteristic of the data. RS data are regarded as RS ``Big Data''. Fortunately, we are witness the coming technological leapfrogging. In this paper, we give a brief overview on the Big Data and data-intensive problems, including the analysis of RS Big Data, Big Data challenges, current techniques and works for processing RS Big Data.},
  journal = {Future Generation Computer Systems},
  doi = {10/gdq98h},
  author = {Ma, Yan and Wu, Haiping and Wang, Lizhe and Huang, Bormin and Ranjan, Rajiv and Zomaya, Albert and Jie, Wei},
  month = oct,
  year = {2015},
  keywords = {Big data,Data-intensive computing,Remote sensing data processing},
  pages = {47-60},
  file = {/home/yuri/Zotero/storage/WNCIQ97D/Ma et al. - 2015 - Remote sensing big data computing Challenges and .pdf;/home/yuri/Zotero/storage/XR5N5YYZ/S0167739X14002234.html}
}
% == BibTeX quality report for maRemoteSensingBig2015:
% Missing required field 'number'

@incollection{bravoArchitectureColombianData2017,
  address = {{Cham}},
  title = {Architecture for a {{Colombian Data Cube Using Satellite Imagery}} for {{Environmental Applications}}},
  volume = {735},
  isbn = {978-3-319-66561-0 978-3-319-66562-7},
  booktitle = {Advances in {{Computing}}},
  publisher = {{Springer International Publishing}},
  author = {Bravo, Germ{\'a}n and Castro, Harold and Moreno, Andr{\'e}s and {Ariza-Porras}, Christian and Galindo, Gustavo and Cabrera, Edersson and Valbuena, Saralux and {Lozano-Rivera}, Pilar},
  editor = {Solano, Andr{\'e}s and Ordo{\~n}ez, Hugo},
  year = {2017},
  pages = {227-241},
  file = {/home/yuri/Zotero/storage/MMYH23JA/Bravo et al. - 2017 - Architecture for a Colombian Data Cube Using Satel.pdf},
  doi = {10.1007/978-3-319-66562-7_17}
}
% == BibTeX quality report for bravoArchitectureColombianData2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{adamskiDataAnalyticsLarge2016,
  series = {{{SpaceOps Conferences}}},
  title = {Data {{Analytics}} for {{Large Constellations}}},
  booktitle = {{{SpaceOps}} 2016 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Adamski, Greg},
  month = may,
  year = {2016},
  file = {/home/yuri/Zotero/storage/IDZB3X5U/Adamski - 2016 - Data Analytics for Large Constellations.pdf},
  doi = {10.2514/6.2016-2377}
}
% == BibTeX quality report for adamskiDataAnalyticsLarge2016:
% Missing required field 'pages'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{lhcbcollaborationSearchLeptonFlavour2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.8548},
  title = {Search for the Lepton Flavour Violating Decay \$\textbackslash{}tau\^-\textbackslash{}to \textbackslash{}mu\^-\textbackslash{}mu\^+\textbackslash{}mu\^-\$},
  volume = {2015},
  issn = {1029-8479},
  abstract = {A search for the lepton flavour violating decay \$\textbackslash{}tau\^-\textbackslash{}to \textbackslash{}mu\^-\textbackslash{}mu\^+\textbackslash{}mu\^-\$ is performed with the LHCb experiment. The data sample corresponds to an integrated luminosity of \$1.0\textbackslash{}mathrm\{\textbackslash,fb\}\^\{-1\}\$ of proton-proton collisions at a centre-of-mass energy of \$7\textbackslash{}mathrm\{\textbackslash,Te\textbackslash{}kern -0.1em V\}\$ and \$2.0\textbackslash{}mathrm\{\textbackslash,fb\}\^\{-1\}\$ at \$8\textbackslash{}mathrm\{\textbackslash,Te\textbackslash{}kern -0.1em V\}\$. No evidence is found for a signal, and a limit is set at \$90\textbackslash\%\$ confidence level on the branching fraction, \$\textbackslash{}mathcal\{B\}(\textbackslash{}tau\^-\textbackslash{}to \textbackslash{}mu\^-\textbackslash{}mu\^+\textbackslash{}mu\^-) {$<$} 4.6 \textbackslash{}times 10\^\{-8\}\$.},
  number = {2},
  journal = {Journal of High Energy Physics},
  doi = {10/gdqddb},
  author = {{LHCb collaboration} and Aaij, R. and Beteta, C. Abell{\'a}n and Adeva, B. and Adinolfi, M. and Affolder, A. and Ajaltouni, Z. and Akar, S. and Albrecht, J. and Alessio, F. and Alexander, M. and Ali, S. and Alkhazov, G. and Cartelle, P. Alvarez and Alves Jr, A. A. and Amato, S. and Amerio, S. and Amhis, Y. and An, L. and Anderlini, L. and Anderson, J. and Andreassen, R. and Andreotti, M. and Andrews, J. E. and Appleby, R. B. and Gutierrez, O. Aquines and Archilli, F. and Artamonov, A. and Artuso, M. and Aslanides, E. and Auriemma, G. and Baalouch, M. and Bachmann, S. and Back, J. J. and Badalov, A. and Baesso, C. and Baldini, W. and Barlow, R. J. and Barschel, C. and Barsuk, S. and Barter, W. and Batozskaya, V. and Battista, V. and Bay, A. and Beaucourt, L. and Beddow, J. and Bedeschi, F. and Bediaga, I. and Belogurov, S. and Belous, K. and Belyaev, I. and {Ben-Haim}, E. and Bencivenni, G. and Benson, S. and Benton, J. and Berezhnoy, A. and Bernet, R. and Bettler, M.-O. and {van Beuzekom}, M. and Bien, A. and Bifani, S. and Bird, T. and Bizzeti, A. and Bj{\o}rnstad, P. M. and Blake, T. and Blanc, F. and Blouw, J. and Blusk, S. and Bocci, V. and Bondar, A. and Bondar, N. and Bonivento, W. and Borghi, S. and Borgia, A. and Borsato, M. and Bowcock, T. J. V. and Bowen, E. and Bozzi, C. and Brambach, T. and Brett, D. and Britsch, M. and Britton, T. and Brodzicka, J. and Brook, N. H. and Brown, H. and Bursche, A. and Busetto, G. and Buytaert, J. and Cadeddu, S. and Calabrese, R. and Calvi, M. and Gomez, M. Calvo and Campana, P. and Perez, D. Campora and Carbone, A. and Carboni, G. and Cardinale, R. and Cardini, A. and Carson, L. and Akiba, K. Carvalho and Casse, G. and Cassina, L. and Garcia, L. Castillo and Cattaneo, M. and Cauet, Ch and Cenci, R. and Charles, M. and Charpentier, Ph and Chefdeville, M. and Chen, S. and Cheung, S.-F. and Chiapolini, N. and Chrzaszcz, M. and Vidal, X. Cid and Ciezarek, G. and Clarke, P. E. L. and Clemencic, M. and Cliff, H. V. and Closier, J. and Coco, V. and Cogan, J. and Cogneras, E. and Cogoni, V. and Cojocariu, L. and Collins, P. and {Comerma-Montells}, A. and Contu, A. and Cook, A. and Coombes, M. and Coquereau, S. and Corti, G. and Corvo, M. and Counts, I. and Couturier, B. and Cowan, G. A. and Craik, D. C. and Torres, M. Cruz and Cunliffe, S. and Currie, R. and D'Ambrosio, C. and Dalseno, J. and David, P. and David, P. N. Y. and Davis, A. and De Bruyn, K. and De Capua, S. and De Cian, M. and De Miranda, J. M. and De Paula, L. and De Silva, W. and De Simone, P. and Decamp, D. and Deckenhoff, M. and Del Buono, L. and D{\'e}l{\'e}age, N. and Derkach, D. and Deschamps, O. and Dettori, F. and Di Canto, A. and Dijkstra, H. and Donleavy, S. and Dordei, F. and Dorigo, M. and Su{\'a}rez, A. Dosil and Dossett, D. and Dovbnya, A. and Dreimanis, K. and Dujany, G. and Dupertuis, F. and Durante, P. and Dzhelyadin, R. and Dziurda, A. and Dzyuba, A. and Easo, S. and Egede, U. and Egorychev, V. and Eidelman, S. and Eisenhardt, S. and Eitschberger, U. and Ekelhof, R. and Eklund, L. and Rifai, I. El and Elena, E. and Elsasser, Ch and Ely, S. and Esen, S. and Evans, H.-M. and Evans, T. and Falabella, A. and F{\"a}rber, C. and Farinelli, C. and Farley, N. and Farry, S. and Fay, R. F. and Ferguson, D. and Albor, V. Fernandez and Rodrigues, F. Ferreira and {Ferro-Luzzi}, M. and Filippov, S. and Fiore, M. and Fiorini, M. and Firlej, M. and Fitzpatrick, C. and Fiutowski, T. and Fol, P. and Fontana, M. and Fontanelli, F. and Forty, R. and Francisco, O. and Frank, M. and Frei, C. and Frosini, M. and Fu, J. and Furfaro, E. and Torreira, A. Gallas and Galli, D. and Gallorini, S. and Gambetta, S. and Gandelman, M. and Gandini, P. and Gao, Y. and Pardi{\~n}as, J. Garc{\'i}a and Garofoli, J. and Tico, J. Garra and Garrido, L. and Gaspar, C. and Gauld, R. and Gavardi, L. and Gavrilov, G. and Geraci, A. and Gersabeck, E. and Gersabeck, M. and Gershon, T. and Ghez, Ph and Gianelle, A. and Gian{\`i}, S. and Gibson, V. and Giubega, L. and Gligorov, V. V. and G{\"o}bel, C. and Golubkov, D. and Golutvin, A. and Gomes, A. and Gotti, C. and G{\'a}ndara, M. Grabalosa and Diaz, R. Graciani and Cardoso, L. A. Granado and Graug{\'e}s, E. and Graziani, G. and Grecu, A. and Greening, E. and Gregson, S. and Griffith, P. and Grillo, L. and Gr{\"u}nberg, O. and Gui, B. and Gushchin, E. and Guz, Yu and Gys, T. and Hadjivasiliou, C. and Haefeli, G. and Haen, C. and Haines, S. C. and Hall, S. and Hamilton, B. and Hampson, T. and Han, X. and {Hansmann-Menzemer}, S. and Harnew, N. and Harnew, S. T. and Harrison, J. and He, J. and Head, T. and Heijne, V. and Hennessy, K. and Henrard, P. and Henry, L. and Morata, J. A. Hernando and {van Herwijnen}, E. and He{\ss}, M. and Hicheur, A. and Hill, D. and Hoballah, M. and Hombach, C. and Hulsbergen, W. and Hunt, P. and Hussain, N. and Hutchcroft, D. and Hynds, D. and Idzik, M. and Ilten, P. and Jacobsson, R. and Jaeger, A. and Jalocha, J. and Jans, E. and Jaton, P. and Jawahery, A. and Jing, F. and John, M. and Johnson, D. and Jones, C. R. and Joram, C. and Jost, B. and Jurik, N. and Kaballo, M. and Kandybei, S. and Kanso, W. and Karacson, M. and Karbach, T. M. and Karodia, S. and Kelsey, M. and Kenyon, I. R. and Ketel, T. and Khanji, B. and Khurewathanakul, C. and Klaver, S. and Klimaszewski, K. and Kochebina, O. and Kolpin, M. and Komarov, I. and Koopman, R. F. and Koppenburg, P. and Korolev, M. and Kozlinskiy, A. and Kravchuk, L. and Kreplin, K. and Kreps, M. and Krocker, G. and Krokovny, P. and Kruse, F. and Kucewicz, W. and Kucharczyk, M. and Kudryavtsev, V. and Kurek, K. and Kvaratskheliya, T. and La Thi, V. N. and Lacarrere, D. and Lafferty, G. and Lai, A. and Lambert, D. and Lambert, R. W. and Lanfranchi, G. and Langenbruch, C. and Langhans, B. and Latham, T. and Lazzeroni, C. and Gac, R. Le and {van Leerdam}, J. and Lees, J.-P. and Lef{\`e}vre, R. and Leflat, A. and Lefran{\c c}ois, J. and Leo, S. and Leroy, O. and Lesiak, T. and Leverington, B. and Li, Y. and Likhomanenko, T. and Liles, M. and Lindner, R. and Linn, C. and Lionetto, F. and Liu, B. and Lohn, S. and Longstaff, I. and Lopes, J. H. and {Lopez-March}, N. and Lowdon, P. and Lucchesi, D. and Luo, H. and Lupato, A. and Luppi, E. and Lupton, O. and Machefert, F. and Machikhiliyan, I. V. and Maciuc, F. and Maev, O. and Malde, S. and Malinin, A. and Manca, G. and Mapelli, A. and Maratas, J. and Marchand, J. F. and Marconi, U. and Benito, C. Marin and Marino, P. and M{\"a}rki, R. and Marks, J. and Martellotti, G. and S{\'a}nchez, A. Mart{\'i}n and Martinelli, M. and Santos, D. Martinez and Vidal, F. Martinez and Tostes, D. Martins and Massafferri, A. and Matev, R. and Mathe, Z. and Matteuzzi, C. and Mazurov, A. and McCann, M. and McCarthy, J. and McNab, A. and McNulty, R. and McSkelly, B. and Meadows, B. and Meier, F. and Meissner, M. and Merk, M. and Milanes, D. A. and Minard, M.-N. and Moggi, N. and Rodriguez, J. Molina and Monteil, S. and Morandin, M. and Morawski, P. and Mord{\`a}, A. and Morello, M. J. and Moron, J. and Morris, A.-B. and Mountain, R. and Muheim, F. and M{\"u}ller, K. and Mussini, M. and Muster, B. and Naik, P. and Nakada, T. and Nandakumar, R. and Nasteva, I. and Needham, M. and Neri, N. and Neubert, S. and Neufeld, N. and Neuner, M. and Nguyen, A. D. and Nguyen, T. D. and {Nguyen-Mau}, C. and Nicol, M. and Niess, V. and Niet, R. and Nikitin, N. and Nikodem, T. and Novoselov, A. and O'Hanlon, D. P. and {Oblakowska-Mucha}, A. and Obraztsov, V. and Oggero, S. and Ogilvy, S. and Okhrimenko, O. and Oldeman, R. and Onderwater, G. and Orlandea, M. and Goicochea, J. M. Otalora and Otto, A. and Owen, P. and Oyanguren, A. and Pal, B. K. and Palano, A. and Palombo, F. and Palutan, M. and Panman, J. and Papanestis, A. and Pappagallo, M. and Pappalardo, L. L. and Parkes, C. and Parkinson, C. J. and Passaleva, G. and Patel, G. D. and Patel, M. and Patrignani, C. and Alvarez, A. Pazos and Pearce, A. and Pellegrino, A. and Altarelli, M. Pepe and Perazzini, S. and Trigo, E. Perez and Perret, P. and {Perrin-Terrin}, M. and Pescatore, L. and Pesen, E. and Petridis, K. and Petrolini, A. and Olloqui, E. Picatoste and Pietrzyk, B. and Pila{\v r}, T. and Pinci, D. and Pistone, A. and Playfer, S. and Casasus, M. Plo and Polci, F. and Poluektov, A. and Polycarpo, E. and Popov, A. and Popov, D. and Popovici, B. and Potterat, C. and Price, E. and Price, J. D. and Prisciandaro, J. and Pritchard, A. and Prouve, C. and Pugatch, V. and Navarro, A. Puig and Punzi, G. and Qian, W. and Rachwal, B. and Rademacker, J. H. and Rakotomiaramanana, B. and Rama, M. and Rangel, M. S. and Raniuk, I. and Rauschmayr, N. and Raven, G. and Redi, F. and Reichert, S. and Reid, M. M. and dos Reis, A. C. and Ricciardi, S. and Richards, S. and Rihl, M. and Rinnert, K. and Molina, V. Rives and Robbe, P. and Rodrigues, A. B. and Rodrigues, E. and Perez, P. Rodriguez and Roiser, S. and Romanovsky, V. and Vidal, A. Romero and Rotondo, M. and Rouvinet, J. and Ruf, T. and Ruiz, H. and Valls, P. Ruiz and Silva, J. J. Saborido and Sagidova, N. and Sail, P. and Saitta, B. and Guimaraes, V. Salustino and Mayordomo, C. Sanchez and Sedes, B. Sanmartin and Santacesaria, R. and Rios, C. Santamarina and Santovetti, E. and Sarti, A. and Satriano, C. and Satta, A. and Saunders, D. M. and Savrie, M. and Savrina, D. and Schiller, M. and Schindler, H. and Schlupp, M. and Schmelling, M. and Schmidt, B. and Schneider, O. and Schopper, A. and Schune, M.-H. and Schwemmer, R. and Sciascia, B. and Sciubba, A. and Seco, M. and Semennikov, A. and Sepp, I. and Serra, N. and Serrano, J. and Sestini, L. and Seyfert, P. and Shapkin, M. and Shapoval, I. and Shcheglov, Y. and Shears, T. and Shekhtman, L. and Shevchenko, V. and Shires, A. and Coutinho, R. Silva and Simi, G. and Sirendi, M. and Skidmore, N. and Skillicorn, I. and Skwarnicki, T. and Smith, N. A. and Smith, E. and Smith, E. and Smith, J. and Smith, M. and Snoek, H. and Sokoloff, M. D. and Soler, F. J. P. and Soomro, F. and Souza, D. and De Paula, B. Souza and Spaan, B. and Spradlin, P. and Sridharan, S. and Stagni, F. and Stahl, M. and Stahl, S. and Steinkamp, O. and Stenyakin, O. and Stevenson, S. and Stoica, S. and Stone, S. and Storaci, B. and Stracka, S. and Straticiuc, M. and Straumann, U. and Stroili, R. and Subbiah, V. K. and Sun, L. and Sutcliffe, W. and Swientek, K. and Swientek, S. and Syropoulos, V. and Szczekowski, M. and Szczypka, P. and Szilard, D. and Szumlak, T. and T'Jampens, S. and Teklishyn, M. and Tellarini, G. and Teubert, F. and Thomas, C. and Thomas, E. and {van Tilburg}, J. and Tisserand, V. and Tobin, M. and Todd, J. and Tolk, S. and Tomassetti, L. and Tonelli, D. and {Topp-Joergensen}, S. and Torr, N. and Tournefier, E. and Tourneur, S. and Tran, M. T. and Tresch, M. and Tsaregorodtsev, A. and Tsopelas, P. and Tuning, N. and Garcia, M. Ubeda and Ukleja, A. and Ustyuzhanin, A. and Uwer, U. and Vacca, C. and Vagnoni, V. and Valenti, G. and Vallier, A. and Gomez, R. Vazquez and Regueiro, P. Vazquez and Sierra, C. V{\'a}zquez and Vecchi, S. and Velthuis, J. J. and Veltri, M. and Veneziano, G. and Vesterinen, M. and Viaud, B. and Vieira, D. and Diaz, M. Vieites and {Vilasis-Cardona}, X. and Vollhardt, A. and Volyanskyy, D. and Voong, D. and Vorobyev, A. and Vorobyev, V. and Vo{\ss}, C. and Voss, H. and {de Vries}, J. A. and Waldi, R. and Wallace, C. and Wallace, R. and Walsh, J. and Wandernoth, S. and Wang, J. and Ward, D. R. and Watson, N. K. and Websdale, D. and Whitehead, M. and Wicht, J. and Wiedner, D. and Wilkinson, G. and Williams, M. P. and Williams, M. and Wilschut, H. W. and Wilson, F. F. and Wimberley, J. and Wishahi, J. and Wislicki, W. and Witek, M. and Wormser, G. and Wotton, S. A. and Wright, S. and Wyllie, K. and Xie, Y. and Xing, Z. and Xu, Z. and Yang, Z. and Yuan, X. and Yushchenko, O. and Zangoli, M. and Zavertyaev, M. and Zhang, L. and Zhang, W. C. and Zhang, Y. and Zhelezov, A. and Zhokhov, A. and Zhong, L.},
  month = feb,
  year = {2015},
  keywords = {High Energy Physics - Experiment},
  file = {/home/yuri/Zotero/storage/9LBPGZTF/LHCb collaboration et al. - 2015 - Search for the lepton flavour violating decay $ta.pdf;/home/yuri/Zotero/storage/3TQNTRNC/1409.html}
}
% == BibTeX quality report for lhcbcollaborationSearchLeptonFlavour2015:
% Missing required field 'pages'

@book{hanDataMiningConcepts2011,
  address = {{Haryana, India; Burlington, MA}},
  edition = {3 edition},
  title = {Data {{Mining}}: {{Concepts}} and {{Techniques}}, {{Third Edition}}},
  isbn = {978-93-80931-91-3},
  shorttitle = {Data {{Mining}}},
  abstract = {The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it{\^a}ÂÂs still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition{\^a}ÂÂs publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today{\^a}ÂÂs most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of your data},
  language = {English},
  publisher = {{Morgan Kaufmann}},
  author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  month = jul,
  year = {2011},
  file = {/home/yuri/Zotero/storage/P8CFW3ZI/Han et al_2011_Data Mining.pdf}
}
% == BibTeX quality report for hanDataMiningConcepts2011:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{awanIntelligentAnalysisData2015a,
  title = {Intelligent Analysis of Data Cube via Statistical Methods},
  abstract = {Data cube is a multi-dimensional structure of data representation used in data warehousing, which is analyzed using Online Analytical Process(OLAP). However, OLAP in itself is incapable of intelligent analysis in terms of generating a compact and useful data cube as well as lacks the power of predicting empty measures in data cube. Recently statistical methods have been applied for compact cube generation and prediction of empty measures in data cube. In this paper we reviewed and critically evaluated the available work done in this area. Literature review highlighted that sparsity, data redundancy and need of domain knowledge are problems in the way of intelligent analysis. Statistical methods have solved these issues in schema generation, compact cube generation and in prediction of empty measures of data cube individually. Methodology to solve these problems and then to perform an intelligent analysis has not yet been developed. In order to develop such an integrated methodology, we propose a conceptual model for intelligent analysis of data cube via statistical methods in the first stage. This model is integrating statistical methods used in generating a compact cube with a prediction mechanism. Our next target is to develop this model into a complete methodology.},
  booktitle = {2015 {{Tenth International Conference}} on {{Digital Information Management}} ({{ICDIM}})},
  doi = {10.1109/ICDIM.2015.7381880},
  author = {Awan, M. M. and Usman, M.},
  month = oct,
  year = {2015},
  keywords = {Analysis of variance,Area measurement,compact cube generation,Cube analysis,data cube,Data cube with statistical methods,data mining,data redundancy,data representation,data structures,data warehouses,data warehousing,intelligent analysis,Machine learning and OLAP,multidimensional structure,OLAP,OLAP analysis,online analytical process,Prediction,prediction mechanism,Redundancy,schema generation,statistical analysis,statistical methods,Statistical methods for OLAP},
  pages = {20-27},
  file = {/home/yuri/Zotero/storage/25NHEWW2/Awan and Usman - 2015 - Intelligent analysis of data cube via statistical .pdf;/home/yuri/Zotero/storage/5DE8YHN2/Awan and Usman - 2015 - Intelligent analysis of data cube via statistical .pdf;/home/yuri/Zotero/storage/NMTSP55W/Awan and Usman - 2015 - Intelligent analysis of data cube via statistical .pdf;/home/yuri/Zotero/storage/48GNSEJ3/7381880.html;/home/yuri/Zotero/storage/ETVZS7NW/7381880.html;/home/yuri/Zotero/storage/I3SMDMYZ/7381880.html}
}
% == BibTeX quality report for awanIntelligentAnalysisData2015a:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{sohrabiCUSENovelCubebased2016,
  title = {{{CUSE}}: {{A}} Novel Cube-Based Approach for Sequential Pattern Mining},
  shorttitle = {{{CUSE}}},
  abstract = {Sequences are one of the most important types of patterns which are extracted from datasets and are used to construct association rules. Several sequential pattern mining methods have been proposed in the literature. This paper introduces a novel bit wise approach to compress and represent the sequence database as a 3-dimentional array and use a corresponding mining method to extract frequent sequences from the compressed structure. Experimental results and performance study show that this algorithm outperforms the best previously developed algorithms.},
  booktitle = {2016 4th {{International Symposium}} on {{Computational}} and {{Business Intelligence}} ({{ISCBI}})},
  doi = {10.1109/ISCBI.2016.7743281},
  author = {Sohrabi, M. K. and Ghods, V.},
  month = sep,
  year = {2016},
  keywords = {Algorithm design and analysis,database management systems,Data mining,data cube,data mining,Clustering algorithms,association rule,Biology,Classification algorithms,cube-based sequential pattern mining,CUSE,feature extraction,frequent sequence mining,Itemsets,pattern extraction,sequence database,sequential pattern},
  pages = {186-190},
  file = {/home/yuri/Zotero/storage/3WFBRSMI/Sohrabi and Ghods - 2016 - CUSE A novel cube-based approach for sequential p.pdf;/home/yuri/Zotero/storage/KIL92WVZ/Sohrabi and Ghods - 2016 - CUSE A novel cube-based approach for sequential p.pdf;/home/yuri/Zotero/storage/2LAA3I2M/7743281.html;/home/yuri/Zotero/storage/ZM95GHR9/7743281.html}
}
% == BibTeX quality report for sohrabiCUSENovelCubebased2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{fengScalableInformativeRule2017,
  title = {Scalable {{Informative Rule Mining}}},
  abstract = {We present SIRUM: a system for Scalable Informative RUle Mining from multi-dimensional data. Informative rules have recently been studied in several contexts, including data summarization, data cube exploration and data quality. The objective is to produce a small set of rules (patterns) over the values of the dimension attributes that provide the most information about the distribution of a numeric measure attribute. Within SIRUM, we propose several optimizations for tall, wide and distributed datasets. We implemented SIRUM in Spark and observed significant performance and scalability improvements on real datasets due to our optimizations. As a result, SIRUM is able to generate informative rules on much wider and taller datasets than using distributed implementations of the previous state of the art.},
  booktitle = {2017 {{IEEE}} 33rd {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  doi = {10.1109/ICDE.2017.101},
  author = {Feng, G. and Golab, L. and Srivastava, D.},
  month = apr,
  year = {2017},
  keywords = {optimisation,Optimization,Data mining,data mining,Distributed databases,multidimensional data,Airports,Context,data cube exploration,data quality,data summarization,Delays,distributed datasets,numeric measure attribute distribution,optimization,rules set,scalable informative rule mining,SIRUM,Spark,Sparks},
  pages = {437-448},
  file = {/home/yuri/Zotero/storage/6K9SVPZ3/Feng et al. - 2017 - Scalable Informative Rule Mining.pdf;/home/yuri/Zotero/storage/2Z3BA36X/7929997.html}
}
% == BibTeX quality report for fengScalableInformativeRule2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@misc{EngineeringVillageExpert,
  title = {Engineering {{Village}} - {{Expert Search Abstract Format}}},
  howpublished = {https://www.engineeringvillage.com/search/doc/abstract.url?\&pageType=expertSearch\&usageZone=resultslist\&usageOrigin=searchresults\&searchtype=Expert\&SEARCHID=43800d73Made2M4542M97dcM8e95d0f27faa\&DOCINDEX=147\&ignore\_docid=cpx\_49e3b6be15fb7f5a456M63f110178163176\&database=1\&format=expertSearchAbstractFormat\&tagscope=\&displayPagination=yes},
  file = {/home/yuri/Zotero/storage/6THYNZSS/abstract.html}
}
% == BibTeX quality report for EngineeringVillageExpert:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{jangDemonstrationMultispectralTarget2016,
  title = {Demonstration of Multispectral Target Locator Using Collocated {{RF}} Antenna/{{LWIR}} Joint Sensor System and Datacube},
  volume = {10004},
  abstract = {Recently, we configured RF antennas and a LWIR camera connected to an actuator system to form a collocated sensor system. We also developed a GUI which directly controls both RF and IR systems, azimuth motion, as well as performs post-processing for data integration and location finding. RF range data and LWIR images were collected simultaneously by using our configured sensor system as azimuth was varied from 0 to 70\&deg;. Series of collected RF data was transformed into a single 2-D radar image showing range profile of targets against azimuth. For LWIR, data was aligned into a single panoramic image as a function of azimuth by incorporating shift parameters observed in the measurements. Both RF/IR images were then arranged into a 3-D datacube, having azimuth as a common domain, and this datacube directly provided locational information of targets. For demonstration, we successfully located objects such as a corner reflector and a blackbody source under a dark background. In addition, we highlight some additional features available in our sensor system including target classification using both Euclidean and SVM based multi-classifier techniques, and tracking capability for region of interest on moving targets. Future work would be to improve the current system for outdoor measurement to locate distant targets.},
  booktitle = {Image and {{Signal Processing}} for {{Remote Sensing XXII}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2235350},
  author = {Jang, Woo-Yong and Park, James and Kakas, George and Noyola, Michael},
  month = oct,
  year = {2016},
  pages = {100040D},
  file = {/home/yuri/Zotero/storage/DB263SML/Jang et al. - 2016 - Demonstration of multispectral target locator usin.pdf;/home/yuri/Zotero/storage/43RMG6MQ/12.2235350.html}
}
% == BibTeX quality report for jangDemonstrationMultispectralTarget2016:
% ? Unsure about the formatting of the booktitle

@inproceedings{suBusinessIntelligenceRevisited2017,
  title = {Business {{Intelligence Revisited}}},
  abstract = {Combining big data with machine learning is a powerful tool for business intelligence (BI). Developed in the database community, The traditional ETL-data warehouse-OLAP approach to BI is effective to deal with multi-dimensional data (i.e. data cubes) but not suitable for flexible analytics such as exploration with ad hoc queries and process/data changes. Process mining techniques developed in the BPM community focus on activities and control flow but ignore data. In this paper, we propose a new framework for business analytics based on workflow logs. We introduce the key notions, illustrate querying logs as one useful aspect, and then discuss a range of interesting technical problems to be studied further.},
  booktitle = {2017 {{Fifth International Conference}} on {{Advanced Cloud}} and {{Big Data}} ({{CBD}})},
  doi = {10.1109/CBD.2017.9},
  author = {Su, J. and Tang, Y.},
  month = aug,
  year = {2017},
  keywords = {Tools,machine learning,Data models,learning (artificial intelligence),Databases,Data mining,Big Data,Business intelligence,competitive intelligence,business analytics,business intelligence,querying logs,Unified modeling language,workflow logs},
  pages = {1-6},
  file = {/home/yuri/Zotero/storage/7LJV6J9A/Su and Tang - 2017 - Business Intelligence Revisited.pdf;/home/yuri/Zotero/storage/Z7PEIGCI/8026904.html}
}
% == BibTeX quality report for suBusinessIntelligenceRevisited2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{wangGaussianCubesRealTime2017,
  title = {Gaussian {{Cubes}}: {{Real}}-{{Time Modeling}} for {{Visual Exploration}} of {{Large Multidimensional Datasets}}},
  volume = {23},
  issn = {1077-2626},
  shorttitle = {Gaussian {{Cubes}}},
  abstract = {Recently proposed techniques have finally made it possible for analysts to interactively explore very large datasets in real time. However powerful, the class of analyses these systems enable is somewhat limited: specifically, one can only quickly obtain plots such as histograms and heatmaps. In this paper, we contribute Gaussian Cubes, which significantly improves on state-of-the-art systems by providing interactive modeling capabilities, which include but are not limited to linear least squares and principal components analysis (PCA). The fundamental insight in Gaussian Cubes is that instead of precomputing counts of many data subsets (as state-of-the-art systems do), Gaussian Cubes precomputes the best multivariate Gaussian for the respective data subsets. As an example, Gaussian Cubes can fit hundreds of models over millions of data points in well under a second, enabling novel types of visual exploration of such large datasets. We present three case studies that highlight the visualization and analysis capabilities in Gaussian Cubes, using earthquake safety simulations, astronomical catalogs, and transportation statistics. The dataset sizes range around one hundred million elements and 5 to 10 dimensions. We present extensive performance results, a discussion of the limitations in Gaussian Cubes, and future research directions.},
  number = {1},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2016.2598694},
  author = {Wang, Z. and Ferreira, N. and Wei, Y. and Bhaskar, A. S. and Scheidegger, C.},
  month = jan,
  year = {2017},
  keywords = {Data models,Computational modeling,data visualisation,Data visualization,Visualization,Analytical models,astronomical catalogs,data cubes,Data modeling,data subsets,dimensionality reduction,earthquake safety simulations,Gaussian cubes,Gaussian processes,interactive modeling capabilities,interactive visualization,large multidimensional datasets,least squares approximations,linear least squares,Manuals,multivariate Gaussian,PCA,principal component analysis,Principal component analysis,real-time modeling,transportation statistics,visual exploration},
  pages = {681-690},
  file = {/home/yuri/Zotero/storage/V5AC368P/Wang et al. - 2017 - Gaussian Cubes Real-Time Modeling for Visual Expl.pdf;/home/yuri/Zotero/storage/GA7URSJP/7536648.html}
}
% == BibTeX quality report for wangGaussianCubesRealTime2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{guptaBigDataImplementation2014,
  title = {Big Data Implementation and Visualization},
  abstract = {Government agencies and large corporations are launching research programs to address big data's challenges. Visualization in today's time is very effective for presenting essential information in vast amounts of data. Big-data discovery tools present new research opportunities to the graphics and visualization community. The size of the collected data about the Web and mobile device users is even greater. To provide the ability to make sense and maximize utilization of such vast amounts of data for knowledge discovery and decision making is crucial to scientific advancement; we need new tools beyond conventional data mining and statistical analysis. Visualization is a tool which is shown to be effective for gleaning insight in big data. Here we also discuss data cube that fits in a tablet or a smart phone memory, actually for billions of entrances; we call this information structure a nanocube. [13]. We present pseudo code to compute and query a nanocube [13], and show how it can be used to generate well-known visual encodings such as heat maps, histograms, and parallel coordinate plots. While Apache* Hadoop* and other technologies are emerging to support back-end concerns such as storage and processing, visualization-based data discovery tools focus on the front end of big data-on helping businesses explore the data more easily and understand it more fully.},
  booktitle = {2014 {{International Conference}} on {{Advances}} in {{Engineering Technology Research}} ({{ICAETR}} - 2014)},
  doi = {10.1109/ICAETR.2014.7012883},
  author = {Gupta, D. and Siddiqui, S.},
  month = aug,
  year = {2014},
  keywords = {Real-time systems,Servers,data cube,data mining,statistical analysis,Big Data,data visualisation,Data visualization,Visualization,Conferences,query processing,Analytics,Big data,decision making,data cubes,Apache Hadoop,Apache* Hadoop*,Big-Data discovery tools,Business,government agencies,graphics community,information structure,knowledge discovery,nanocube query,pseudocode,public domain software,smart phone memory,tablet,visual encodings,visualization community,visualization-based data discovery tools},
  pages = {1-10},
  file = {/home/yuri/Zotero/storage/GWW6NGYX/Gupta and Siddiqui - 2014 - Big data implementation and visualization.pdf;/home/yuri/Zotero/storage/HABJQHKI/7012883.html}
}
% == BibTeX quality report for guptaBigDataImplementation2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{scrineyGeneratingCubesSmart2017,
  address = {{New York, NY, USA}},
  series = {{{ACSW}} '17},
  title = {Generating {{Cubes}} from {{Smart City Web Data}}},
  isbn = {978-1-4503-4768-6},
  abstract = {A smart city necessitates the incorporation of data sources from multiple providers and data formats. Similar to the Internet Of Things, this data is primarily obtained from web streams producing XML or JSON data. Various combinations of data obtained from different providers can be used to enhance the lives of citizens with respect to different characteristics such as transport, city planning, the environment and housing. However, data provided from these streams is not necessarily in a format suitable for analysis and OLAP queries, despite the fact that these streams often provide measures and some elements of dimensionality often found in OLAP queries. In this research, we present a StarGraph construct which is designed to import web generated data streams and automatically generate the cube format necessary for OLAP queries. Our validation shows how the data streams can be captured as StarGraphs and using a traffic data case study, demonstrates an efficiency for populating and updating the data cube.},
  booktitle = {Proceedings of the {{Australasian Computer Science Week Multiconference}}},
  publisher = {{ACM}},
  doi = {10.1145/3014812.3014863},
  author = {Scriney, Michael and O'Connor, Martin F. and Roantree, Mark},
  year = {2017},
  keywords = {OLAP,data cubes,smart city},
  pages = {49:1--49:8},
  file = {/home/yuri/Zotero/storage/AL9TLLF7/Scriney et al. - 2017 - Generating Cubes from Smart City Web Data.pdf}
}
% == BibTeX quality report for scrineyGeneratingCubesSmart2017:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{ioannidisRudolfHTTPAPI2016,
  title = {Rudolf: {{An HTTP API}} for Exposing Semantically Represented Fiscal {{OLAP}} Cubes},
  shorttitle = {Rudolf},
  abstract = {OpenSpending is the world's largest repository of open fiscal datasets. As of now it only supported loading datasets from CSV files, leaving out more heavily-structured formats like RDF. Rudolf, an RDF backend has been developed to expose Data Cube RDF triples and OLAP operations in JSON format through an OpenSpending-compatible HTTP API. This API can be directly utilized by the OpenSpending Viewer application to transparently offer aggregate visualizations for RDF fiscal datasets out of the box.},
  booktitle = {2016 11th {{International Workshop}} on {{Semantic}} and {{Social Media Adaptation}} and {{Personalization}} ({{SMAP}})},
  doi = {10.1109/SMAP.2016.7753406},
  author = {Ioannidis, L. and Bratsas, C. and Karabatakis, S. and Filippidis, P. and Bamidis, P.},
  month = oct,
  year = {2016},
  keywords = {Data models,Databases,data mining,Aggregates,data visualisation,Data visualization,application program interfaces,aggregate visualizations,CSV files,data cube RDF triples,Data structures,HTTP API,hypermedia,JSON format,OLAP operations,open fiscal dataset repository,OpenSpending,OpenSpending Viewer application,Organizations,public finance,RDF backend,Resource description framework,Rudolf,semantically represented fiscal OLAP cubes},
  pages = {177-182},
  file = {/home/yuri/Zotero/storage/ZPCR92WF/Ioannidis et al. - 2016 - Rudolf An HTTP API for exposing semantically repr.pdf;/home/yuri/Zotero/storage/GK228DVY/7753406.html}
}
% == BibTeX quality report for ioannidisRudolfHTTPAPI2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{mirandaTopKubeRankAwareData2018,
  title = {{{TopKube}}: {{A Rank}}-{{Aware Data Cube}} for {{Real}}-{{Time Exploration}} of {{Spatiotemporal Data}}},
  volume = {24},
  issn = {1077-2626},
  shorttitle = {{{TopKube}}},
  abstract = {From economics to sports to entertainment and social media, ranking objects according to some notion of importance is a fundamental tool we humans use all the time to better understand our world. With the ever-increasing amount of user-generated content found online, ``what's trending'' is now a commonplace phrase that tries to capture the zeitgeist of the world by ranking the most popular microblogging hashtags in a given region and time. However, before we can understand what these rankings tell us about the world, we need to be able to more easily create and explore them, given the significant scale of today's data. In this paper, we describe the computational challenges in building a real-time visual exploratory tool for finding top-ranked objects; build on the recent work involving in-memory and rank-aware data cubes to propose TopKube: a data structure that answers top-k queries up to one order of magnitude faster than the previous state of the art; demonstrate the usefulness of our methods using a set of real-world, publicly available datasets; and provide a new set of benchmarks for other researchers to validate their methods and compare to our own.},
  number = {3},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  doi = {10.1109/TVCG.2017.2671341},
  author = {Miranda, F. and Lins, L. and Klosowski, J. T. and Silva, C. T.},
  month = mar,
  year = {2018},
  keywords = {Internet,Real-time systems,Benchmark testing,Proposals,data cube,data structures,Data visualization,Visualization,query processing,data structure,Data structures,Interactive visualization,rank merging,rank-aware data cube,real-time exploration,real-time visual exploratory tool,spatiotemporal data,top-k queries,top-K queries,TopKube,Urban areas},
  pages = {1394-1407},
  file = {/home/yuri/Zotero/storage/TP4U2GP8/Miranda et al. - 2018 - TopKube A Rank-Aware Data Cube for Real-Time Expl.pdf;/home/yuri/Zotero/storage/TGGKE2T6/7858782.html}
}
% == BibTeX quality report for mirandaTopKubeRankAwareData2018:
% ? Title looks like it was stored in title-case in Zotero

@incollection{singhFrequentPatternsMining2016,
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  title = {Frequent {{Patterns Mining}} from {{Data Cube Using Aggregation}} and {{Directed Graph}}},
  isbn = {978-81-322-2693-2 978-81-322-2695-6},
  abstract = {An algorithm has been proposed for mining frequent maximal itemsets from data cube. Discovering frequent itemsets has been a key process in association rule mining. One of the major drawbacks of traditional algorithms is that lot of time is taken to find candidate itemsets. Proposed algorithm discovers frequent itemsets using aggregation function and directed graph. It uses directed graph for candidate itemsets generation and aggregation for dimension reduction. Experimental results show that the proposed algorithm can quickly discover maximal frequent itemsets and effectively mine potential association rules.},
  language = {en},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Frontiers}} in {{Intelligent Computing}}: {{Theory}} and {{Applications}} ({{FICTA}}) 2015},
  publisher = {{Springer, New Delhi}},
  author = {Singh, Kuldeep and Shakya, Harish Kumar and Biswas, Bhaskar},
  year = {2016},
  pages = {167-177},
  file = {/home/yuri/Zotero/storage/CUPTBEBG/10.html},
  doi = {10.1007/978-81-322-2695-6_15}
}
% == BibTeX quality report for singhFrequentPatternsMining2016:
% ? Title looks like it was stored in title-case in Zotero

@article{kimPrivacypreservingDataCube2017,
  title = {Privacy-Preserving Data Cube for Electronic Medical Records: {{An}} Experimental Evaluation},
  volume = {97},
  issn = {1386-5056},
  shorttitle = {Privacy-Preserving Data Cube for Electronic Medical Records},
  abstract = {Introduction
: The aim of this study is to evaluate the effectiveness and efficiency of privacy-preserving data cubes of electronic medical records (EMRs). An EMR data cube is a complex of EMR statistics that are summarized or aggregated by all possible combinations of attributes. Data cubes are widely utilized for efficient big data analysis and also have great potential for EMR analysis. For safe data analysis without privacy breaches, we must consider the privacy preservation characteristics of the EMR data cube. In this paper, we introduce a design for a privacy-preserving EMR data cube and the anonymization methods needed to achieve data privacy. We further focus on changes in efficiency and effectiveness that are caused by the anonymization process for privacy preservation. Thus, we experimentally evaluate various types of privacy-preserving EMR data cubes using several practical metrics and discuss the applicability of each anonymization method with consideration for the EMR analysis environment.
Methods
: We construct privacy-preserving EMR data cubes from anonymized EMR datasets. A real EMR dataset and demographic dataset are used for the evaluation. There are a large number of anonymization methods to preserve EMR privacy, and the methods are classified into three categories (i.e., global generalization, local generalization, and bucketization) by anonymization rules. According to this classification, three types of privacy-preserving EMR data cubes were constructed for the evaluation. We perform a comparative analysis by measuring the data size, cell overlap, and information loss of the EMR data cubes.
Results
: Global generalization considerably reduced the size of the EMR data cube and did not cause the data cube cells to overlap, but incurred a large amount of information loss. Local generalization maintained the data size and generated only moderate information loss, but there were cell overlaps that could decrease the search performance. Bucketization did not cause cells to overlap and generated little information loss; however, the method considerably inflated the size of the EMR data cubes.
Conclusions
: The utility of anonymized EMR data cubes varies widely according to the anonymization method, and the applicability of the anonymization method depends on the features of the EMR analysis environment. The findings help to adopt the optimal anonymization method considering the EMR analysis environment and goal of the EMR analysis.},
  journal = {International Journal of Medical Informatics},
  doi = {10/f9hs5d},
  author = {Kim, Soohyung and Lee, Hyukki and Chung, Yon Dohn},
  month = jan,
  year = {2017},
  keywords = {Data cube,Anonymization,Electronic medical records,Medical privacy},
  pages = {33-42},
  file = {/home/yuri/Zotero/storage/QMLZ6W43/Kim et al. - 2017 - Privacy-preserving data cube for electronic medica.pdf;/home/yuri/Zotero/storage/JINSGGCG/S1386505616302015.html}
}
% == BibTeX quality report for kimPrivacypreservingDataCube2017:
% Missing required field 'number'

@article{wrightElectricMicropropulsionSystems2015,
  title = {Electric Micropropulsion Systems},
  volume = {74},
  issn = {0376-0421},
  abstract = {With the development of microspacecraft, the field of electrical micropropulsion is a rapidly expanding discipline. New ideas are being explored constantly and a review of the current state of technological development in the field will be useful. This review deals with electrostatic and electromagnetic micropropulsion systems that are either miniaturization attempts of existing technologies or novel systems in their own right. A brief discussion of the development of microspacecraft and a general overview of the types of micropropulsion are given. The essential mechanism of operation of each electrical micropropulsion system is described and recent progress in the development of these systems is explored, giving latest available data of their performance parameters.},
  journal = {Progress in Aerospace Sciences},
  doi = {10/f68mfg},
  author = {Wright, W. P. and Ferrer, P.},
  month = apr,
  year = {2015},
  keywords = {Electromagnetic,Electrostatic,Micropropulsion,Microsatellites,Propulsion,Spacecraft},
  pages = {48-61},
  file = {/home/yuri/Zotero/storage/L3DSPDS5/Wright and Ferrer - 2015 - Electric micropropulsion systems.pdf;/home/yuri/Zotero/storage/JT8Z5BQJ/S0376042114000979.html}
}
% == BibTeX quality report for wrightElectricMicropropulsionSystems2015:
% Missing required field 'number'

@article{leomanniPropulsionOptionsVery2017,
  title = {Propulsion Options for Very Low {{Earth}} Orbit Microsatellites},
  volume = {133},
  issn = {0094-5765},
  abstract = {The growing competitiveness in the commercial space market has raised the interest in operating small spacecraft at very low altitudes. To make this feasible, the space industry has started developing propulsion options tailored specifically to these platforms. This paper presents a review of emerging micropropulsion technologies and evaluates their applicability to microsatellite missions in the altitude range 250\textendash{}500km. The results of the proposed analysis are demonstrated on two different remote sensing applications.},
  journal = {Acta Astronautica},
  doi = {10/f9x83p},
  author = {Leomanni, Mirko and Garulli, Andrea and Giannitrapani, Antonio and Scortecci, Fabrizio},
  month = apr,
  year = {2017},
  keywords = {Low Earth orbit,Microsatellite,Space Propulsion,Station-keeping},
  pages = {444-454},
  file = {/home/yuri/Zotero/storage/XG4NRIUA/Leomanni et al. - 2017 - Propulsion options for very low Earth orbit micros.pdf;/home/yuri/Zotero/storage/JH73RE5M/S0094576516305197.html}
}
% == BibTeX quality report for leomanniPropulsionOptionsVery2017:
% Missing required field 'number'

@inproceedings{bakshiConsiderationsBigData2012,
  title = {Considerations for Big Data: {{Architecture}} and Approach},
  shorttitle = {Considerations for Big Data},
  abstract = {The amount of data in our industry and the world is exploding. Data is being collected and stored at unprecedented rates. The challenge is not only to store and manage the vast volume of data (``big data''), but also to analyze and extract meaningful value from it. There are several approaches to collecting, storing, processing, and analyzing big data. The main focus of the paper is on unstructured data analysis. Unstructured data refers to information that either does not have a pre-defined data model or does not fit well into relational tables. Unstructured data is the fastest growing type of data, some example could be imagery, sensors, telemetry, video, documents, log files, and email data files. There are several techniques to address this problem space of unstructured analytics. The techniques share a common characteristics of scale-out, elasticity and high availability. MapReduce, in conjunction with the Hadoop Distributed File System (HDFS) and HBase database, as part of the Apache Hadoop project is a modern approach to analyze unstructured data. Hadoop clusters are an effective means of processing massive volumes of data, and can be improved with the right architectural approach.},
  booktitle = {2012 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2012.6187357},
  author = {Bakshi, K.},
  month = mar,
  year = {2012},
  keywords = {Computer architecture,Benchmark testing,data analysis,data management,MapReduce,Distributed databases,Apache Hadoop project,architectural approach,Availability,data storage,File systems,Hadoop cluster,Hadoop distributed file system,HBase database,NoSQL,Relational databases,SQL,unstructured data analysis},
  pages = {1-7},
  file = {/home/yuri/Zotero/storage/5U5C3VR7/Bakshi - 2012 - Considerations for big data Architecture and appr.pdf;/home/yuri/Zotero/storage/N2TLC2ZW/Bakshi - 2012 - Considerations for big data Architecture and appr.pdf;/home/yuri/Zotero/storage/SXQIQTX6/6187357.html;/home/yuri/Zotero/storage/ZDTKM224/6187357.html}
}
% == BibTeX quality report for bakshiConsiderationsBigData2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@incollection{gillesFlyingLargeConstellations2016,
  series = {{{SpaceOps Conferences}}},
  title = {Flying {{Large Constellations Using Automation}} and {{Big Data}}},
  booktitle = {{{SpaceOps}} 2016 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Gilles, Kbidy},
  month = may,
  year = {2016},
  file = {/home/yuri/Zotero/storage/J3MZXDKQ/Gilles - 2016 - Flying Large Constellations Using Automation and B.pdf;/home/yuri/Zotero/storage/F52I66MM/6.html},
  doi = {10.2514/6.2016-2387}
}
% == BibTeX quality report for gillesFlyingLargeConstellations2016:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@article{grzymala-busseComparisonThreeStrategies2003,
  series = {International {{Workshop}} on {{Rough Sets}} in {{Knowledge Discovery}} and {{Soft Computing}} ({{Satellite Event}} for {{ETAPS}} 2003)},
  title = {A {{Comparison}} of {{Three Strategies}} to {{Rule Induction}} from {{Data}} with {{Numerical Attributes}}},
  volume = {82},
  issn = {1571-0661},
  abstract = {Our main objective was to compare two discretization techniques, both based on cluster analysis, with a new rule induction algorithm called MLEM2, in which discretization is performed simultaneously with rule induction. The MLEM2 algorithm is an extension of the existing LEM2 rule induction algorithm. The LEM2 algorithm works correctly only for symbolic attributes and is a part of the LERS data mining system. For the two strategies, based on cluster analysis, rules were induced by the LEM2 algorithm. Our results show that MLEM2 outperformed both strategies based on cluster analysis, in terms of complexity (size of rule sets) and, more importantly, error rates.},
  number = {4},
  journal = {Electronic Notes in Theoretical Computer Science},
  doi = {10/dhrthk},
  author = {{Grzymala-Busse}, Jerzy W.},
  month = mar,
  year = {2003},
  keywords = {machine learning,data mining,discretization,Rough set theory,rule induction},
  pages = {132-140},
  file = {/home/yuri/Zotero/storage/UVEWSV4I/Grzymala-Busse - 2003 - A Comparison of Three Strategies to Rule Induction.pdf;/home/yuri/Zotero/storage/N66FVS4Y/S1571066104807126.html}
}
% == BibTeX quality report for grzymala-busseComparisonThreeStrategies2003:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{dongMiningMethodAttribute2008,
  title = {Mining {{Method}} of {{Attribute Reduction Based}} on {{Rough Fuzzy Set Theory}}},
  volume = {3},
  abstract = {In order to solve the reasoning and decision problems when the given information is not sufficient, a mining method for attribute reduction of classification rules based on Rough Fuzzy Set theory is proposed, and its flow chart is also presented. The method can mine hidden association rules in samples and make decisions by using the reduction algorithm of characteristic attribute based on rough set and induction algorithm of decision rules based on fuzzy set. Finally, information system of satellite navigation system combat effectiveness evaluation is applied with this approach, and the results are satisfactory, which have proved that the approach is practicable and effective.},
  booktitle = {2008 {{International Conference}} on {{Computer Science}} and {{Software Engineering}}},
  doi = {10.1109/CSSE.2008.1113},
  author = {Dong, C. and Wu, D. and Liu, H.},
  month = dec,
  year = {2008},
  keywords = {Data mining,data mining,Association rules,association rule,rule induction,attribute reduction,characteristic attribute based,classification rules,Computer science,decision rules,effectiveness evaulation,Flowcharts,fuzzy set theory,Fuzzy set theory,Fuzzy sets,hidden association rules,induction algorithm,information system,Information systems,mining method,reduction algorithm,Rough Fuzzy Set,rough fuzzy set theory,rough set theory,satellite navigation system,Satellite navigation systems,Set theory,Software engineering},
  pages = {746-749},
  file = {/home/yuri/Zotero/storage/6TANFYIL/Dong et al. - 2008 - Mining Method of Attribute Reduction Based on Roug.pdf;/home/yuri/Zotero/storage/PADP25YA/4722450.html}
}
% == BibTeX quality report for dongMiningMethodAttribute2008:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{eichhoffInducingProductionRules2017,
  title = {Inducing Production Rules to Extend Existing Design Grammars: {{The}} Parse/Derive Method},
  volume = {14},
  issn = {null},
  shorttitle = {Inducing Production Rules to Extend Existing Design Grammars},
  abstract = {Graph-rewriting is a promising computation model for computer-aided design (CAD) applications that operate on graph-based design models. Graph-rewriting-based CAD systems rely on predefined production rules. These rules encode the set of possible actions that may be taken within the design process to make changes to the current design. This paper presents a method for automatically inducing new production rules from existing sample designs. The methods applicability is illustrated in context of conceptual spacecraft design. Results gained from experiments, where existing rules were deliberately left out and had to be rediscovered, show that the induced rules are often similar or identical to the original rules.},
  number = {4},
  journal = {Computer-Aided Design and Applications},
  doi = {10/gdk2tw},
  author = {Eichhoff, Julian R. and Schmidt, Jens and Roller, Dieter},
  month = jun,
  year = {2017},
  keywords = {rule induction,design grammar,Graph-rewriting,rule inference},
  pages = {535-548},
  file = {/home/yuri/Zotero/storage/H92UYXK6/Eichhoff et al. - 2017 - Inducing production rules to extend existing desig.pdf;/home/yuri/Zotero/storage/I855J2C2/16864360.2016.html}
}

@inproceedings{hayhurstHistoricalMassPower2016,
  title = {Historical Mass, Power, Schedule, and Cost Growth for {{NASA}} Spacecraft},
  abstract = {Although spacecraft developers have been moving towards standardized product lines as the aerospace industry has matured, NASA's continual need to push the cutting edge of science to accomplish unique, challenging missions can still lead to spacecraft resource growth over time. This paper assesses historical mass, power, cost, and schedule growth for multiple NASA spacecraft from the last twenty years and compares to industry reserve guidelines to understand where the guidelines may fall short. Growth is assessed from project start to launch, from the time of the preliminary design review (PDR) to launch and from the time of the critical design review (CDR) to launch. Data is also assessed not just at the spacecraft bus level, but also at the subsystem level wherever possible, to help obtain further insight into possible drivers of growth. Potential recommendations to minimize spacecraft mass, power, cost, and schedule growth for future missions are also discussed.},
  booktitle = {2016 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2016.7500553},
  author = {Hayhurst, M. R. and Judnick, D. C. and Bitten, R. E. and Hallgrimson, I. E. and Shinn, S. A. and Youngs, M. A.},
  month = mar,
  year = {2016},
  keywords = {Space vehicles,Schedules,scheduling,space vehicles,NASA,Instruments,Guidelines,aerospace industry,Astrophysics,CDR,cost growth,critical design review,Geoscience,historical mass,multiple NASA spacecraft schedule growth,PDR,power assessment,preliminary design review,spacecraft bus level,spacecraft mass minimization,spacecraft resource growth},
  pages = {1-17},
  file = {/home/yuri/Zotero/storage/38LV588R/Hayhurst et al. - 2016 - Historical mass, power, schedule, and cost growth .pdf;/home/yuri/Zotero/storage/QI7NLBJL/7500553.html}
}
% == BibTeX quality report for hayhurstHistoricalMassPower2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{nigriLightCurveAnalysis2017,
  address = {{New York, NY, USA}},
  series = {{{ICMR}} '17},
  title = {Light {{Curve Analysis From Kepler Spacecraft Collected Data}}},
  isbn = {978-1-4503-4701-3},
  abstract = {Although scarce, previous work on the application of machine learning and data mining techniques on large corpora of astronomical data has produced promising results. For example, on the task of detecting so-called Kepler objects of interest (KOIs), a range of different `off the shelf' classifiers has demonstrated outstanding performance. These rather preliminary research efforts motivate further exploration of this data domain. In the present work we focus on the analysis of threshold crossing events (TCEs) extracted from photometric data acquired by the Kepler spacecraft. We show that the task of classifying TCEs as being effected by actual planetary transits as opposed to confounding astrophysical phenomena is significantly more challenging than that of KOI detection, with different classifiers exhibiting vastly different performances. Nevertheless, the best performing classifier type, the random forest, achieved excellent accuracy, correctly predicting in approximately 96\% of the cases. Our results and analysis should illuminate further efforts into the development of more sophisticated, automatic techniques, and encourage additional work in the area.},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{International Conference}} on {{Multimedia Retrieval}}},
  publisher = {{ACM}},
  doi = {10.1145/3078971.3080544},
  author = {Nigri, Eduardo and Arandjelovic, Ognjen},
  year = {2017},
  keywords = {random forests,astronomy,big data,pattern recognition,photometry,space,support vector machines},
  pages = {93--98},
  file = {/home/yuri/Zotero/storage/GWMRBY5C/Nigri and Arandjelovic - 2017 - Light Curve Analysis From Kepler Spacecraft Collec.pdf}
}
% == BibTeX quality report for nigriLightCurveAnalysis2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{killoughIntegratingCCSDSMILSTD15532002,
  title = {Integrating {{CCSDS}} and {{MIL}}-{{STD}}-1553: {{What}} You Should Know},
  volume = {4},
  shorttitle = {Integrating {{CCSDS}} and {{MIL}}-{{STD}}-1553},
  abstract = {The use of the MIL-STD-1553B (1553) communications bus and the Consultative Committee for Space Data Systems (CCSDS) recommendations are increasingly common elements in the design of satellite command and data handling systems. However, the 1553 bus has some fundamental characteristics which are often not understood by those implementing the data protocol which will run on top of it. In addition to spacecraft-to-ground communications, various CCSDS data protocol units are being used as the format for the communication of commands, messages, and telemetry data between the spacecraft, instruments, and other subsystems. Experience has produced a list of "dos and don'ts" in implementing 1553 bus command and telemetry protocols using CCSDS.},
  booktitle = {Proceedings, {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2002.1036904},
  author = {Killough, R.},
  year = {2002},
  keywords = {Space vehicles,protocols,Protocols,satellite communication,telemetry,satellite telemetry,Instruments,Artificial satellites,Buildings,CCSDS data protocol,Communication standards,Control systems,MIL-STD-1553B communications bus,Observatories,Open systems,Payloads,satellite command and data handling system,spacecraft-to-ground communication,system buses,telecommunication standard,telecommunication standards},
  pages = {4-1917-4-1926 vol.4},
  file = {/home/yuri/Zotero/storage/YCNHFCE7/Killough - 2002 - Integrating CCSDS and MIL-STD-1553 What you shoul.pdf}
}
% == BibTeX quality report for killoughIntegratingCCSDSMILSTD15532002:
% Missing required field 'publisher'

@inproceedings{vilnrotterMaximumLikelihoodEstimation2010,
  title = {Maximum Likelihood Estimation of Navigation Parameters from Downlink Telemetry},
  abstract = {Deep-space navigation uses estimates of range and Doppler to update and improve spacecraft trajectory solutions. Operationally, Doppler is generally extracted directly from the ground receiver's carrier tracking loop, and range is determined primarily by the use of specially designed {\^A}\textquestiondown{}ranging tones,{\^A}\textquestiondown{} or more recently (e.g., on New Horizons), Pseudo-Noise (PN) sequences. Transmission of tones or PN sequences drain power and bandwidth that could be better used for transmitting additional science data from the spacecraft. Here we describe and evaluate a novel technique that extracts range and Doppler estimates directly from the decoded data, thus enabling data-transmission at the maximum rate consistent with spacecraft range, antenna gain and available signal power. In this paper, the structure of the maximum likelihood estimator for range and Doppler is derived, and its performance determined relative to Crame{\^A}\textquestiondown{}r-Rao bounds via simulation and analysis. Performance of conventional Doppler and delay estimators based on carrier tracking loops is also derived, and contrasted with the performance of the optimum Doppler-delay estimator over a range of symbol SNR typically encountered in deep-space applications.},
  booktitle = {2010 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2010.5446954},
  author = {Vilnrotter, V. and Andrews, K. and Hamkins, J. and Tkacenko, A.},
  month = mar,
  year = {2010},
  keywords = {Space vehicles,space communication links,space vehicles,Telemetry,Navigation,space telemetry,Data mining,Downlink,antenna gain,Bandwidth,carrier tracking loops,CrameÃÂ¿r-Rao bounds,data transmission,deep-space navigation,Delay estimation,Doppler estimation,downlink telemetry,Maximum likelihood decoding,maximum likelihood estimation,Maximum likelihood estimation,navigation parameters,pseudo-noise sequences,pseudonoise codes,range estimation,ranging tones,spacecraft trajectory solution,Tracking loops},
  pages = {1-9},
  file = {/home/yuri/Zotero/storage/UAI62PCZ/Vilnrotter et al. - 2010 - Maximum likelihood estimation of navigation parame.pdf;/home/yuri/Zotero/storage/9QKZA5MT/5446954.html}
}
% == BibTeX quality report for vilnrotterMaximumLikelihoodEstimation2010:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{nassarStatisticalLearningApproach2016,
  title = {Statistical Learning Approach for Spacecraft Systems Health Monitoring},
  abstract = {The operations support technology continues to seek more efficient and effective ways to monitor for spacecraft operations and future low earth orbit exploration missions and beyond. This search for improvement has led to a significant movement to advance mission operations monitoring tools. Anomaly detection is an important field for the anticipation of spacecraft operations, working as an enabler of diagnostic and prognostic functions. This article discusses a new real application of a well-known data driven statistical software known as soft independent modeling for class analogy (SIMCA-P) developed by Umetrics to historical telemetry for attitude determination and control system (ADCS) of actual remote sensing spacecraft. Our design work is to detect anomalies from mission control center (MCC) through analyzing the telemetry readings received by MCC and respond to ADCS off-nominal situations by sending corrective actions tele-commands within real time to avoid critical and risky situations. We have implemented our approach on the engineering model of Egyptian satellite project Egypt-Sat1 and this model was our vehicle to simulate and verify the results. In conclusion, the analysis results provide a deep insight information and physical interpretation about the ADCS performance behavior.},
  booktitle = {2016 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2016.7500497},
  author = {Nassar, B. and Hussein, W.},
  month = mar,
  year = {2016},
  keywords = {artificial satellites,aerospace computing,Data models,learning (artificial intelligence),Monitoring,satellite telemetry,statistical analysis,Principal component analysis,ADCS,advance mission operation monitoring tools,anomaly detection,attitude determination and control system,condition monitoring,data driven statistical software,Detection algorithms,Egypt-Sat1,Egyptian satellite project engineering model,historical telemetry,low earth orbit exploration missions,MCC,mission control center,operations support technology,prognostic functions,remote sensing spacecraft,SIMCA-P,soft independent modeling for class analogy,Space shuttles,spacecraft operations,spacecraft systems health monitoring,statistical learning approach,telemetry reading analysis,Training data},
  pages = {1-9},
  file = {/home/yuri/Zotero/storage/RMQ5GLDW/Nassar and Hussein - 2016 - Statistical learning approach for spacecraft syste.pdf;/home/yuri/Zotero/storage/WVRR2RIM/7500497.html}
}
% == BibTeX quality report for nassarStatisticalLearningApproach2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{mohammadOpenSpaceBox2015,
  title = {Open {{Space Box}}: Communication to Support {{Big Data}} in Orbit},
  volume = {9468},
  shorttitle = {Open {{Space Box}}},
  abstract = {Communication to and from a small spacecraft can be at an extremely slow Baud rate, means both sending and receiving any communication will take some time. Extract, Transform and Load tools designed to transmit and receive data needs to have a flexible protocol. The Open Space Box model provides this base for smaller spacecraft to provide users data in a fashion that is pervasive within satellites as well as the ground stations. It also autonomically distinguishes data streams and disseminates relevant information to the related end users. Streaming Data can also be considered the generation of Big Data. At a ground station, the receiving of data can create the problem of Big Data and its management. Messages are sent in batch mode and communications are done using MapReduce.},
  booktitle = {Unmanned {{Systems Technology XVII}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2176728},
  author = {Mohammad, Atif Farid and Straub, Jeremy},
  month = may,
  year = {2015},
  pages = {94680P},
  file = {/home/yuri/Zotero/storage/LJRG4A3R/Mohammad and Straub - 2015 - Open Space Box communication to support Big Data .pdf;/home/yuri/Zotero/storage/IJFRL85N/12.2176728.html}
}
% == BibTeX quality report for mohammadOpenSpaceBox2015:
% ? Unsure about the formatting of the booktitle

@inproceedings{mohammadEconomicAnalysisOpen2015,
  title = {Economic Analysis of Open Space Box Model Utilization in Spacecraft},
  volume = {9469},
  abstract = {It is a known fact that the amount of data about space that is stored is getting larger on an everyday basis. However, the utilization of Big Data and related tools to perform ETL (Extract, Transform and Load) applications will soon be pervasive in the space sciences. We have entered in a crucial time where using Big Data can be the difference (for terrestrial applications) between organizations underperforming and outperforming their peers. The same is true for NASA and other space agencies, as well as for individual missions and the highly-competitive process of mission data analysis and publication. In most industries, conventional opponents and new candidates alike will influence data-driven approaches to revolutionize and capture the value of Big Data archives. The Open Space Box Model is poised to take the proverbial ``giant leap'', as it provides autonomic data processing and communications for spacecraft. We can find economic value generated from such use of data processing in our earthly organizations in every sector, such as healthcare, retail. We also can easily find retailers, performing research on Big Data, by utilizing sensors driven embedded data in products within their stores and warehouses to determine how these products are actually used in the real world.},
  booktitle = {Sensors and {{Systems}} for {{Space Applications VIII}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2176724},
  author = {Mohammad, Atif Farid and Straub, Jeremy},
  month = may,
  year = {2015},
  pages = {94690P},
  file = {/home/yuri/Zotero/storage/U6F2QHYM/Mohammad and Straub - 2015 - Economic analysis of open space box model utilizat.pdf;/home/yuri/Zotero/storage/4PIZAENS/12.2176724.html}
}
% == BibTeX quality report for mohammadEconomicAnalysisOpen2015:
% ? Unsure about the formatting of the booktitle

@inproceedings{sherwoodLessonsImplementationBeacon2000,
  title = {Lessons from Implementation of Beacon Spacecraft Operations on {{Deep Space One}}},
  volume = {2},
  abstract = {A new approach to mission operations has been flight validated on NASA's Deep Space One (DS1) mission that launched in October 1998. The beacon monitor operations technology is aimed at decreasing the total volume of downlinked engineering telemetry by reducing the frequency of downlink and the volume of data received per pass. Cost savings are achieved by reducing the amount of routine telemetry processing and analysis performed by ground staff. With beacon monitoring, the spacecraft will assess its own health and will transmit one of four subcarrier frequency tones to inform the ground how urgent it is to track the spacecraft for telemetry. If all conditions are nominal, the tone provides periodic assurance to ground personnel that the mission is proceeding as planned without having to receive and analyze downlinked telemetry. If there is a problem, the tone will indicate that tracking is required and the resulting telemetry will contain a concise summary of what has occurred since the last telemetry pass. The beacon technology has been proven successful on DS1 through a series of tone tests and data summarization experiments. This collection of experiments was called the DS1 Beacon Monitor Experiment or BMOX. There are important lessons still to be learned from this experiment that can be applied to future spacecraft missions},
  booktitle = {2000 {{IEEE Aerospace Conference}}. {{Proceedings}} ({{Cat}}. {{No}}.{{00TH8484}})},
  doi = {10.1109/AERO.2000.878245},
  author = {Sherwood, R. and Schlutsmeyer, A. and Sue, M. and Wyatt, E. J.},
  year = {2000},
  keywords = {Space vehicles,Mars,space vehicles,ground support systems,Monitoring,NASA,space research,Telemetry,space telemetry,astronomical instruments,Space missions,Downlink,data summarization,astronomy,Aerospace engineering,asteroid,astronomical techniques,beacon spacecraft operations,BMOX,comet,Costs,Data engineering,Deep Space One,downlinked engineering telemetry,Frequency,ground personnel,interplanetary mission,mission operations,planet,routine telemetry processing,Space technology,spacecraft mission,subcarrier frequency tones,tone tests},
  pages = {377-387 vol.2},
  file = {/home/yuri/Zotero/storage/8BID6BQV/Sherwood et al. - 2000 - Lessons from implementation of beacon spacecraft o.pdf;/home/yuri/Zotero/storage/YBVKBZBU/878245.html}
}
% == BibTeX quality report for sherwoodLessonsImplementationBeacon2000:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{zhangBigDataFramework2017,
  title = {A Big Data Framework for Spacecraft Prognostics and Health Monitoring},
  abstract = {The capability of prognostics and health monitoring of spacecraft such as satellites and spaceships is of key importance for their correct operation and the success of their mission. Before launching, various experiments are usually carried out on the spacecraft to thoroughly examine its performance and reliability, resulting in a large amount of experiment/test data. During its operation in orbit, the spacecraft will periodically generate operation data which can be collected by ground stations. The analysis and management of the spacecraft experiment and operation big data are essential for the monitoring of the system health and the prediction of potential failures. In this paper, we propose a complete framework for the collection, cleansing, storage, analysis and visualization of the spacecraft experiment and operation big data. All components of this framework such as the database, webserver and middleware are built on mature open source software frameworks. This big data framework has been deployed on a number of Linux servers and operates well in production.},
  booktitle = {2017 {{Prognostics}} and {{System Health Management Conference}} ({{PHM}}-{{Harbin}})},
  doi = {10.1109/PHM.2017.8079320},
  author = {Zhang, X. and Wu, P. and Tan, C.},
  month = jul,
  year = {2017},
  keywords = {Space vehicles,aerospace computing,space vehicles,data analysis,Monitoring,Servers,Big Data,Data visualization,Aircraft manufacture,public domain software,big data,condition monitoring,big data framework,framework,Linux,Linux servers,open source software frameworks,operation big data,PHM,Prognostics and health management,satellite,spacecraft,spacecraft prognostics,system health monitoring},
  pages = {1-7},
  file = {/home/yuri/Zotero/storage/E4D7ND2N/Zhang et al. - 2017 - A big data framework for spacecraft prognostics an.pdf;/home/yuri/Zotero/storage/5GBM8NKD/8079320.html}
}
% == BibTeX quality report for zhangBigDataFramework2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{miebachHubbleSpaceTelescope1998,
  title = {Hubble {{Space Telescope}}: Cost Reduction by Re-Engineering Telemetry Processing and Archiving},
  volume = {3351},
  shorttitle = {Hubble {{Space Telescope}}},
  abstract = {The Hubble Space Telescope (HST), the first of NASA's Great Observatories, was launched on April 24, 1990. The HST was designed for a minimum fifteen-year mission with on-orbit servicing by the Space Shuttle System planned at approximately three-year intervals. Major changes to the HST ground system are planned to be in place for the third servicing mission in December 1999. The primary objectives of the ground system reengineering effort, a project called 'vision December 1999. The primary objectives of the ground system re-engineering effort, a project called 'vision 2000 control center systems (CCS)', are to reduce both development and operating costs significantly for the remaining years of HST's lifetime. Development costs will be reduced by providing a modern hardware and software architecture and utilizing commercial of f the shelf (COTS) products wherever possible. Operating costs will be reduced by eliminating redundant legacy systems and processes and by providing an integrated ground system geared toward autonomous operation. Part of CCS is a Space Telescope Engineering Data Store, the design of which is based on current Data Warehouse technology. The purpose of this data store is to provide a common data source of telemetry data for all HST subsystems. This data store will become the engineering data archive and will include a queryable database for the user to analyze HST telemetry. The access to the engineering data in the Data Warehouse is platform- independent from an office environment using commercial standards. Latest internet technology is used to reach the HST engineering community. A WEB-based user interface allows easy access to the data archives. This paper will provide a high level overview of the CCS system and will illustrate some of the CCS telemetry capabilities. Samples of CCS user interface pages will be given. Vision 2000 is an ambitious project, but one that is well under way. It will allow the HST program to realize reduced operations costs for the Third Servicing Mission and beyond.},
  booktitle = {Telescope {{Control Systems III}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.308828},
  author = {Miebach, Manfred P.},
  month = may,
  year = {1998},
  pages = {127-135},
  file = {/home/yuri/Zotero/storage/ZIW37YAC/Miebach - 1998 - Hubble Space Telescope cost reduction by re-engin.pdf;/home/yuri/Zotero/storage/BFRE9XIM/12.308828.html}
}
% == BibTeX quality report for miebachHubbleSpaceTelescope1998:
% ? Unsure about the formatting of the booktitle

@inproceedings{nassarStateofhealthAnalysisApplied2015,
  title = {State-of-Health Analysis Applied to Spacecraft Telemetry Based on a New Projection to Latent Structure Discriminant Analysis Algorithm},
  abstract = {The potential for space mission operations and the supporting ground infrastructure is growing dramatically, fueled by new technologies, but with that growth comes increased complexity, and daunting reliability and security challenges. And like most complex endeavors, space operations are being asked to do more with less. In order to deliver cost-effective space operations services, researchers must explore novel ways to build and operate systems under study. Innovation is the engine that drives progress in today's high-tech global economy. Statistical multivariate latent techniques are one of the vital learning tools that are used to tackle the aforementioned problem coherently. There has been a tremendous increase in the volume of telemetry data over the last decade from contemporary spacecrafts. All these datasets need to be analyzed for finding interesting patterns or for searching for both moderate and significant outliers. Information extraction from such rich data sources using advanced statistical methodologies is a challenging task due to the massive volume of data. To solve this problem, in this paper, we present a novel supervised learning algorithm based on projection to latent structure discriminant analysis technique (PLS-DA). The algorithm is particularly uses to model, analyze, classify telemetry data and identify key contributors to anomalous events while simultaneously measuring several predictors and response variables. The performance of the algorithm using the telemetry acquired from of attitude determination and control system (ADCS) of actual remote sensing spacecraft was presented. In addition, a critical compression between the analysis results obtained by the algorithm and the multivariate statistical analysis software Simca-P developed by Umetrics was presented. Finally, the algorithm provides competent information in modelling, classifying, diagnosis and prediction as well as adding more insight and physical interpretation to the ADCS s- ate of health (SOH).},
  booktitle = {2015 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2015.7118887},
  author = {Nassar, B. and Hussein, W.},
  month = mar,
  year = {2015},
  keywords = {Software,learning (artificial intelligence),space vehicles,spacecraft telemetry,Telemetry,space telemetry,aerospace instrumentation,statistical analysis,Load modeling,ADCS,attitude determination and control system,ADCS state of health,advanced statistical methodology,Biographies,computational complexity,cost-effective space operations services,Hidden Markov models,latent structure discriminant analysis algorithm,multivariate statistical analysis software,Software measurement,space mission operations,state-of-health analysis,statistical multivariate latent technique,supervised learning algorithm},
  pages = {1-11},
  file = {/home/yuri/Zotero/storage/GCYIFYKC/Nassar and Hussein - 2015 - State-of-health analysis applied to spacecraft tel.pdf;/home/yuri/Zotero/storage/3QZAL5KI/7118887.html}
}
% == BibTeX quality report for nassarStateofhealthAnalysisApplied2015:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{asundiDesignCommandData2013,
  title = {Design of Command, Data and Telemetry Handling System for a Distributed Computing Architecture {{CubeSat}}},
  abstract = {Among the size, weight and power constraints imposed by the CubeSat specification, the limitation associated with power can be addressed through a distributed computing architecture. This paper describes such a distributed computing architecture and its operational design in the form of command and data handling system and telemetry formulation, adapted for a CubeSat whose power requirements for proving the mission are significantly larger than the on-orbit average power generated. The 1U CubeSat with the mission objective of precision three axes attitude control is composed of a low power flight computer and a high power, high speed auxiliary processor (CMG controller), along with a high capacity battery. The precision sensors, actuators and complex computing algorithms, are interfaced and implemented on the high speed auxiliary processor, which is operated intermittently. Health monitoring sensors, transceiver and other housekeeping tasks are interfaced and implemented on the flight computer, which is in continuous operation. To facilitate effective operation and telemetry packaging, each computing unit is designed to host a storage device. The flight software, designed as operating modes, is distributed across the two computing platforms. Distributed operations are initiated through the flight computer and executed on the auxiliary processor. The paper describes in detail the distributed design of these operating modes as flowcharts and the associated telemetry budget as tables.},
  booktitle = {2013 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2013.6496901},
  author = {Asundi, S. A. and {Fitz-Coy}, N. G.},
  month = mar,
  year = {2013},
  keywords = {Satellites,Computer architecture,microprocessor chips,Sensors,Telemetry,satellite telemetry,attitude control,data handling,condition monitoring,1U CubeSat specification,actuator,actuators,aircraft computers,auxiliary processor,command and data handling system,complex computing algorithm,Computers,Distributed computing,distributed computing architecture,distributed programming,flight computer,flowcharting,flowcharts,health monitoring sensor,housekeeping task,operational design,power requirements,precision sensor,Receivers,sensors,storage device,telemetry handling system,telemetry packaging,transceiver,transceivers},
  pages = {1-14},
  file = {/home/yuri/Zotero/storage/VPJCP6HM/Asundi and Fitz-Coy - 2013 - Design of command, data and telemetry handling sys.pdf;/home/yuri/Zotero/storage/CESQYLN9/6496901.html}
}
% == BibTeX quality report for asundiDesignCommandData2013:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{liaghatiNovelSchemeTelemetry2016,
  title = {A Novel Scheme for Telemetry System Data Rate Optimization},
  abstract = {Limited telemetry bandwidth due to restricted radio frequency spectrum allocation is typically one of the most challenging problems when designing a telemetry system for space applications. To further complicate the problem, a large percentage of the allotted bandwidth is consumed by the over-head required by each packet to follow various standards and layer of protocols used in the telemetry system. This results in inefficiency in the actual telemetry data downlinked to the ground station. In the typical telemetry design, only one virtual channel is used per packet. As a result, in order to achieve the required packet size, filled data is inserted into the remaining bits. This Idle Packet consists of a set pattern of binary digits, and is considered part of the overhead, as its sole purpose is to fill the packet. In this work, a new scheme is proposed which takes advantage of the empty bits by starting the next virtual channel instead of inserting filled bits. This method will reduce the overall amount of overheard, in addition to allowing more data to be transmitted.},
  booktitle = {2016 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2016.7500762},
  author = {Liaghati, A. and Chang, N. and Liaghati, M. and Maffei, A.},
  month = mar,
  year = {2016},
  keywords = {Protocols,space communication links,space applications,Telemetry,Downlink,access protocols,Payloads,Bandwidth,binary digits,Buffer storage,data rate optimization,Encapsulation,ground station,idle packet,limited telemetry bandwidth,packet radio networks,protocol layer,radiotelemetry,resource allocation,restricted radiofrequency spectrum allocation,telemetry system,virtual channel},
  pages = {1-7},
  file = {/home/yuri/Zotero/storage/BRTE3SY7/Liaghati et al. - 2016 - A novel scheme for telemetry system data rate opti.pdf;/home/yuri/Zotero/storage/VGI87QCF/7500762.html}
}
% == BibTeX quality report for liaghatiNovelSchemeTelemetry2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{bagriAccurateSpacecraftAngular2009,
  title = {Accurate Spacecraft Angular Position from {{DSN VLBI}} Phases Using {{X}}-Band Telemetry or {{DOR}} Tones},
  abstract = {At present spacecraft angular position with Deep Space Network (DSN) is determined using group delay estimates from very long baseline interferometer (VLBI) phase measurements employing differential one way ranging (DOR) tones. As an alternative to this approach, we propose estimating position of a spacecraft to half a fringe cycle accuracy using time variations between measured and calculated phases as the Earth rotates using DSN VLBI baseline(s). Combining fringe location of the target with the phase allows high accuracy for spacecraft angular position estimate. This can be achieved using telemetry signals of at least 4-8 MSamples/sec data rate or DOR tones.},
  booktitle = {2009 {{IEEE Aerospace}} Conference},
  doi = {10.1109/AERO.2009.4839367},
  author = {Bagri, D. S. and Majid, W.},
  month = mar,
  year = {2009},
  keywords = {Space vehicles,Earth,space vehicles,telemetry,Telemetry,Delay estimation,angular measurement,deep space network,Extraterrestrial measurements,group delay estimates,interferometry,Phase estimation,phase measurement,Phase measurement,phase measurements,Position measurement,Rotation measurement,spacecraft angular position,target fringe location,Time measurement,very long baseline interferometer,X-band telemetry},
  pages = {1-7},
  file = {/home/yuri/Zotero/storage/LZKZWU37/Bagri_Majid_2009_Accurate spacecraft angular position from DSN VLBI phases using X-band.pdf;/home/yuri/Zotero/storage/VGXEGDGS/4839367.html}
}
% == BibTeX quality report for bagriAccurateSpacecraftAngular2009:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{losikUsingTelemetryMeasure2012,
  title = {Using Telemetry to Measure Equipment Usable Life on the {{NASA Orion}} Spacecraft},
  abstract = {The NASA Orion manned spacecraft will replace the NASA Space Shuttle for getting astronauts to low earth orbit, moon and Mars and returned to the earth safely. It also serves as a crew escape vehicle at the ISS to increase astronaut safety The NASA integrated, vehicle health management (IVHM) program has been adopted by many segments of the aerospace industry. The IVHM is the tool to prevent catastrophic failures by identifying the equipment that will fail prematurely for replacement, thus preventing/eliminating surprise equipment failures from threatening both safety and mission success. Due to the extreme environments experienced by both the launch vehicle and its payload (s) during launch and while operating in space the space industry often rejects most advancements made in other aerospace industries in technology related to and improving reliability. Personnel on the ground responsible for launch vehicle and spacecraft receive limited technical information from an on-board telemetry system that generates routes engineering data, whose accuracy is suspect, available from spacecraft when both planned and unplanned events occur. Included in this paper are examples of spacecraft equipment failures that have been accurately predicted to occur or prevented from occurring. We provide many examples of equipment telemetry measurements used to measure spacecraft equipment usable life. The equipment with a measured usable will fail prematurely with certainty. The equipment that will fail prematurely should be replaced at the factory or if the spacecraft is in space, the failure can be managed to a positive conclusion. This paper summarizes the use of a prognostic and health management program including using predictive algorithms to measure equipment life and predict remaining usable life with certainty on the many NASA space missions as specified in the NASA IVHM technical plan published in 2009. The non-Markov reliability paradigm for measuring equipment usable life wa- specifically developed for preventing surprise catastrophic equipment failures. It is already in use on the Air Force F-35 Joint Strike Fighter. The decision to use a PHM is in part, based on the results from our prognostic analysis we completed on the NASA GSFC Extreme Ultra Violet Explorer LEO space science satellite and the NASA Space Shuttle Challenger and Columbia accidents. Our results identified the information that was present but was ignored for a variety of reasons that would have prevented both Space Shuttle accidents if the information had been properly interpreted.},
  booktitle = {2012 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2012.6187384},
  author = {Losik, L.},
  month = mar,
  year = {2012},
  keywords = {artificial satellites,Mars,aerospace safety,NASA,Telemetry,Moon,space telemetry,Space shuttles,PHM,Prognostics and health management,spacecraft,aerospace industries,air force F-35 joint strike fighter,catastrophic failures,earth orbit,ISS,IVHM program,launch vehicle,NASA GSFC extreme ultra violet explorer LEO space science satellite,NASA integrated vehicle health management program,NASA Orion manned spacecraft,NASA space shuttle accident,nonMarkov reliability paradigm,on-board telemetry system,Prediction algorithms,reliability,safety},
  pages = {1-15},
  file = {/home/yuri/Zotero/storage/8NWSA4A2/Losik - 2012 - Using telemetry to measure equipment usable life o.pdf;/home/yuri/Zotero/storage/ICLRTK66/6187384.html}
}
% == BibTeX quality report for losikUsingTelemetryMeasure2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{epperlyLevel0TelemetryCollection2001,
  title = {Level-0 Telemetry Collection for Spacecraft Command and Data Handling Subsystems},
  volume = {5},
  abstract = {This paper presents the continuing development of flexible Level-0 command and telemetry collection hardware. Results of the requirements analysis, systems design and hardware fabrication and test are presented in detail. The objective of this research program was to provide a module that could handle a variety of interfaces accessible from either a spacecraft's main computer or an on-board microcontroller. The module must serve as a level-0 ``safe'' mode processor and be able to execute commands received from the ground. The system designed as a result of this IR\&D will be used in those applications where a critical need exists for a level-0 ``safe'' mode processor. The design may also be used to speed up system throughput by way of parallel processing with the main computer},
  booktitle = {2001 {{IEEE Aerospace Conference Proceedings}} ({{Cat}}. {{No}}.{{01TH8542}})},
  doi = {10.1109/AERO.2001.931176},
  author = {Epperly, M. E. and Walls, B. J.},
  year = {2001},
  keywords = {Space vehicles,radiation hardening (electronics),aerospace computing,Microcontrollers,space vehicle electronics,aerospace control,Telemetry,space telemetry,data handling,parallel processing,system buses,Application software,command and control systems,Computer interfaces,Fabrication,flexible collection hardware,flexible interface capability,Hardware,hardware fabrication,hardware test,level-0 telemetry collection,microcontrollers,on-board microcontroller,Process design,requirements analysis,safe mode processor,SEU,spacecraft command and data handling subsystem,spacecraft control,System analysis and design,System testing,systems analysis,systems design,VME bus},
  pages = {2191-2198 vol.5},
  file = {/home/yuri/Zotero/storage/XNH4XLZY/Epperly and Walls - 2001 - Level-0 telemetry collection for spacecraft comman.pdf;/home/yuri/Zotero/storage/M793IWNI/931176.html}
}
% == BibTeX quality report for epperlyLevel0TelemetryCollection2001:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{staudingerLosslessCompressionArchiving2000,
  title = {Lossless Compression for Archiving Satellite Telemetry Data},
  volume = {2},
  abstract = {We present a method for lossless compression of satellite data for archiving. The method is of low complexity and has been demonstrated to work successfully on actual telemetry data},
  booktitle = {2000 {{IEEE Aerospace Conference}}. {{Proceedings}} ({{Cat}}. {{No}}.{{00TH8484}})},
  doi = {10.1109/AERO.2000.878236},
  author = {Staudinger, P. and Hershey, J. and Grabb, M. and Joshi, N. and Ross, F. and Nowak, T.},
  year = {2000},
  keywords = {Satellites,Monitoring,Telemetry,satellite telemetry,probability,data reduction,data compression,archiving,Autocorrelation,bit probability,complexity,frame based telemetry,Information retrieval,lossless compression,Mobile communication,Mobile handsets,Reactive power,Research and development,Sampling methods,satellite telemetry data},
  pages = {299-304 vol.2},
  file = {/home/yuri/Zotero/storage/KEG865V5/Staudinger et al. - 2000 - Lossless compression for archiving satellite telem.pdf;/home/yuri/Zotero/storage/CJICNIX3/878236.html}
}
% == BibTeX quality report for staudingerLosslessCompressionArchiving2000:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@incollection{evansDataMiningDrastically2016,
  series = {{{SpaceOps Conferences}}},
  title = {Data {{Mining}} to {{Drastically Improve Spacecraft Telemetry Checking}}: {{A Scientist}}?S {{Approach}}},
  shorttitle = {Data {{Mining}} to {{Drastically Improve Spacecraft Telemetry Checking}}},
  booktitle = {{{SpaceOps}} 2016 {{Conference}}},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  author = {Evans, David J. and Martinez, Jose and {Korte-Stapff}, Moritz and Vandenbussche, Bart and Royer, Pierre and De Ridder, Joris},
  month = may,
  year = {2016},
  file = {/home/yuri/Zotero/storage/46NW7VGA/Evans et al. - 2016 - Data Mining to Drastically Improve Spacecraft Tele.pdf;/home/yuri/Zotero/storage/T5HZSC3B/6.html},
  doi = {10.2514/6.2016-2398}
}
% == BibTeX quality report for evansDataMiningDrastically2016:
% Missing required field 'pages'

@article{suciuBigDataProcessing2016,
  title = {Big {{Data Processing}} for {{Renewable Energy Telemetry Using}} a {{Decentralized Cloud M2M System}}},
  volume = {87},
  issn = {0929-6212, 1572-834X},
  abstract = {Hydro-, Aeolian and Solar energy show significant promise in helping to reduce pollutants and greenhouse gases, which is a primary focus in today's sustainable development culture. To enjoy the power of them, the human society needs solutions to transform the energies mentioned above into electric energy, at specific reliability, efficiency and sustainability parameters. Maintaining this kind of parameters implies, among other things, the careful and permanent monitoring of equipment. The monitoring process implies remote monitoring, as we are talking about preserving natural resources. The equipment is mostly situated in the middle of nature, covering large areas mostly outside of populated locations. In their great majority installations for wind and solar energy have been designed and manufactured relatively recently, which makes them contain systems of tele-monitoring that are designed and included by default, as a part of great importance to the entire investment. The situation is different in the case of hydroelectric plants, which have a tradition of over 130 years and have been and are still built to this day. Renewable energy sources are being increasingly used and need to be constantly monitored for optimizing the power grid. Unfortunately, such micro power plants are located in difficult to access remote locations where often only satellite or sparse GSM radio signals are available. In this paper we study the way how to process big data gathered by a decentralized cloud system, based on general systems and remote telemetry units (RTUs), for tele-monitoring of renewable energy objectives. Also, we analyze a proposed cloud M2M system, where each RTU communicates by radio with a telemetry data gateway connected to a cloud computing infrastructure equipped with appropriate software that delivers processed data. Furthermore, we present how we use a search based application built on Exalead CloudView to search for weak signals in big data. In particular, given the telemetry application, we propose to leverage trivial and non-trivial connections between different sensor signals and data from other online environmental wireless sensor networks, in order to find patterns that are likely to provide innovative solutions to existing problems. The aggregation of such weak signals will provide evidence of connections between renewable energies and environmental related issues faster and better than trivial mining of sensor data. As a consequence, the software has a significant potential for matching environmental applications and challenges that are related in non-obvious ways. Finally, we present the measurement results for a hydro-energy case study and discuss the applicability for other renewable sources such as solar or wind energy.},
  language = {en},
  number = {3},
  journal = {Wireless Personal Communications},
  doi = {10/gdk2j8},
  author = {Suciu, George and Vulpe, Alexandru and Martian, Alexandru and Halunga, Simona and Vizireanu, Dragos Nicolae},
  month = apr,
  year = {2016},
  pages = {1113-1128},
  file = {/home/yuri/Zotero/storage/PIFD4UE6/Suciu et al. - 2016 - Big Data Processing for Renewable Energy Telemetry.pdf;/home/yuri/Zotero/storage/5CMGWAB6/10.html}
}
% == BibTeX quality report for suciuBigDataProcessing2016:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{suciuM2MRemoteTelemetry2015,
  title = {{{M2M}} Remote Telemetry and Cloud {{IoT}} Big Data Processing in Viticulture},
  abstract = {Current M2M communication platforms are being integrated in cloud IoT applications for providing remote sensing and actuating. Nevertheless, requirements for energy efficiency and resilience in severe operating environments are driving the development of new algorithms and infrastructures. This paper presents a survey of the measurement results for the winegrowing season 2014, as it was seen by an M2M remote telemetry station in cooperation with a big data processing platform and several sensors. We demonstrate the use of recent technologies such as Cloud IoT systems and Big Data processing in order to implement disease prediction and alerting application for viticulture. Finally, the extension of the proposed system for other agriculture applications is discussed.},
  booktitle = {2015 {{International Wireless Communications}} and {{Mobile Computing Conference}} ({{IWCMC}})},
  doi = {10.1109/IWCMC.2015.7289239},
  author = {Suciu, G. and Vulpe, A. and Fratu, O. and Suciu, V.},
  month = aug,
  year = {2015},
  keywords = {Telemetry,Big Data,remote sensing,Big data,Cloud computing,Agriculture,agriculture applications,big data processing,big data processing platform,Cloud,cloud IoT big data processing,IoT,M2M,M2M communication platforms,M2M remote telemetry station,mobile communication,remote actuating,Remote telemetry,Temperature sensors,viticulture,Viticulture},
  pages = {1117-1121},
  file = {/home/yuri/Zotero/storage/N6F8MNST/Suciu et al. - 2015 - M2M remote telemetry and cloud IoT big data proces.pdf;/home/yuri/Zotero/storage/N4RYQ89Q/7289239.html}
}
% == BibTeX quality report for suciuM2MRemoteTelemetry2015:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{ahmedRoleBigData2017,
  series = {Special {{Issue}} on {{5G Wireless Networks}} for {{IoT}} and {{Body Sensors}}},
  title = {The Role of Big Data Analytics in {{Internet}} of {{Things}}},
  volume = {129},
  issn = {1389-1286},
  abstract = {The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.},
  journal = {Computer Networks},
  doi = {10/f99nm9},
  author = {Ahmed, Ejaz and Yaqoob, Ibrar and Hashem, Ibrahim Abaker Targio and Khan, Imran and Ahmed, Abdelmuttlib Ibrahim Abdalla and Imran, Muhammad and Vasilakos, Athanasios V.},
  month = dec,
  year = {2017},
  keywords = {Analytics,Big data,Distributed computing,Internet of Things,Smart city},
  pages = {459-471},
  file = {/home/yuri/Zotero/storage/5JWKX6ND/Ahmed et al. - 2017 - The role of big data analytics in Internet of Thin.pdf;/home/yuri/Zotero/storage/V35I56SJ/S1389128617302591.html}
}
% == BibTeX quality report for ahmedRoleBigData2017:
% Missing required field 'number'

@article{chaudhuriOverviewDataWarehousing1997,
  title = {An Overview of Data Warehousing and {{OLAP}} Technology},
  volume = {26},
  issn = {01635808},
  abstract = {Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed. This overview is based on a tutorial that the authors presented at the VLDB Conference, 1996.},
  language = {en},
  number = {1},
  journal = {ACM SIGMOD Record},
  doi = {10/bst468},
  author = {Chaudhuri, Surajit and Dayal, Umeshwar},
  month = mar,
  year = {1997},
  pages = {65-74},
  file = {/home/yuri/Zotero/storage/5F3DLQGS/Chaudhuri and Dayal - 1997 - An overview of data warehousing and OLAP technolog.pdf;/media/yuri/7EA5CFD816134804/INPE/master/Articles/Chaudhuri and Dayal - 1997 - An overview of data warehousing and OLAP technolog.pdf}
}

@article{morfoniosROLAPImplementationsData2007,
  title = {{{ROLAP}} Implementations of the Data Cube},
  volume = {39},
  issn = {03600300},
  language = {en},
  number = {4},
  journal = {ACM Computing Surveys},
  doi = {10/b5f2sh},
  author = {Morfonios, Konstantinos and Konakas, Stratis and Ioannidis, Yannis and Kotsis, Nikolaos},
  month = nov,
  year = {2007},
  pages = {12-es},
  file = {/home/yuri/Zotero/storage/PGXTQMMN/Morfonios et al. - 2007 - ROLAP implementations of the data cube.pdf;/media/yuri/7EA5CFD816134804/INPE/master/Articles/Morfonios et al. - 2007 - ROLAP implementations of the data cube.pdf}
}

@incollection{bouazizTraditionalDataWarehouse2017,
  address = {{Cham}},
  title = {From {{Traditional Data Warehouse To Real Time Data Warehouse}}},
  volume = {557},
  isbn = {978-3-319-53479-4 978-3-319-53480-0},
  abstract = {The Traditional data warehouse did not contain data as today. Hence, it is difficult to retrieve these data and treat them. Furthermore, its content is not updated, which may lead to bad decisions. Data are typically loaded from conventional operational systems. Given that today's decisions, in the business world, are becoming more and more in real time. Thus, the system supporting these decisions is to be held. In this paper, we are interested in giving a survey on data warehousing starting from a traditional data warehouse to a real time data warehouse. This survey, focus firstly, on data warehouse architecture. Secondly, it details the changes in the Extract-Transform-Load process to deal with real time data warehousing. Thirdly, it sketches the integration data in the real time data warehouse. Finally, a comparative study concerning the real data warehouse approaching is also presented in this paper.},
  language = {en},
  booktitle = {Intelligent {{Systems Design}} and {{Applications}}},
  publisher = {{Springer International Publishing}},
  author = {Bouaziz, Senda and Nabli, Ahlem and Gargouri, Faiez},
  editor = {Madureira, Ana Maria and Abraham, Ajith and Gamboa, Dorabela and Novais, Paulo},
  year = {2017},
  pages = {467-477},
  file = {/home/yuri/Zotero/storage/Q36EDCR9/Bouaziz et al. - 2017 - From Traditional Data Warehouse To Real Time Data .pdf},
  doi = {10.1007/978-3-319-53480-0_46}
}
% == BibTeX quality report for bouazizTraditionalDataWarehouse2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{grayDataCubeRelational1996,
  title = {Data Cube: A Relational Aggregation Operator Generalizing {{GROUP}}-{{BY}}, {{CROSS}}-{{TAB}}, and {{SUB}}-{{TOTALS}}},
  isbn = {978-0-8186-7240-8},
  shorttitle = {Data Cube},
  abstract = {Data analysis applications typ\&ally aggregate data across many dimensions lookingfor unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional answers. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, cross-tabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The cube treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. Aggregation points are represented by an "infinite value", ALL, so the point (ALL,ALL,...,ALL, sum(*)) represents the global sum of all items. Each ALL value actually represents the set of values contributing to that aggregation.},
  language = {en},
  publisher = {{IEEE Comput. Soc. Press}},
  doi = {10.1109/ICDE.1996.492099},
  author = {Gray, J. and Bosworth, A. and Lyaman, A. and Pirahesh, H.},
  year = {1996},
  pages = {152-159},
  file = {/home/yuri/Zotero/storage/48LGCAAB/Gray et al. - 1996 - Data cube a relational aggregation operator gener.pdf}
}
% == BibTeX quality report for grayDataCubeRelational1996:
% Missing required field 'booktitle'

@article{jainDataClusteringReview1999,
  title = {Data Clustering: A Review},
  volume = {31},
  issn = {03600300},
  shorttitle = {Data Clustering},
  language = {en},
  number = {3},
  journal = {ACM Computing Surveys},
  doi = {10/brjtj7},
  author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
  month = sep,
  year = {1999},
  pages = {264-323},
  file = {/home/yuri/Zotero/storage/9W7AI4P7/Jain et al. - 1999 - Data clustering a review.pdf;/media/yuri/7EA5CFD816134804/INPE/master/Articles/Jain et al. - 1999 - Data clustering a review.pdf}
}

@incollection{malikMLRIndexIndexStructure2009,
  address = {{Berlin, Heidelberg}},
  title = {{{MLR}}-{{Index}}: {{An Index Structure}} for {{Fast}} and {{Scalable Similarity Search}} in {{High Dimensions}}},
  volume = {5566},
  isbn = {978-3-642-02278-4 978-3-642-02279-1},
  shorttitle = {{{MLR}}-{{Index}}},
  abstract = {High-dimensional indexing has been very popularly used for performing similarity search over various data types such as multimedia (audio/image/video) databases, document collections, time-series data, sensor data and scientific databases. Because of the curse of dimensionality, it is already known that well-known data structures like kd-tree, R-tree, and M-tree suffer in their performance over high-dimensional data space which is inferior to a brute-force approach linear scan. In this paper, we focus on an approximate nearest neighbor search for two different types of queries: r-Range search and k-NN search. Adapting a novel concept of a ring structure, we define a new index structure MLR-Index (Multi-Layer Ring-based Index) in a metric space and propose time and space efficient algorithms with high accuracy. Evaluations through comprehensive experiments comparing with the bestknown high-dimensional indexing method LSH show that our approach is faster for a similar accuracy, and shows higher accuracy for a similar response time than LSH.},
  language = {en},
  booktitle = {Scientific and {{Statistical Database Management}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Malik, Rahul and Kim, Sangkyum and Jin, Xin and Ramachandran, Chandrasekar and Han, Jiawei and Gupta, Indranil and Nahrstedt, Klara},
  editor = {Winslett, Marianne},
  year = {2009},
  pages = {167-184},
  file = {/home/yuri/Zotero/storage/2GQAYNE4/55a925f208ae815a04253d99.pdf;/home/yuri/Zotero/storage/R7ZQ3TCK/Malik et al. - 2009 - MLR-Index An Index Structure for Fast and Scalabl.pdf},
  doi = {10.1007/978-3-642-02279-1_12}
}
% == BibTeX quality report for malikMLRIndexIndexStructure2009:
% ? Title looks like it was stored in title-case in Zotero

@incollection{gosainSystematicReviewMaterialized2017,
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  title = {A {{Systematic Review}} on {{Materialized View Selection}}},
  isbn = {978-981-10-3152-6 978-981-10-3153-3},
  abstract = {The purpose of materialized view selection is to minimize the cost of answering queries and fast query response time for timely access to information and decision support. Besides various research issues related to data warehouse evolution, materialized view selection is one of the most challenging ones. Various authors have given different methodologies, strategies and followed algorithms to solve this problem in an efficient manner. The main motivation behind this systematic review is to provide a path for future research scope in materialized view selection. Various techniques presented in the papers are identified, evaluated, and compared in terms of memory storage space, cost, and query processing time to find if any particular approach is superior to others. By means of a review of the available literature, the authors have drawn several conclusions about the status quo of materialized view selection and a future outlook is predicted on bridging the large gaps that were found in the existing methods.},
  language = {en},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Frontiers}} in {{Intelligent Computing}}: {{Theory}} and {{Applications}}},
  publisher = {{Springer, Singapore}},
  author = {Gosain, Anjana and Sachdeva, Kavita},
  year = {2017},
  pages = {663-671},
  file = {/home/yuri/Zotero/storage/FKMMS86R/Gosain and Sachdeva - 2017 - A Systematic Review on Materialized View Selection.pdf;/home/yuri/Zotero/storage/VWBFV4MV/10.html},
  doi = {10.1007/978-981-10-3153-3_66}
}
% == BibTeX quality report for gosainSystematicReviewMaterialized2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{lavinFPGAbasedSpacetimeCoded2008,
  title = {An {{FPGA}}-Based {{Space}}-Time Coded Telemetry Receiver},
  abstract = {The significant problem of data dropouts in aeronautical telemetry due to multiple transmit antennas has escalated as transmit data rates have increased. A proposed solution of using a space-time coded signal can resolve these data dropouts at the expense of increased receiver complexity. This paper describes an implementation overview of an FPGA-based space-time coded telemetry receiver and the various challenges associated with its realization. In addition, we discuss the productivity of the high-level design tool used in constructing the receiver, Xilinx system generator for DSP. With some overhead in terms of FPGA fabric usage and clock speed, our estimates show a 2 - 3x productivity improvement over standard HDLs.},
  booktitle = {2008 {{IEEE National Aerospace}} and {{Electronics Conference}}},
  doi = {10.1109/NAECON.2008.4806555},
  author = {Lavin, C. and Nelson, B. and Palmer, J. and Rice, M.},
  month = jul,
  year = {2008},
  keywords = {field programmable gate arrays,Field programmable gate arrays,Telemetry,space telemetry,Bandwidth,aeronautical telemetry,antenna arrays,Antennas and propagation,Computer vision,data dropout,DSP,FPGA-based space-time coded telemetry receiver,HDL,high-level design tool,multiple transmit antenna,Productivity,Receiving antennas,reliable communication link,space-time codes,telecommunication network reliability,Transmitters,Transmitting antennas,Vehicles,Xilinx system generator},
  pages = {250-256},
  file = {/home/yuri/Zotero/storage/Y8VE3PYF/Lavin et al. - 2008 - An FPGA-based Space-time coded telemetry receiver.pdf;/home/yuri/Zotero/storage/5FTH6UZD/4806555.html}
}
% == BibTeX quality report for lavinFPGAbasedSpacetimeCoded2008:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{losikUsingAnalogTelemetry2012,
  title = {Using Analog Telemetry to Measure Spacecraft Equipment Usable Life Upgrading Factory {{ATP}}},
  abstract = {When probability reliability analysis engineering was adopted in 1960 by the U.S. government, it became required use by spacecraft and launch vehicle suppliers to quantify spacecraft and launch vehicle reliability in probabilistic terms. Due to its use, spacecraft and launch vehicle suppliers stopped looking for the cause of premature equipment failures that was plaguing the industry. The government procurement contracts did not financially penalize space vehicle suppliers when their equipment failed prematurely, but did financially penalize them for a late delivery, increasing the motivation for space vehicle suppliers to overlook and misdiagnose equipment behavior in the test data that slowed down testing and increased risk of earning a financial penalty for a late delivery. The risk of a premature failure is so high that NASA GSFC pays for NOAA's long-life weather satellites after they get to space and function properly for at least one year. Military satellites fail prematurely so often that in 2010, the Air Force Space and Missiles Systems Center fined an Air Force space vehicle supplier \$15M for the premature failure of the apogee engine of the AEHF-1 satellite. The private space companies that own satellites and earn income selling the satellite services anticipate the premature failure of either their launch vehicle or satellite and purchase space insurance policies that cost up to \$60M to pay for all the financial losses that occur when their satellites and launch vehicles fail prematurely. To minimize the public from learning about the frequency that space vehicle equipment purchased by NASA and the military, fails prematurely, the information is proprietary to the company that produced it. The government agency's that purchase space vehicle equipment that fails prematurely do not want it to be known publically either and so the financial losses due to the premature failure of NASA and military satellites to the taxpayer is difficult to ascertain but is- identified by some sources as ranging from \$4B to \$10B per year. To stop the premature failure of space vehicle equipment, spacecraft equipment usable life can be measured using the same equipment telemetry data generated and used to measure and confirm equipment performance. Predictive algorithms use equipment analog telemetry to identify non-repeatable transient behavior in equipment telemetry related to equipment end-of-life. This paper will explain the technology, tools necessary to identify the space vehicle equipment that will fail prematurely from parts will accelerated aging in the space and ground segment environment with the presence of noise from a variety of sources.},
  booktitle = {2012 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.2012.6187383},
  author = {Losik, L.},
  month = mar,
  year = {2012},
  keywords = {Satellites,Space vehicles,Testing,space vehicles,NASA,Telemetry,satellite telemetry,Extraterrestrial measurements,reliability,AEHF-1 satellite,Air Force Space,analog telemetry,ground support equipment,launch vehicle reliability,military communication,military satellites,missiles,Missiles Systems Center,NASA GSFC,NOAA long-life weather satellites,nonrepeatable transient,probability reliability analysis engineering,space vehicle equipment,spacecraft equipment,spacecraft vehicle,U.S. government,usable life upgrading factory ATP},
  pages = {1-21},
  file = {/home/yuri/Zotero/storage/JPWPRZNN/Losik - 2012 - Using analog telemetry to measure spacecraft equip.pdf;/home/yuri/Zotero/storage/4S3MY44W/6187383.html}
}
% == BibTeX quality report for losikUsingAnalogTelemetry2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@incollection{suciuMonitoringBlackSea2016,
  series = {Lecture {{Notes}} of the {{Institute}} for {{Computer Sciences}}, {{Social Informatics}} and {{Telecommunications Engineering}}},
  title = {Monitoring the {{Black Sea Region Using Satellite Earth Observation}} and {{Ground Telemetry}}},
  isbn = {978-3-319-74934-1 978-3-319-74935-8},
  abstract = {The Black Sea region is affected by important environmental transformations and EO (Earth Observation) is considered a new way to monitor and, possibly, solve some of its critical issues. The ecological alterations, essentially caused by anthropogenic factors, are the main cause of many transformations such as the ecosystem changes, coastal erosion or pollution that affects water quality. The purpose of this paper is to present the environmental measurements performed with ground telemetry systems near the Black Sea coast in the Danube Delta and the fusion of sensor data with datasets from EO satellite applications. We present the BEIA telemetry system that has been installed and is further being developed for the National Administration ``Romanian Waters'' (ANAR), an automatic system able to continuously monitor the level and water temperature along the Danube, Danube Delta and some of its tributary rivers. Furthermore, we demonstrate how big data processing software can be used for extracting non-trivial correlations from telemetry and EO data. This paper is a general overview of the results for telemetry and EO integration in the Black Sea and the Danube region and could support ESA (European Space Agency) in defining future investments in EO research and development activities to foster EO innovation in the region.},
  language = {en},
  booktitle = {Pervasive {{Computing Paradigms}} for {{Mental Health}}},
  publisher = {{Springer, Cham}},
  author = {Suciu, George and Fratu, Octavian and Suciu, Victor and Grigore, Iulian},
  month = nov,
  year = {2016},
  pages = {126-132},
  file = {/home/yuri/Zotero/storage/QC5R5ZNE/10.html},
  doi = {10.1007/978-3-319-74935-8_18}
}
% == BibTeX quality report for suciuMonitoringBlackSea2016:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{miebachHubbleSpaceTelescope2002,
  title = {Hubble {{Space Telescope}} On-Line Telemetry Archive for Monitoring Scientific Instruments},
  volume = {4844},
  abstract = {A major milestone in an effort to update the aging Hubble Space Telescope (HST) ground system was completed when HST operations were switched to a new ground system, a project called "Vision 2000 Control Center System CCS)", at the time of the third Servicing Mission in December 1999. A major CCS subsystem is the Space Telescope Engineering Data Store, the design of which is based on modern Data Warehousing technology. In fact, the Data Warehouse (DW) as implemented in the CCS Ground System that operates and monitors the Hubble Space Telescope represents, the first use of a commercial Data Warehouse to manage engineering data. By the end of February 2002, the process of populating the Data Warehouse with HST historical telemetry data had been completed, providing access to HST engineering data for a period of over 12 years with a current data volume of 2.8 Terabytes. This paper describes hands-on experience from an end user perspective, using the CCS system capabilities, including the Data Warehouse as an HST engineering telemetry archive. The Engineering Team at the Space Telescope Science Institute is using HST telemetry extensively for monitoring the Scientific Instruments, in particular for {$\cdot$} Spacecraft anomaly resolutions {$\cdot$} Scientific Instrument trending {$\cdot$} Improvements of Instrument operational efficiency The overall idea is to maximize science output of the space observatory. Furthermore, the CCS provides a powerful feature to build, save, and recall real-time display pages customized to specific subsystems and operational scenarios. Engineering teams are using the real-time monitoring capabilities intensively during Servicing Missions and real time commanding to handle anomaly situations, while the Flight Operations Team (FOT) monitors the spacecraft around the clock.},
  booktitle = {Observatory {{Operations}} to {{Optimize Scientific Return III}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.460637},
  author = {Miebach, Manfred P.},
  month = jan,
  year = {2002},
  pages = {408-417},
  file = {/home/yuri/Zotero/storage/8RMXX7IM/Miebach - 2002 - Hubble Space Telescope on-line telemetry archive f.pdf;/home/yuri/Zotero/storage/NYD3MAY7/12.460637.html}
}
% == BibTeX quality report for miebachHubbleSpaceTelescope2002:
% ? Unsure about the formatting of the booktitle

@inproceedings{dongEffectiveMethodMining2014,
  title = {An {{Effective Method}} for {{Mining Quantitative Association Rules}} with {{Clustering Partition}} in {{Satellite Telemetry Data}}},
  abstract = {The correlation analysis of telemetry data plays a significant role in satellite performance analysis. However, the existing methods cannot be well applied, because the telemetry data is large and high-dimensional. In this paper, an efficient algorithm named QARC Apriori is proposed. First, to reduce the redundant attributes and lower the problem complexity, grey relational analysis method is applied. Second, each filtered attribute is partitioned into several subintervals, combining with K-Means clustering algorithm. During clustering, the outliers are removed to improve the accuracy of clustering results. Due to different distributions and scopes of attributes, the clustering centers are automatically adjusted. Moreover, the statistical information of each attribute is used to avoid repeatedly scanning database. Finally, all quantitative association rules are mined by an improved Apriori algorithm. In order to improve the mining efficiency, two pruning strategies are used. The experiments are conducted with the power supply data of a China's satellite from 2011.6.1 to 2011.9.1. It indicates that the proposed algorithm is suitable for quantitative association rules mining and is important for satellite on-orbit performance analysis.},
  booktitle = {2014 {{Second International Conference}} on {{Advanced Cloud}} and {{Big Data}}},
  doi = {10.1109/CBD.2014.12},
  author = {Dong, X. and Pi, D.},
  month = nov,
  year = {2014},
  keywords = {Satellites,Telemetry,satellite telemetry,data mining,Correlation,Clustering,Clustering algorithms,Association rules,Itemsets,satellite telemetry data,clustering centers,Discretization,Grey correlation analysis,grey relational analysis method,grey systems,K-means clustering algorithm,pattern clustering,pruning strategies,QARC_apriori algorithm,Quantitative association rules,quantitative association rules mining method with clustering partition,satellite on-orbit performance analysis,Telemetry data},
  pages = {26-33},
  file = {/home/yuri/Zotero/storage/APBDKZ7F/Dong and Pi - 2014 - An Effective Method for Mining Quantitative Associ.pdf;/home/yuri/Zotero/storage/QM6FD8GY/Dong_Pi_2014_An Effective Method for Mining Quantitative Association Rules with Clustering.pdf;/home/yuri/Zotero/storage/L6NE2DK6/7176068.html;/home/yuri/Zotero/storage/U3QTNM9Q/7176068.html}
}
% == BibTeX quality report for dongEffectiveMethodMining2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{evansDataMiningDrastically2016a,
  title = {Data {{Mining}} to {{Drastically Improve Spacecraft Telemetry Checking}}: {{An Engineer}}'s {{Approach}}},
  isbn = {978-1-62410-426-8},
  shorttitle = {Data {{Mining}} to {{Drastically Improve Spacecraft Telemetry Checking}}},
  language = {en},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/6.2016-2397},
  author = {Evans, David J. and Martinez, Jose and {Korte-Stapff}, Moritz and Brighenti, Attilio and Brighenti, Chiara and Biancat, Jacopo},
  month = may,
  year = {2016},
  file = {/home/yuri/Zotero/storage/4EPV3UN8/Evans et al. - 2016 - Data Mining to Drastically Improve Spacecraft Tele.pdf}
}
% == BibTeX quality report for evansDataMiningDrastically2016a:
% Missing required field 'booktitle'
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{yairiTelemetryminingMachineLearning2006,
  title = {Telemetry-Mining: A Machine Learning Approach to Anomaly Detection and Fault Diagnosis for Space Systems},
  shorttitle = {Telemetry-Mining},
  abstract = {For any space mission, safety and reliability are the most important issues. To tackle this problem, we have studied anomaly detection and fault diagnosis methods for spacecraft systems based on machine learning (ML) and data mining (DM) technology. In these methods, the knowledge or model which is necessary for monitoring a spacecraft system is (semi-)automatically acquired from the spacecraft telemetry data. In this paper, we first overview the anomaly detection/diagnosis problem in the spacecraft systems and conventional techniques such as limit-check, expert systems and model-based diagnosis. Then we explain the concept of ML/DM-based approach to this problem, and introduce several anomaly detection/diagnosis methods which have been developed by us},
  booktitle = {2nd {{IEEE International Conference}} on {{Space Mission Challenges}} for {{Information Technology}} ({{SMC}}-{{IT}}'06)},
  doi = {10.1109/SMC-IT.2006.79},
  author = {Yairi, T. and Kawahara, Y. and Fujimaki, R. and Sato, Y. and Machida, K.},
  year = {2006},
  keywords = {Space vehicles,machine learning,learning (artificial intelligence),aerospace safety,Telemetry,space telemetry,Data mining,Space missions,aerospace instrumentation,space systems,data mining,anomaly detection,Space technology,aerospace expert systems,aerospace reliability,Delta modulation,expert systems,Fault detection,fault diagnosis,Fault diagnosis,Machine learning,model-based diagnosis,Safety,space mission,spacecraft telemetry data,telemetry mining},
  pages = {8 pp.-476},
  file = {/home/yuri/Zotero/storage/QBFC5GCZ/Yairi et al. - 2006 - Telemetry-mining a machine learning approach to a.pdf;/home/yuri/Zotero/storage/3PRXXPT4/1659593.html}
}
% == BibTeX quality report for yairiTelemetryminingMachineLearning2006:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{owensMaunderMinimumLittle2017,
  title = {The {{Maunder}} Minimum and the {{Little Ice Age}}: An Update from Recent Reconstructions and Climate Simulations},
  volume = {7},
  issn = {2115-7251},
  shorttitle = {The {{Maunder}} Minimum and the {{Little Ice Age}}},
  abstract = {The Maunder minimum (MM) was a period of extremely low solar activity from approximately AD 1650 to 1715. In the solar physics literature, the MM is sometimes associated with a period of cooler global temperatures, referred to as the Little Ice Age (LIA), and thus taken as compelling evidence of a large, direct solar influence on climate. In this study, we bring together existing simulation and observational studies, particularly the most recent solar activity and paleoclimate reconstructions, to examine this relation. Using northern hemisphere surface air temperature reconstructions, the LIA can be most readily defined as an approximately 480 year period spanning AD 1440\textendash{}1920, although not all of this period was notably cold. While the MM occurred within the much longer LIA period, the timing of the features are not suggestive of causation and should not, in isolation, be used as evidence of significant solar forcing of climate. Climate model simulations suggest multiple factors, particularly volcanic activity, were crucial for causing the cooler temperatures in the northern hemisphere during the LIA. A reduction in total solar irradiance likely contributed to the LIA at a level comparable to changing land use.},
  language = {en},
  journal = {Journal of Space Weather and Space Climate},
  doi = {10/gdg3jg},
  author = {Owens, Mathew J. and Lockwood, Mike and Hawkins, Ed and Usoskin, Ilya and Jones, Gareth S. and Barnard, Luke and Schurer, Andrew and Fasullo, John},
  year = {2017},
  keywords = {\#nosource},
  pages = {A33}
}
% == BibTeX quality report for owensMaunderMinimumLittle2017:
% Missing required field 'number'

@article{hathawaySolarCycle2015,
  title = {The {{Solar Cycle}}},
  volume = {12},
  issn = {2367-3648, 1614-4961},
  abstract = {The solar cycle is reviewed. The 11-year cycle of solar activity is characterized by the rise and fall in the numbers and surface area of sunspots. A number of other solar activity indicators also vary in association with the sunspots including; the 10.7 cm radio flux, the total solar irradiance, the magnetic field, flares and coronal mass ejections, geomagnetic activity, galactic cosmic ray fluxes, and radioisotopes in tree rings and ice cores. Individual solar cycles are characterized by their maxima and minima, cycle periods and amplitudes, cycle shape, the equatorward drift of the active latitudes, hemispheric asymmetries, and active longitudes. Cycle-to-cycle variability includes the Maunder Minimum, the Gleissberg Cycle, and the Gnevyshev\textendash{}Ohl (even-odd) Rule. Short-term variability includes the 154-day periodicity, quasi-biennial variations, and double-peaked maxima. We conclude with an examination of prediction techniques for the solar cycle and a closer look at cycles 23 and 24.},
  language = {en},
  number = {1},
  journal = {Living Reviews in Solar Physics},
  doi = {10/bjxj},
  author = {Hathaway, David H.},
  month = dec,
  year = {2015},
  keywords = {\#nosource}
}
% == BibTeX quality report for hathawaySolarCycle2015:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@article{gosainSecurityIssuesData2015,
  series = {International {{Conference}} on {{Computer}}, {{Communication}} and {{Convergence}} ({{ICCC}} 2015)},
  title = {Security {{Issues}} in {{Data Warehouse}}: {{A Systematic Review}}},
  volume = {48},
  issn = {1877-0509},
  shorttitle = {Security {{Issues}} in {{Data Warehouse}}},
  abstract = {As Data Warehouse store huge amount of data with the span of more than decades, the security of this huge information base is crucial for the sustainability and reliability of data warehouse. Since its advent the data warehouse has gone through various technological changes, which has prompted changes in the security strategies as well. This article, is taking a deep look at the various changes in the security mechanisms of the Data Warehouse, along with the changes in the strategies for the data warehouse development. It helps in understanding the various security aspects related to Data Warehouse, in coherence with the different methodologies employed for its development and functioning.},
  journal = {Procedia Computer Science},
  doi = {10/gdf4ss},
  author = {Gosain, Anjana and Arora, Amar},
  month = jan,
  year = {2015},
  keywords = {Security,Data Warehouse Security,Systematic Review},
  pages = {149-157},
  file = {/home/yuri/Zotero/storage/ZZ2IFVLJ/Gosain and Arora - 2015 - Security Issues in Data Warehouse A Systematic Re.pdf;/home/yuri/Zotero/storage/J368J689/S1877050915006730.html}
}
% == BibTeX quality report for gosainSecurityIssuesData2015:
% Missing required field 'number'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{azeemIntelligentDataCube2014,
  title = {Intelligent Data Cube Construction and Exploration},
  abstract = {Data cubes are multi-dimensional structures that consist of dimensions and measures. Analysts can view the performance measures via different perspectives provided by the available dimensions in data cubes. However, modeling of these meaningful dimensions and selection of informative measure is a difficult task for human data warehouse developers. In high dimensional environments, the sheer size and volume of data poses a number of challenges in order to generate meaningful data cubes. Nowadays, there is a growing requirement of automated and intelligent techniques that allows analysts to construct and explore the large cubes for better decision making. In this paper, we have reviewed the literature on intelligent data cubes construction and exploration. Literature review reveals that a number of techniques have been proposed to embed intelligence in data cubes. However, majority of the previously proposed technique targeted either on providing intelligence in cube construction or focused assisting the intelligent exploration of data cubes. However, there is very limited amount of work has been done in the integration of intelligent techniques for both cube construction and exploration. We believe that it is a strong area of research and the modern analytical systems demand the availability of intelligent techniques for both construction and exploration of large data cubes for making intelligent decisions. The objective of this paper is to present a critical review of the existing techniques and to propose a conceptual model that not only overcomes the individual limitations in the previous work but also merges the benefits of intelligent construction and exploration of cubes in parallel. However, the implementation of the proposed model is beyond the scope of this paper.},
  booktitle = {Ninth {{International Conference}} on {{Digital Information Management}} ({{ICDIM}} 2014)},
  doi = {10.1109/ICDIM.2014.6991408},
  author = {Azeem, M. and Usman, M. and Ahmad, W.},
  month = sep,
  year = {2014},
  keywords = {Algorithm design and analysis,data mining,data warehouses,Aggregates,Artificial intelligence,decision making,Approximation algorithms,automated technique,Data Cube Construction,Data cube Exploration,Heuristic algorithms,human data warehouse developer,informative measure,intelligent data cube construction and exploration,intelligent exploration,Intelligent OLAP,intelligent techniques,knowledge based systems,multidimensional structures,performance measures,Semantics,Time factors},
  pages = {168-174},
  file = {/home/yuri/Zotero/storage/JNJP2YYI/Azeem et al. - 2014 - Intelligent data cube construction and exploration.pdf;/home/yuri/Zotero/storage/ND3AIU2R/6991408.html}
}
% == BibTeX quality report for azeemIntelligentDataCube2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{kampgenOLAP4LDFrameworkBuilding2014,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {{{OLAP4LD}} \textendash{} {{A Framework}} for {{Building Analysis Applications Over Governmental Statistics}}},
  isbn = {978-3-319-11954-0 978-3-319-11955-7},
  abstract = {Although useful governmental statistics have been published as Linked Data, there are query processing and data pre-processing challenges to allow citizens exploring such multidimensional datasets in pivot tables. In this demo paper we present OLAP4LD, a framework for developers of applications over Linked Data sources reusing the RDF Data Cube Vocabulary. Our demonstration will let visiting developers and dataset publishers explore European statistics with the Linked Data Cubes Explorer (LDCX), will explain how LDCX makes use of OLAP4LD, and will show common dataset modelling errors.},
  language = {en},
  booktitle = {The {{Semantic Web}}: {{ESWC}} 2014 {{Satellite Events}}},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-11955-7_54},
  author = {K{\"a}mpgen, Benedikt and Harth, Andreas},
  month = may,
  year = {2014},
  pages = {389-394},
  file = {/home/yuri/Zotero/storage/ZGZ6KFRC/10.html}
}
% == BibTeX quality report for kampgenOLAP4LDFrameworkBuilding2014:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{kampgenInteractingStatisticalLinked2012,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interacting with {{Statistical Linked Data}} via {{OLAP Operations}}},
  isbn = {978-3-662-46640-7 978-3-662-46641-4},
  abstract = {Online Analytical Processing (OLAP) promises an interface to analyse Linked Data containing statistics going beyond other interaction paradigms such as follow-your-nose browsers, faceted-search interfaces and query builders. Transforming statistical Linked Data into a star schema to populate a relational database and applying a common OLAP engine do not allow to optimise OLAP queries on RDF or to directly propagate changes of Linked Data sources to clients. Therefore, as a new way to interact with statistics published as Linked Data, we investigate the problem of executing OLAP queries via SPARQL on an RDF store. First, we define projection, slice, dice and roll-up operations on single data cubes published as Linked Data reusing the RDF Data Cube vocabulary and show how a nested set of operations lead to an OLAP query. Second, we show how to transform an OLAP query to a SPARQL query which generates all required tuples from the data cube. In a small experiment, we show the applicability of our OLAP-to-SPARQL mapping in answering a business question in the financial domain.},
  language = {en},
  booktitle = {The {{Semantic Web}}: {{ESWC}} 2012 {{Satellite Events}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-46641-4_7},
  author = {K{\"a}mpgen, Benedikt and O'Riain, Se{\'a}n and Harth, Andreas},
  month = may,
  year = {2012},
  pages = {87-101},
  file = {/home/yuri/Zotero/storage/JB4UQK2H/10.html}
}
% == BibTeX quality report for kampgenInteractingStatisticalLinked2012:
% ? Title looks like it was stored in title-case in Zotero

@misc{EngineeringVillageQuickb,
  title = {Engineering {{Village}} - {{Quick Search Detailed Format}}},
  howpublished = {https://www.engineeringvillage.com/search/doc/detailed.url?SEARCHID=3f8bb73cMe339M462aM8d55M17c0a9c33918\&DOCINDEX=8\&database=1\&pageType=quickSearch\&searchtype=Quick\&dedupResultCount=null\&format=quickSearchDetailedFormat\&usageOrigin=recordpage\&usageZone=abstracttab\&toolsinScopus=Noload},
  file = {/home/yuri/Zotero/storage/UBLJAIU2/detailed.html}
}
% == BibTeX quality report for EngineeringVillageQuickb:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{kannanMiningSatelliteTelemetry2016,
  title = {Mining Satellite Telemetry Data: {{Comparison}} of Rule-Induction and Association Mining Techniques},
  shorttitle = {Mining Satellite Telemetry Data},
  abstract = {An artificial Satellite's health and functioning in space is monitored by analysing the stream of Telemetry Data received at the Satellite Ground Control Centre. This data is received continually during the entire mission life of the Satellite. Hence a huge volume of Data is collected for any given satellite. Data Mining techniques are now regularly applied to analyse such large quantity of data for outlier analysis, fault detection in past data and fault prediction. In the present work, hidden relationships between apparently unrelated Telemetry parameters are mined by applying Rule Induction and Association Rule mining algorithms separately on a data set with randomly selected parameters. From the results obtained, a comparison is made to assess the suitability of either of these techniques to the application domain. This paper presents a finding that Rule Mining Algorithm is most appropriate for the domain application since Association rule mining is not amenable to data sets of Satellite Telemetry Parameters which are highly unrelated but whose functional inter-relationships during specified on-orbit stages are to be mined.},
  booktitle = {2016 {{IEEE International Conference}} on {{Advances}} in {{Computer Applications}} ({{ICACA}})},
  doi = {10.1109/ICACA.2016.7887962},
  author = {Kannan, S. A. and Devi, T.},
  month = oct,
  year = {2016},
  keywords = {artificial satellites,Satellites,aerospace computing,data analysis,Satellite,Monitoring,Telemetry,satellite telemetry,Databases,Data mining,aerospace instrumentation,data mining,Conferences,condition monitoring,fault diagnosis,Algorithm,artificial satellite functioning,artificial satellite health monitoring,Association,association rule mining algorithms,Computer applications,fault detection,fault prediction,functional inter-relationships,on-orbit stages,outlier analysis,random parameter selection,Rule Induction,rule-induction,Satellite Ground Control Centre,satellite telemetry data analysis,satellite telemetry parameter data sets,Telemetry Parameter},
  pages = {259-264},
  file = {/home/yuri/Zotero/storage/CS4AAGPY/Kannan and Devi - 2016 - Mining satellite telemetry data Comparison of rul.pdf;/home/yuri/Zotero/storage/3992ZPCM/7887962.html}
}
% == BibTeX quality report for kannanMiningSatelliteTelemetry2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{crowleyAnalysisSatelliteTelemetry1997,
  title = {Analysis of Satellite Telemetry Data},
  volume = {4},
  abstract = {Satellites transmit data streams to the ground that consist of data values, called mnemonics, that are used to determine the health and location of the satellite. These data streams can contain between 700 and 12000 or more mnemonics. These mnemonics must be analyzed to ensure that the satellite is healthy, to assist in resolving any anomalies, and to determine if there are any trends that would indicate a possible future problem. Satellite engineers need assistance to assimilate and act on these large volumes of data. The USAF Phillips Laboratory Space System Technologies Division (PL/VTS) has developed a telemetry analysis system called the Visual Interface for Satellite Telemetry Analysis (VISTA) that captures and displays satellite telemetry data in real time and provides tools for analyzing stored data. An important aspect of VISTA is the analysis of stored telemetry data. The satellite engineer needs quick, accurate access to the stored satellite data in numerous forms, such as plots, reports; curve fitting, and Fourier analysis graphs. The challenge was to develop a telemetry analysis system that was flexible, fast, accurate, and able to provide the multiple views of the data required by the satellite engineers. This paper provides an overview of VISTA, and then discusses how the telemetry analysis tool (TAT) was developed, how the system functions, timing information, and future work},
  booktitle = {1997 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.1997.577497},
  author = {Crowley, N. L. and Apodaca, V.},
  month = feb,
  year = {1997},
  keywords = {Satellites,aerospace computing,data analysis,Telemetry,satellite telemetry,data visualisation,Data analysis,Data engineering,Space technology,satellite telemetry data,curve fitting,Curve fitting,data streams,Displays,Fourier analysis,Fourier analysis graphs,health,Information analysis,Laboratories,location,mnemonics,Real time systems,real-time systems,satellite engineers,Satellite Telemetry Analysis,stored telemetry data,telemetry analysis,USAF Phillips Laboratory,VISTA,Visual Interface},
  pages = {57-67 vol.4},
  file = {/home/yuri/Zotero/storage/6FQ4K9AX/Crowley and Apodaca - 1997 - Analysis of satellite telemetry data.pdf;/home/yuri/Zotero/storage/GKZPYJ39/577497.html}
}
% == BibTeX quality report for crowleyAnalysisSatelliteTelemetry1997:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@misc{SatelliteTelemetryAmateur,
  title = {Satellite {{Telemetry}} | {{Amateur Radio}} \textendash{} {{PE{\O}SAT}}},
  abstract = {| Information about Amateur Radio \textendash{} Satellite experiments |},
  language = {en-US},
  file = {/home/yuri/Zotero/storage/Q5A857ZY/satellite-telemetry.html}
}
% == BibTeX quality report for SatelliteTelemetryAmateur:
% Missing required field 'author'
% Missing required field 'howpublished'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{liuFragmentAnomalyDetection2017,
  title = {Fragment {{Anomaly Detection With Prediction}} and {{Statistical Analysis}} for {{Satellite Telemetry}}},
  volume = {5},
  abstract = {In aerospace engineering, condition monitoring is an important reference for evaluating the performance of complex systems. Especially, effective anomaly detection, based on telemetry data, plays an important role for the system health management of a spacecraft. With the advantages of easy-to-use, high efficiency, and data-driven, the predicted model has been applied for anomalous point detection for monitoring data. However, compared with the point abnormal mode, fragment anomaly is more attractive and meaningful for the system identification. Therefore, the detection strategy of fragment anomaly is proposed based on the uncertainty estimation of least square support vector machine and statistical analysis. Moreover, some effective estimation indicators are presented to evaluate the performance of the detection method. Experimental validations are also carried out for some typical simulation data sets and open source data sets. In particular, relied on the analysis of fragment anomaly modes, experiments are conducted with the real satellite telemetry data and different anomaly modes are injected to examine the applicability of the proposed framework.},
  journal = {IEEE Access},
  doi = {10/gdf4sp},
  author = {Liu, D. and Pang, J. and Song, G. and Xie, W. and Peng, Y. and Peng, X.},
  year = {2017},
  keywords = {aerospace computing,aerospace engineering,Predictive models,Data models,telemetry,Satellite,Training,Anomaly detection,Telemetry,Computational modeling,satellite telemetry,uncertainty,statistical analysis,least squares approximations,support vector machines,anomaly detection,condition monitoring,satellite telemetry data,anomalous point detection,fragment anomaly,fragment anomaly detection,fragment anomaly modes,LS-SVM,open source data sets,point abnormal mode,square support vector machine,Support vector machines,system health management,system identification},
  pages = {19269-19281},
  file = {/home/yuri/Zotero/storage/JAJWKH2B/Liu et al. - 2017 - Fragment Anomaly Detection With Prediction and Sta.pdf;/home/yuri/Zotero/storage/CS6B59RK/8048464.html}
}
% == BibTeX quality report for liuFragmentAnomalyDetection2017:
% Missing required field 'number'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{ariza-porrasCDColGeoscienceData2017,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {{{CDCol}}: {{A Geoscience Data Cube}} That {{Meets Colombian Needs}}},
  isbn = {978-3-319-66561-0 978-3-319-66562-7},
  shorttitle = {{{CDCol}}},
  abstract = {Environmental analysts and researchers' time is an expensive and scarce resource that should be used efficiently. Creating analysis products from remote sensing images involves several steps that take time and can be either automatized or centralized. Among all these steps, product's lineage and reproducibility must be assured. We present CDCol, a geoscience data cube that addresses these concerns and fits the analysis needs of Colombian institutions, the forest and carbon monitoring system.},
  language = {en},
  booktitle = {Advances in {{Computing}}},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-66562-7_7},
  author = {{Ariza-Porras}, Christian and Bravo, Germ{\'a}n and Villamizar, Mario and Moreno, Andr{\'e}s and Castro, Harold and Galindo, Gustavo and Cabera, Edersson and Valbuena, Saralux and Lozano, Pilar},
  month = sep,
  year = {2017},
  pages = {87-99},
  file = {/home/yuri/Zotero/storage/QDD4NYVI/10.html}
}
% == BibTeX quality report for ariza-porrasCDColGeoscienceData2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{chapmanDataIntensiveStatistical2013,
  title = {A {{Data Intensive Statistical Aggregation Engine}}: {{A Case Study}} for {{Gridded Climate Records}}},
  shorttitle = {A {{Data Intensive Statistical Aggregation Engine}}},
  abstract = {Satellite derived climate instrument records are often highly structured and conform to the "Data-Cube" topology. However, data scales on the order of tens to hundreds of Terabytes make it more difficult to perform the rigorous statistical aggregation and analytics necessary to investigate how our climate is changing over time and space. It is especially cumbersome to supply the full derivation (provenance) of this analysis, as is increasingly required by scientific conferences and journals. In this paper, we address our approach toward the creation of a 55 Terabyte decadal record of Outgoing Long wave Spectrum (OLS) from the NASA Atmospheric Infrared Sounder (AIRS), and describe our open source data-intensive statistical aggregation engine "Gridderama" intended primarily for climate trend analysis, and may be applicable to other aggregation problems involving large structured datasets.},
  booktitle = {2013 {{IEEE International Symposium}} on {{Parallel Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  doi = {10.1109/IPDPSW.2013.87},
  author = {Chapman, D. and Simon, T. A. and Nguyen, P. and Halem, M.},
  month = may,
  year = {2013},
  keywords = {artificial satellites,Meteorology,Market research,NASA,Instruments,statistical analysis,data handling,Arrays,parallel processing,public domain software,Aggregation,AIRS,Big-data,climate trend analysis,data intensive statistical aggregation engine,data-cube topology,Engines,geophysics computing,gridded climate records,Gridderama,large structured datasets,NASA Atmospheric Infrared Sounder,OLS,open source data-intensive statistical aggregation engine,Outgoing Longwave Spectrum,rigorous statistical aggregation,rigorous statistical analytics,Runtime environment,Satellite derived climate instrument records,Scientific,topology,Workflow},
  pages = {2157-2164},
  file = {/home/yuri/Zotero/storage/HSI4BPYD/Chapman et al. - 2013 - A Data Intensive Statistical Aggregation Engine A.pdf;/home/yuri/Zotero/storage/F6E6C49I/6651122.html}
}
% == BibTeX quality report for chapmanDataIntensiveStatistical2013:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{matasciSpacetimeDataCube2017,
  title = {A Space-Time Data Cube: {{Multi}}-Temporal Forest Structure Maps from Landsat and Lidar},
  shorttitle = {A Space-Time Data Cube},
  abstract = {In this study, we prototype the combination of samples of airborne LiDAR (LiDAR plots) and Landsat data to characterize the development of forest structure attributes through time. A nearest neighbor imputation model was developed using predictors generated from wall-to-wall Landsat best available pixel (BAP) composites and reference measurements of forest structure derived from LiDAR plots. The imputation model was then applied through time on a study area in Canada's boreal forest, resulting in forest structure maps with a 30 m resolution for the period 1984-2012. We characterize post-disturbance trends in these forest structural metrics following wildfire and harvest and offer insights on the large-area, temporally dense mapping opportunities offered by the synergistic use of samples of airborne LiDAR and Landsat BAP composites.},
  booktitle = {2017 {{IEEE International Geoscience}} and {{Remote Sensing Symposium}} ({{IGARSS}})},
  doi = {10.1109/IGARSS.2017.8127523},
  author = {Matasci, G. and Hermosilla, T. and Wulder, M. A. and White, J. C. and Hobart, G. W. and Zald, H. S. J. and Coops, N. C.},
  month = jul,
  year = {2017},
  keywords = {Earth,Computational modeling,forestry,Forestry,Remote sensing,vegetation mapping,geophysical image processing,Artificial satellites,AD 1984 to 2014,airborne LiDAR,Atmospheric modeling,best available pixel,boreal forest,Canada,forest mapping,forest structural metrics,imputation,Landsat BAP composite,Landsat BAP composites,Landsat data,Landsat pixel composites,Laser radar,LiDAR plots,multitemporal forest structure maps,nearest neighbor imputation model,optical radar,Random Forest,reference measurements,remote sensing by laser beam,space-time data cube,time-series,wildfire},
  pages = {2581-2584},
  file = {/home/yuri/Zotero/storage/62X9ICMN/Matasci et al. - 2017 - A space-time data cube Multi-temporal forest stru.pdf;/home/yuri/Zotero/storage/KCZMMR4Z/8127523.html}
}
% == BibTeX quality report for matasciSpacetimeDataCube2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{rabideauInteractiveRepairbasedPlanning1997,
  title = {Interactive, Repair-Based Planning and Scheduling for {{Shuttle}} Payload Operations},
  volume = {1},
  abstract = {This paper describes the DATA-CHASER Automated Planner/Scheduler (DCAPS) system for automatically generating low-level command sequences from high-level user goals. DCAPS uses Artificial Intelligence (AI)-based search techniques and an iterative repair framework in which the system selectively resolves conflicts with the resource and temporal constraints of the DATA-CHASER Shuttle payload activities},
  booktitle = {1997 {{IEEE Aerospace Conference}}},
  doi = {10.1109/AERO.1997.574423},
  author = {Rabideau, G. and Chien, S. and Mann, T. and Eggemeyer, C. and Willis, J. and Siewert, S. and Stone, P.},
  month = feb,
  year = {1997},
  keywords = {Space vehicles,aerospace computing,scheduling,optimisation,artificial intelligence,aerospace control,Instruments,Artificial intelligence,Propulsion,Payloads,Artificial Intelligence,DATA-CHASER Automated Planner/Scheduler,high-level user goals,intelligent control,Iterative algorithms,iterative methods,iterative repair,low-level command sequences,model representation,Optical imaging,Power system planning,Processor scheduling,repair-based planning,Scheduling algorithm,search problems,Shuttle payload operations,temporal constraints},
  pages = {325-341 vol.1},
  file = {/home/yuri/Zotero/storage/6G9Z3D35/Rabideau et al. - 1997 - Interactive, repair-based planning and scheduling .pdf;/home/yuri/Zotero/storage/ZW3RCWCI/574423.html}
}
% == BibTeX quality report for rabideauInteractiveRepairbasedPlanning1997:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{steelAdvancedPlanningScheduling,
  title = {Advanced {{Planning}} and {{Scheduling Initiative MrSPOCK AIMS}} for {{XMAS}} in the Space Domain},
  abstract = {This paper will outline the framework and tools developed under the Advanced Planning and Schedule Initiative (APSI) study performed by VEGA for the European Space Agency in collaboration with three academic institutions, ISTC-CNR, ONERA, and Politecnico di Milano. We will start by illustrating the background history to APSI and why it was needed, giving a brief summary of all the partners within the project and the roles they played within it. We will then take a closer look at what the APSI study actually consisted of, showing the techniques that were used and illustrating the framework that was developed within the scope of the project. This will be followed by an elaboration on the three demonstration test scenarios that have been developed as part of the project to validated the framework and demonstrate in an operational environment its applicability, illustrating the re-use and synergies between the three cases along the way. We will finally conclude with a summary of some pros and cons of the approach devised during the project and outline future directions to be further investigated and expanded on within the context of the work performed within the project.},
  language = {en},
  author = {Steel, R and Ni{\'e}zette, M and Cesta, A and Fratini, S and Oddi, A and Cortellessa, G and Rasconi, R and Verfaillie, G and Pralet, C and Lavagna, M and Brambilla, A and Castellini, F and Donati, A and Policella, N},
  pages = {7},
  file = {/home/yuri/Zotero/storage/IIVIHB8I/Steel et al. - Advanced Planning and Scheduling Initiative MrSPOC.pdf}
}
% == BibTeX quality report for steelAdvancedPlanningScheduling:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'

@inproceedings{bobbieClassificationArrhythmiaUsing2011,
  title = {Classification of {{Arrhythmia Using Machine LearningTechniques}}},
  abstract = {One of the central problems of the information age is dealing with the enormous amount of raw information that is available. More and more data is being collected and stored in databases},
  booktitle = {{{WSEAS Transactions}} on {{Computers}} 32 {{Journal}} on {{Soft Computing}} ( {{IJSC}} ), {{Vol}}.2, {{No}}.1},
  author = {Bobbie, Patrick O.},
  year = {2011},
  file = {/home/yuri/Zotero/storage/XAKRAII5/Bobbie - 2011 - Classification of Arrhythmia Using Machine Learnin.pdf;/home/yuri/Zotero/storage/8K6NEHI6/summary.html}
}
% == BibTeX quality report for bobbieClassificationArrhythmiaUsing2011:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{tominagaSimuladorSatelitesPara2010,
  title = {Simulador de Sat{\'e}lites Para Verifica{\c c}{\~a}o de Planos de Opera{\c c}{\~o}es Em Voo},
  author = {Tominaga, Jun},
  year = {2010},
  pages = {176},
  file = {/home/yuri/Zotero/storage/FN6ZP2SU/Tominaga - 2010 - Simulador de satÃ©lites para verificaÃ§Ã£o de planos .pdf}
}
% == BibTeX quality report for tominagaSimuladorSatelitesPara2010:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'

@article{castelliniAdvancedPlanningScheduling2018,
  title = {Advanced {{Planning}} and {{Scheduling Initiative}}'s {{XMAS}} Tool: {{AI}} for Automatic Scheduling of {{XMM}}-{{Newton}} Long Term Plan},
  shorttitle = {Advanced {{Planning}} and {{Scheduling Initiative}}'s {{XMAS}} Tool},
  abstract = {This paper introduces XMAS (Xmm Mission Apsi Scheduler), a tool for the automatic scheduling of XMM (X-ray Multi-Mirror) \textendash{} Newton Long Term Planning (LTP), developed under the Advanced Planning and Scheduling Initiative (APSI) study for the European Space Agency. The current work has been built upon the previous experience of the development of a similar tool for the planning of the observations of another ESA mission, the INTErnational Gamma-Ray Astrophysics Laboratory (INTEGRAL). Although tailored for specific XMM's requirements, XMAS is for this reason capable of dealing with modeling features, constraints and objectives common to both missions and may therefore be regarded as a more generic tool, covering many aspects of the scheduling problems for single-instrument space observatories. After a brief introduction on APSI study, the paper focuses on the peculiarities of XMM-Newton observations features and on the performance indexes, constraints and heuristics that have been necessary to deal with the given requirements. Results will then be presented according to two test sets, representative of typical XMM's LTP problems, and some conclusions and possible improvements will finally be outlined.},
  author = {Castellini, Francesco and Lavagna, Michelle},
  month = may,
  year = {2018},
  file = {/home/yuri/Zotero/storage/CL6KQZ57/Castellini_Lavagna_2018_Advanced Planning and Scheduling Initiative's XMAS tool.pdf}
}
% == BibTeX quality report for castelliniAdvancedPlanningScheduling2018:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@article{silvaKPlanOOUmMetamodelo2010,
  title = {{{KPlanOO}}: Um Meta-Modelo Orientado a Objetos Para Descri{\c c}{\~a}o de Dom{\'i}nios e Problemas de Planejamento},
  author = {Silva, Rodrigo Rocha},
  year = {2010},
  pages = {254},
  file = {/home/yuri/Zotero/storage/ZI3G9XKC/Silva - 2010 - KPlanOO um meta-modelo orientado a objetos para d.pdf}
}
% == BibTeX quality report for silvaKPlanOOUmMetamodelo2010:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'

@article{fernandezCompilingDataMining,
  title = {On {{Compiling Data Mining Tasks}} to {{PDDL}} {${_\ast}$}},
  abstract = {Data mining is a difficult task that relies on an exploratory and analytic process of large quantities of data in order to discover meaningful patterns and rules. It requires complex methodologies, and the increasing heterogeneity and complexity of available data requires some skills to build the data mining processes, or knowledge flows. The goal of this work is to describe data-mining processes in terms of Automated Planning, which will allow us to automatize the data-mining knowledge flow construction. The work is based on the use of standards both in data mining and automated-planning communities. We use PMML (Predictive Model Markup Language) to describe data mining tasks. From the PMML, a problem description in PDDL can be generated, so any current planning system can be used to generate a plan. This plan is, again, translated to a KFML format (Knowledge Flow file for the WEKA tool), so the plan or data-mining workflow can be executed in WEKA. In this manuscript we describe the languages, how the translation from PMML to PDDL, and from a plan to KFML are performed, and the complete architecture of our system.},
  language = {en},
  author = {Fernandez, Susana and Manzano, David},
  pages = {10},
  file = {/home/yuri/Zotero/storage/WFALQIGM/Fernandez and Manzano - On Compiling Data Mining Tasks to PDDL â.pdf}
}
% == BibTeX quality report for fernandezCompilingDataMining:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{ribeiroUMAARQUITETURAAGENTE,
  title = {{UMA ARQUITETURA DE AGENTE DE REPLANEJAMENTO EM TEMPO REAL PARA PLANO DE VOO EM MISS{\~O}ES ESPACIAIS}},
  language = {pt},
  author = {Ribeiro, Edson Alves},
  pages = {224},
  file = {/home/yuri/Zotero/storage/J4MDCG3P/Ribeiro - UMA ARQUITETURA DE AGENTE DE REPLANEJAMENTO EM TEM.pdf}
}
% == BibTeX quality report for ribeiroUMAARQUITETURAAGENTE:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{cardosoAPLICACAOTECNOLOGIAAGENTES2006,
  title = {{APLICA{\c C}{\~A}O DA TECNOLOGIA DE AGENTES DE PLANEJAMENTO EM OPERA{\c C}{\~O}ES DE SAT{\'E}LITES}},
  language = {pt},
  author = {Cardoso, Luciana S{\^e}da},
  year = {2006},
  pages = {167},
  file = {/home/yuri/Zotero/storage/5TTXTKCK/Cardoso - APLICAÃÃO DA TECNOLOGIA DE AGENTES DE PLANEJAMENTO.pdf}
}
% == BibTeX quality report for cardosoAPLICACAOTECNOLOGIAAGENTES2006:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@incollection{sarawagiDiscoverydrivenExplorationOLAP1998,
  address = {{Berlin, Heidelberg}},
  title = {Discovery-Driven Exploration of {{OLAP}} Data Cubes},
  volume = {1377},
  isbn = {978-3-540-64264-0 978-3-540-69709-1},
  abstract = {Analysts predominantly use OLAP data cubes to identify regions of anomalies that may represent problem areas or new opportunities. The current OLAP systems support hypothesis-driven exploration of data cubes through operations such as drill-down, roll-up, and selection. Using these operations, an analyst navigates unaided through a huge search space looking at large number of values to spot exceptions. We propose a new discovery-driven exploration paradigm that mines the data for such exceptions and summarizes the exceptions at appropriate levels in advance. It then uses these exceptions to lead the analyst to interesting regions of the cube during navigation. We present the statistical foundation underlying our approach. We then discuss the computational issue of nding exceptions in data and making the process e cient on large multidimensional data bases.},
  language = {en},
  booktitle = {Advances in {{Database Technology}} \textemdash{} {{EDBT}}'98},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Sarawagi, Sunita and Agrawal, Rakesh and Megiddo, Nimrod},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Schek, Hans-J{\"o}rg and Alonso, Gustavo and Saltor, Felix and Ramos, Isidro},
  year = {1998},
  pages = {168-182},
  file = {/home/yuri/Zotero/storage/9LG6W3RV/Sarawagi et al. - 1998 - Discovery-driven exploration of OLAP data cubes.pdf},
  doi = {10.1007/BFb0100984}
}

@article{chenAutonomousAssemblyCollision2018,
  title = {Autonomous Assembly with Collision Avoidance of a Fleet of Flexible Spacecraft Based on Disturbance Observer},
  volume = {147},
  issn = {0094-5765},
  abstract = {This paper presents a distributed control law with disturbance observer for the autonomous assembly of a fleet of flexible spacecraft to construct a large flexible space structure. The fleet of flexible spacecraft is driven to the pre-assembly configuration firstly, and then to the desired assembly configuration. A distributed assembly control law with disturbance observer is proposed by treating the flexible dynamics as disturbances acting on the rigid motion of the flexible spacecraft. Theoretical analysis shows that the control law can actuate the fleet to the desired configuration. Moreover, the collision avoidance between the members is also considered in the process from initial configuration to pre-assembly configuration. Finally, a numerical example is presented to verify the feasibility of proposed mission planning and the effectiveness of control law.},
  journal = {Acta Astronautica},
  doi = {10/gc9z4d},
  author = {Chen, Ti and Wen, Hao},
  month = jun,
  year = {2018},
  keywords = {Autonomous assembly,Collision avoidance,Distributed control,Disturbance observer,Flexible spacecraft},
  pages = {86-96},
  file = {/home/yuri/Zotero/storage/PDXM3HRG/Chen and Wen - 2018 - Autonomous assembly with collision avoidance of a .pdf;/home/yuri/Zotero/storage/GP38QZBR/S0094576517312377.html}
}
% == BibTeX quality report for chenAutonomousAssemblyCollision2018:
% Missing required field 'number'

@misc{PlanningSchedulingHellenic2013,
  title = {Planning and {{Scheduling}} - {{Hellenic Artificial Intelligence Society}}},
  howpublished = {https://web.archive.org/web/20131222165824/http://www.eetn.gr/index.php/eetn-publications/ai-research-in-greece/planning-and-scheduling},
  month = dec,
  year = {2013},
  file = {/home/yuri/Zotero/storage/XIFR9TAE/planning-and-scheduling.html}
}
% == BibTeX quality report for PlanningSchedulingHellenic2013:
% Missing required field 'author'
% ? Title looks like it was stored in title-case in Zotero

@article{AutomatedPlanningScheduling2017,
  title = {Automated Planning and Scheduling},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {Automated planning and scheduling, sometimes denoted as simply AI Planning, is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.
In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.},
  language = {en},
  journal = {Wikipedia},
  month = aug,
  year = {2017},
  file = {/home/yuri/Zotero/storage/28RZEM35/index.html}
}
% == BibTeX quality report for AutomatedPlanningScheduling2017:
% Missing required field 'author'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@book{gammaDesignPatternsElements1994,
  address = {{Reading, Mass}},
  edition = {Edi{\c c}{\~a}o: 1},
  title = {{Design Patterns: Elements of Reusable Object-Oriented Software}},
  isbn = {978-0-201-63361-0},
  shorttitle = {{Design Patterns}},
  abstract = {Capturing a wealth of experience about the design of object-oriented software, four top-notch designers present a catalog of simple and succinct solutions to commonly occurring design problems. Previously undocumented, these 23 patterns allow designers to create more flexible, elegant, and ultimately reusable designs without having to rediscover the design solutions themselves. The authors begin by describing what patterns are and how they can help you design object-oriented software. They then go on to systematically name, explain, evaluate, and catalog recurring designs in object-oriented systems. With Design Patterns as your guide, you will learn how these important patterns fit into the software development process, and how you can leverage them to solve your own design problems most efficiently.},
  language = {Ingl{\^e}s},
  publisher = {{Addison-Wesley Professional}},
  author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
  year = {10 de novembro de 1994}
}
% == BibTeX quality report for gammaDesignPatternsElements1994:
% ? Title looks like it was stored in title-case in Zotero

@book{boehmSoftwareEngineeringEconomics1981,
  address = {{Englewood Cliffs, N.J}},
  edition = {Edi{\c c}{\~a}o: 1},
  title = {{Software Engineering Economics}},
  isbn = {978-0-13-822122-5},
  abstract = {Software Engineering Economics is an invaluable guide to determining software costs, applying the fundamental concepts of microeconomics to software engineering, and utilizing economic analysis in software engineering decision making.},
  language = {Ingl{\^e}s},
  publisher = {{Prentice Hall}},
  author = {Boehm, Barry W.},
  year = {1 de novembro de 1981}
}
% == BibTeX quality report for boehmSoftwareEngineeringEconomics1981:
% ? Title looks like it was stored in title-case in Zotero

@article{denovaeskucinskisPlanningOnboardSatellites2013,
  title = {Planning On-Board Satellites for the Goal-Based Operations for Space Missions},
  volume = {11},
  number = {4},
  journal = {IEEE Latin America Transactions},
  author = {{de Novaes Kucinskis}, Fabricio and Ferreira, Mauricio Goncalves Vieira},
  year = {2013},
  pages = {1110--1120},
  file = {/home/yuri/Zotero/storage/Z7VXBYN2/10.1109@TLA.2013.6601757.pdf}
}

@article{kucinskisOnboardSatelliteSoftware2013,
  title = {On-Board Satellite Software Architecture for the Goal-Based {{Brazilian}} Mission Operations},
  volume = {28},
  issn = {0885-8985},
  abstract = {This article described the development and implementation of a software architecture to perform on-board planning for INPE satellites. This enables, with respect to the space segment, the adoption of the goal-based operations paradigm and consequently allows for an increase of the autonomy level. The performance presented by the architecture in a realistic case study is comparable to those reported by the only three NASA missions that have run on-board planning in the space segment up-to-date. A new planning modeling language was created, which allows a description of the operations domain closer to the real system and with few abstractions. The implemented architecture is integrated to the flight software and fit to run in space-qualified hardware with limited processing power.},
  number = {8},
  journal = {IEEE Aerospace and Electronic Systems Magazine},
  doi = {10.1109/MAES.2013.6575409},
  author = {Kucinskis, F. de Novaes and Ferreira, M. G. V.},
  month = aug,
  year = {2013},
  keywords = {artificial satellites,Satellites,Space vehicles,aerospace computing,planning,NASA,NASA mission,Space missions,Aerospace electronics,autonomy level,flight software,goal-based Brazilian mission operation,goal-based operations paradigm,INPE satellite,on-board planning,on-board satellite software architecture,planning modeling language,processing power,software architecture,Software architecture,space segment,space-qualified hardware,specification languages},
  pages = {32-45},
  file = {/home/yuri/Zotero/storage/KHH493BE/Kucinskis and Ferreira - 2013 - On-board satellite software architecture for the g.pdf;/home/yuri/Zotero/storage/JJVCNB8W/6575409.html}
}

@article{francois-lavetStudyPassiveActive2010,
  title = {Study of Passive and Active Attitude Control Systems for the {{OUFTI}} Nanosatellites},
  journal = {University of Li{\`e}ge, Faculty of Applied Sciences, Belgium},
  author = {{Francois-Lavet}, Vincent},
  year = {2010},
  pages = {22--57},
  file = {/home/yuri/Zotero/storage/JECY2WDJ/OUFTI_ADCS_2010_05_31.pdf}
}
% == BibTeX quality report for francois-lavetStudyPassiveActive2010:
% Missing required field 'number'
% Missing required field 'volume'

@article{hossenfelderScreamsExplanationFinetuning2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.02176},
  primaryClass = {gr-qc, physics:hep-ph, physics:hep-th, physics:physics},
  title = {Screams for {{Explanation}}: {{Finetuning}} and {{Naturalness}} in the {{Foundations}} of {{Physics}}},
  shorttitle = {Screams for {{Explanation}}},
  abstract = {We critically analyze the rationale of arguments from finetuning and naturalness in particle physics and cosmology. Some other numerological coincidences are also discussed.},
  journal = {arXiv:1801.02176 [gr-qc, physics:hep-ph, physics:hep-th, physics:physics]},
  author = {Hossenfelder, S.},
  month = jan,
  year = {2018},
  keywords = {General Relativity and Quantum Cosmology,High Energy Physics - Phenomenology,High Energy Physics - Theory,Physics - History and Philosophy of Physics},
  file = {/home/yuri/Zotero/storage/K93PIQML/1801.html}
}
% == BibTeX quality report for hossenfelderScreamsExplanationFinetuning2018:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1801.02176 [gr-qc, physics:hep-ph, physics:hep-th, physics:physics]
% ? Title looks like it was stored in title-case in Zotero

@article{foersterLearningOpponentLearningAwareness2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.04326},
  primaryClass = {cs},
  title = {Learning with {{Opponent}}-{{Learning Awareness}}},
  abstract = {Multi-agent settings are quickly gathering importance in machine learning. Beyond a plethora of recent work on deep multi-agent reinforcement learning, hierarchical reinforcement learning, generative adversarial networks and decentralized optimization can all be seen as instances of this setting. However, the presence of multiple learning agents in these settings renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method that reasons about the anticipated learning of the other agents. The LOLA learning rule includes an additional term that accounts for the impact of the agent's policy on the anticipated parameter update of the other agents. We show that the LOLA update rule can be efficiently calculated using an extension of the likelihood ratio policy gradient update, making the method suitable for model-free RL. This method thus scales to large parameter and input spaces and nonlinear function approximators. Preliminary results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma (IPD), while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to infinitely repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents can successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also apply LOLA to a grid world task with an embedded social dilemma using deep recurrent policies. Again, by considering the learning of the other agent, LOLA agents learn to cooperate out of selfish interests.},
  journal = {arXiv:1709.04326 [cs]},
  author = {Foerster, Jakob N. and Chen, Richard Y. and {Al-Shedivat}, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory},
  file = {/home/yuri/Zotero/storage/RPBNKJY8/Foerster et al. - 2017 - Learning with Opponent-Learning Awareness.pdf;/home/yuri/Zotero/storage/48N8H7LR/1709.html}
}
% == BibTeX quality report for foersterLearningOpponentLearningAwareness2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1709.04326 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{peiDeepXploreAutomatedWhitebox2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.06640},
  primaryClass = {cs},
  title = {{{DeepXplore}}: {{Automated Whitebox Testing}} of {{Deep Learning Systems}}},
  shorttitle = {{{DeepXplore}}},
  abstract = {Deep learning (DL) systems are increasingly deployed in safety- and security-critical domains including self-driving cars and malware detection, where the correctness and predictability of a system's behavior for corner case inputs are of great importance. Existing DL testing depends heavily on manually labeled data and therefore often fails to expose erroneous behaviors for rare inputs. We design, implement, and evaluate DeepXplore, the first whitebox framework for systematically testing real-world DL systems. First, we introduce neuron coverage for systematically measuring the parts of a DL system exercised by test inputs. Next, we leverage multiple DL systems with similar functionality as cross-referencing oracles to avoid manual checking. Finally, we demonstrate how finding inputs for DL systems that both trigger many differential behaviors and achieve high neuron coverage can be represented as a joint optimization problem and solved efficiently using gradient-based search techniques. DeepXplore efficiently finds thousands of incorrect corner case behaviors (e.g., self-driving cars crashing into guard rails and malware masquerading as benign software) in state-of-the-art DL models with thousands of neurons trained on five popular datasets including ImageNet and Udacity self-driving challenge data. For all tested DL models, on average, DeepXplore generated one test input demonstrating incorrect behavior within one second while running only on a commodity laptop. We further show that the test inputs generated by DeepXplore can also be used to retrain the corresponding DL model to improve the model's accuracy by up to 3\%.},
  journal = {arXiv:1705.06640 [cs]},
  doi = {10.1145/3132747.3132785},
  author = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
  year = {2017},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Learning,Computer Science - Software Engineering},
  pages = {1-18},
  file = {/home/yuri/Zotero/storage/UV26CESY/Pei et al. - 2017 - DeepXplore Automated Whitebox Testing of Deep Lea.pdf;/home/yuri/Zotero/storage/CR8C4SM4/1705.html}
}
% == BibTeX quality report for peiDeepXploreAutomatedWhitebox2017:
% Missing required field 'number'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1705.06640 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{howardMobileNetsEfficientConvolutional2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04861},
  primaryClass = {cs},
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  journal = {arXiv:1704.04861 [cs]},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/yuri/Zotero/storage/QSQ7NFT2/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;/home/yuri/Zotero/storage/JXPXVAUX/1704.html}
}
% == BibTeX quality report for howardMobileNetsEfficientConvolutional2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1704.04861 [cs]
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{joImplementingControlMission2012,
  title = {Implementing Control and Mission Software of {{UAV}} by Exploiting Open Source Software-Based Arinc 653},
  abstract = {The Integrated Modular Avionics (IMA) architecture has been suggested to address the Size, Weight, and Power (SWaP) issues and provide better software consolidation and testability by means of partitioning. Though the IMA architecture is mainly discussed from the view point of large aircrafts or manned aerial vehicles, small Unmanned Aerial Vehicles (UAV) are one that indeed requires IMA to reduce SWaP. In this study, we design and implement UAV control and mission software over ARINC 653. Especially we utilize our Linux-based ARINC-653, which can provide abundant development tools, software libraries, and device drivers due to the nature of Linux. Our control and mission software include Operational Flight Program (OFP), Video Streaming Program (VSP), Ground Control Program (GCP), and Ground Monitoring Program (GMP). We test our programs in a HILS environment and show that these run correctly in terms of functionality and real-time requirements. Our study also suggests few extensions for process scheduling and inter-partition communication of ARINC 653.},
  booktitle = {2012 {{IEEE}}/{{AIAA}} 31st {{Digital Avionics Systems Conference}} ({{DASC}})},
  doi = {10.1109/DASC.2012.6382436},
  author = {Jo, H. C. and Han, S. and Lee, S. H. and Jin, H. W.},
  month = oct,
  year = {2012},
  keywords = {Software,aerospace engineering,control engineering computing,mobile robots,Computer architecture,Monitoring,Aerospace electronics,UAV,public domain software,Vehicles,aircrafts,autonomous aerial vehicles,avionics,control software,device drivers,ground control program,ground monitoring program,Helicopters,HILS environment,IMA architecture,integrated modular avionics,interpartition communication,Linux-based ARINC-653,mission software,open source software-based ARINC 653,operational flight program,power issue,process scheduling,size issue,software consolidation,software libraries,software testability,Streaming media,unmanned aerial vehicles,video streaming program,weight issue},
  pages = {8B2-1-8B2-9},
  file = {/home/yuri/Zotero/storage/ASQBMBNN/6382436.html;/home/yuri/Zotero/storage/PRL4BV2V/6382436.html}
}
% == BibTeX quality report for joImplementingControlMission2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{C3Gui2016,
  title = {C3-{{Gui}}},
  year = {2016},
  file = {/home/yuri/Zotero/storage/P8VPQMZY/publicacao_c3_gui.pdf}
}
% == BibTeX quality report for C3Gui2016:
% Missing required field 'author'
% Missing required field 'booktitle'
% Missing required field 'pages'
% Missing required field 'publisher'

@inproceedings{brzozowskiOverviewResearchStateoftheart2017,
  title = {Overview of the Research on State-of-the-Art Measurement Sensors for {{UAV}} Navigation},
  abstract = {The skyrocketing development of the civilian drones' market, which follows the military solutions, results in boosting research on UAV avionics systems. The research includes a development of classic navigation and control as well as innovative methods that require e.g. infrared and visible light cameras, laser or magnetic field sensors. In this paper, we focus on the research on UAV's navigation and control systems, state-of-the-art measurement sensors involved in classic systems as well as the research on innovative methods of navigation and control.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999532},
  author = {Brzozowski, B. and R{\'o}chala, Z. and Wojtowicz, K.},
  month = jun,
  year = {2017},
  keywords = {Aerospace electronics,UAV,sensors,Temperature sensors,autonomous aerial vehicles,avionics,Accelerometers,aircraft navigation,autopilot,avionics system,civilian drone market,infrared cameras,laser,magnetic field sensors,Magnetic sensors,Magnetometers,measurement sensor,measurement sensors,navigation system,Sensor systems,skyrocketing development,UAV avionics systems,UAV control systems,UAV navigation,visible light cameras},
  pages = {565-570},
  file = {/home/yuri/Zotero/storage/6TY2VUCE/7999532.html;/home/yuri/Zotero/storage/SZSQGIKN/7999532.html}
}
% == BibTeX quality report for brzozowskiOverviewResearchStateoftheart2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{zoppiFunctionalbasedVerificationSpacecraft2017,
  title = {Functional-Based Verification for Spacecraft {{SW}}: {{The}} Electrical Power Subsystem},
  shorttitle = {Functional-Based Verification for Spacecraft {{SW}}},
  abstract = {This paper focuses on verification of spacecraft SW for the electrical power subsystem (EPS) for a particular geostationary satellite. Choosing an appropriate topology is a key point in the process of electrical power subsystem design. Concepts of energy transfer. i.e. Direct Energy Transfer (DET) and Peak Power Tracker (PPT) are discussed. The EPS spacecraft SW verification is based on the functionality of the system and its design. A test software approach is presented in order to verify the spacecraft SW functionality of EPS components.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999527},
  author = {Zoppi, M. and Cerbo, A. Di and Tipaldi, M.},
  month = jun,
  year = {2017},
  keywords = {Satellites,Space vehicles,EPS,Sun,Batteries,Electrcal power subsystem architecture (EPS),electrical power subsystem,energy transfer concept,functional-based verification,geostationary satellite,Photovoltaic cells,power system topology,Regulators,space vehicle power plants,spacecraft SW,spacecraft SW verification,test designs,test software approach,Voltage control},
  pages = {1-5},
  file = {/home/yuri/Zotero/storage/6SDIW743/7999527.html;/home/yuri/Zotero/storage/8625GL43/7999527.html}
}
% == BibTeX quality report for zoppiFunctionalbasedVerificationSpacecraft2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@misc{XavierFranchPersonal,
  title = {Xavier {{Franch}} - {{Personal Page}}},
  howpublished = {http://www.essi.upc.edu/\textasciitilde{}franch/},
  file = {/home/yuri/Zotero/storage/WZ73Y8TL/~franch.html}
}
% == BibTeX quality report for XavierFranchPersonal:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@misc{JulioCesarSampaio,
  title = {Julio {{Cesar Sampaio}} Do {{Prado Leite}} (Jcspl) {{PUC}}-{{Rio}}},
  howpublished = {http://www-di.inf.puc-rio.br/\textasciitilde{}julio/},
  file = {/home/yuri/Zotero/storage/WAPD96LQ/~julio.html}
}
% == BibTeX quality report for JulioCesarSampaio:
% Missing required field 'author'
% Missing required field 'year'

@inproceedings{rodriguez-dapenaUnusedPowerSW2017,
  title = {The Unused Power of {{SW}} Safety Dependability Analyses},
  abstract = {In many domains, when software controls or implements safety critical functionalities, software safety and dependability analyses are needed in order to understand how it may fail, the consequences of these failures and how to avoid or mitigate these failures. But from experience in different projects, when these analyses are performed at the software level, they are not used to their maximum extent: they are always late and their content is often incomplete (just to justify the already assigned criticality level). They are not always considered as bringing safety and dependability requirements into the project. Existing techniques may be used along all software development and used for many different purposes to support the definition, verification and demonstration of software related safety and dependability characteristics. Therefore the questions are: When are these analyses required? Why are they so difficult to be performed? How are they finally used and why in such a limited way so far? This paper will analyze when these analyses are required, what for, which ones are required by ECSS and evaluate how to better use them in Space projects.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999546},
  author = {{Rodr{\'i}guez-Dapena}, P.},
  month = jun,
  year = {2017},
  keywords = {Schedules,Standards,Organizations,systems analysis,Fault diagnosis,Safety,mitigations,safety critical functionalities,safety-critical software,SFMECA,SFTA,software dependability,software safety,Software safety,software safety and dependability,space projects,unused power},
  pages = {6-10},
  file = {/home/yuri/Zotero/storage/64DDNCCJ/7999546.html;/home/yuri/Zotero/storage/VIVY46QU/7999546.html}
}
% == BibTeX quality report for rodriguez-dapenaUnusedPowerSW2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{dangeloSpacecraftAutonomyModeled2017,
  title = {Spacecraft Autonomy Modeled via {{Markov}} Decision Process and Associative Rule-Based Machine Learning},
  abstract = {Spacecraft on-board autonomy is an important topic in currently developed and future space missions. In this study, we present a robust approach to the optimal policy of autonomous space systems modeled via Markov Decision Process (MDP) from the values assigned to its transition probability matrix. After addressing the curse of dimensionality in solving the formulated MDP problem via Approximate Dynamic Programming, we use an Apriori-based Association Classifier to infer a specific optimal policy. Finally, we also assess the effectiveness of such optimal policy in fulfilling the spacecraft autonomy requirements.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999589},
  author = {D'Angelo, G. and Tipaldi, M. and Glielmo, L. and Rampone, S.},
  month = jun,
  year = {2017},
  keywords = {Space vehicles,learning (artificial intelligence),Machine Learning,space vehicles,probability,Aerospace electronics,Aggregates,Itemsets,space mission,approximate dynamic programming,Approximate Dynamic Programming,apriori-based association classifier,Apriori-based classifier,Association Classifier,associative rule-based machine learning,autonomous space system,curse of dimensionality,decision theory,dynamic programming,learning systems,Markov decision process,Markov Decision Process,Markov processes,matrix algebra,MDP,optimal policy inference,pattern classification,robust control,Robustness,Spacecraft Autonomy,spacecraft autonomy model,spacecraft autonomy requirement,spacecraft on-board autonomy,transition probability matrix},
  pages = {324-329},
  file = {/home/yuri/Zotero/storage/464A6IJN/7999589.html;/home/yuri/Zotero/storage/ZF6XU9K2/authors.html}
}
% == BibTeX quality report for dangeloSpacecraftAutonomyModeled2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{brzozowskiMagneticFieldMapping2017,
  title = {Magnetic Field Mapping as a Support for {{UAV}} Indoor Navigation System},
  abstract = {Safe indoor flights of unmanned aerial vehicles (UAVs) requires an independent measurement systems, that will enable efficient navigation in the absence of GPS data. One of the many solutions currently being developed is the use of information about changes in the value of the local magnetic field. This paper presents ways of recording, visualizing and mapping local magnetic field changes that can be used as a support for indoor navigation systems. At the beginning we reviewed devices for acquisition of magnetic field strength and the type of data being recorded. In the next step we analyzed the possibilities of visualization of acquired data. Finally the methods used to generate magnetic field maps of enclosed areas have been presented. In each of the aspects covered in this paper, solutions developed by the authors will be described.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999535},
  author = {Brzozowski, B. and Ka{\'z}mierczak, K.},
  month = jun,
  year = {2017},
  keywords = {Area measurement,Data visualization,UAV,Magnetic fields,autonomous aerial vehicles,unmanned aerial vehicles,aircraft navigation,Magnetometers,Avionics,data visualization,independent measurement systems,indoor flight safety,indoor navigation,Indoor Navigation,local magnetic field recording,local magnetic field visualization,Magnetic field,magnetic field mapping,Magnetic field measurement,magnetic field strength acquisition,magnetic fields,Permanent magnets,Position estimation,UAV indoor navigation system,Unmanned aerial vehicles},
  pages = {583-588},
  file = {/home/yuri/Zotero/storage/DWIM6X9K/7999535.html}
}
% == BibTeX quality report for brzozowskiMagneticFieldMapping2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{liOnorbitCalibrationMethod2017,
  title = {An On-Orbit Calibration Method for Aerospace Hall Current Sensor},
  abstract = {The paper introduces an on-orbit calibration method for aerospace hall current sensor based on excitation response with verification by experiments. The method is applicable for long-life on-orbit spacecraft without manual calibration periodically, and it is possible to improve measurement reliability and accuracy for the hall current sensor in a long-term work life.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999534},
  author = {Li, Y. and Min, L. and Yan, W.},
  month = jun,
  year = {2017},
  keywords = {Space vehicles,Calibration,aerospace instrumentation,Uncertainty,Current measurement,Extraterrestrial measurements,aerospace hall current sensor,aerospace Hall current sensor,Battery charge measurement,calibration,excitation response,Hall effect transducers,long-life on-orbit spacecraft,long-life spacecraft,measurement reliability,Measurement uncertainty,on-orbit calibration,on-orbit calibration method},
  pages = {578-582},
  file = {/home/yuri/Zotero/storage/AGWLBVIB/7999534.html}
}
% == BibTeX quality report for liOnorbitCalibrationMethod2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{addabboFourierIndependentComponent2017,
  title = {Fourier Independent Component Analysis of Radar Micro-{{Doppler}} Features},
  abstract = {The capability of discriminating radar targets exhibiting multiple moving parts has become of great interest for both aerospace and ground-based target recognition and analysis. In particular, helicopters and other targets with rotors, as for instance miniature Unmanned Aerial Vehicles, exhibit peculiar characteristics in the radar return that can be used for their recognition. In this paper a novel algorithm to address the problem of micro-Doppler signature unmixing is proposed, exploiting the signal separation capabilities of the Independent Component Analysis (ICA). The core of the algorithm is represented precisely by the use of the ICA procedure, that has been already proved to be a very effective technique for separating hidden information in mixtures of observations. ICA has been successfully employed in several applications such as wireless communications, radar beamforming, trace-gases unmixing and medical imaging processing. The helicopter's rotor blade signature unmixing from a multi-static radar system is considered as case study and results obtained through the application of ICA to simulated multi-component micro-Doppler signatures show the capability of the proposed approach to successfully accomplish the unmixing operation.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999528},
  author = {Addabbo, P. and Clemente, C. and Ullo, S. L.},
  month = jun,
  year = {2017},
  keywords = {Feature extraction,Receivers,Helicopters,array signal processing,Blades,Doppler radar,Fourier independent component analysis,ground-based target recognition,helicopter classification,ICA,independent component analysis,Independent Component Analysis (ICA),micro-Doppler features,miniature unmanned aerial vehicles,Radar,radar micro-doppler features,radar target recognition,rotor blade signature unmixing,Rotors,signal separation capabilities,Spectrogram},
  pages = {45-49},
  file = {/home/yuri/Zotero/storage/QQ32RYIT/7999528.html}
}
% == BibTeX quality report for addabboFourierIndependentComponent2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{sokenSurveyCalibrationAlgorithms2017,
  title = {A Survey of Calibration Algorithms for Small Satellite Magnetometers},
  abstract = {Magnetometers are an integral part of attitude determination system for the low-Earth orbiting small satellites as they are lightweight, inexpensive and reliable. Yet using magnetometers for attitude determination is not straightforward because of the sensor errors. These errors limit the overall achievable attitude determination accuracy. Thus far different methods to cope with magnetometer errors and calibrate the magnetometers have been proposed. A new research field is the specific errors for magnetometers onboard the small satellites and their time-variation characteristics. In accordance, algorithms which consider also the time-varying error terms are proposed. This paper reviews the recent calibration algorithms for small satellite magnetometers. The survey mainly covers batch and recursive estimation algorithms which are capable of estimating the time-varying magnetometer error terms. It presents the foundation of each algorithm and covers issues about the algorithm design, application and performance. In the end possible directions in this research field are briefly discussed.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999539},
  author = {Soken, H. E.},
  month = jun,
  year = {2017},
  keywords = {artificial satellites,Satellites,Calibration,Magnetometers,calibration,Measurement uncertainty,attitude determination system,attitude estimation,attitude measurement,calibration algorithm,Earth orbit,Estimation,low-Earth orbiting small satellite magnetometer,magnetic field measurement,magnetic sensors,magnetometer calibration,magnetometers,Q measurement,recursive estimation,recursive estimation algorithm,small satellite,time-varying error estimation,TV},
  pages = {62-67},
  file = {/home/yuri/Zotero/storage/E34I6GD7/7999539.html}
}
% == BibTeX quality report for sokenSurveyCalibrationAlgorithms2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{colombattiKorusX2014Drone2017,
  title = {Korus \#x2014; {{A}} Drone Project for Visual and {{IR}} Imaging},
  abstract = {The focus of the project is to create new approaches to the study of the past through the use of innovative aero-space technologies to measure, analyse and reconstruct the ancient landscape and its remaining natural and anthropic traces. The {\'e}quipe, based at the University of Padua, is constituted by archaeologists, egyptologists, mechanical and software engineers, physicists and computer scientists and is now active on the proto-historic site of Rozto in Italy. The research includes the analysis of the historical records, as old maps and aerial photographs of the past, field use of drones and the creation of a GIS platforms to collect and see the data all together.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999536},
  author = {Colombatti, G. and Aboudan, A. and Bettanini, C. and Magnini, L. and Bettineschi, C. and Deotto, G. and Toninello, L. and Debei, S. and Guio, A. De and Zanovello, P. and Menegazzi, A.},
  month = jun,
  year = {2017},
  keywords = {Software,Sensors,geophysical techniques,Remote sensing,geographic information systems,aerial photographs,aerospace technologies,ancient landscape,archaeologists,Cameras,computer scientists,drone project,Drones,egyptologists,GIS platforms,historical records,history,Image reconstruction,IR imaging,Italy,Korus,mechanical engineers,old maps,physicists,Rozto protohistoric site,software engineers,visual imaging},
  pages = {589-592},
  file = {/home/yuri/Zotero/storage/6DHP6MCY/7999536.html}
}
% == BibTeX quality report for colombattiKorusX2014Drone2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{papaObstacleDetectionRanging2017,
  title = {Obstacle Detection and Ranging Sensor Integration for a Small Unmanned Aircraft System},
  abstract = {In the last few years, Unmanned Aerial Systems (UAS) have been attracting enormous research interest, being employed in military and civilian missions (e.g. search and rescue, disaster assessment, urban traffic monitoring, 3D mapping, etc.) that would be risky or impossible for a human to perform. For autonomous or aided operations (e.g. automatic or aided landing), it is crucial to have on board an effective suite of sensors allowing navigation in unknown environments. This work performs obstacle detection and attitude estimation for a small quad-rotor by using low-cost sensors, namely, a Sonic Ranging Sensor (SRS) and an InfraRed Sensor (IRS), widely used in mobile applications for short distance measurements. Both sensors were controlled and managed by a microcontroller (Arduino Mega 2560) and synchronized at 2-Hz sampling. Attitude estimation was performed using multiple distance measurements between a solid surface (e.g. wall or ground) and the SRS/IRS sensors. A short range of 20-150 cm has been considered in order to assist the UAS landing procedure. The main objective was to integrate the SRS and IRS measurements for accurate distance and attitude estimation by means of variance minimization. Simulations and experimental results show the feasibility of low-cost sensor fusion for obstacle detection and ranging applications on a small rotary wing UAS.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999533},
  author = {Papa, U. and Core, G. Del and Giordano, G. and Ponte, S.},
  month = jun,
  year = {2017},
  keywords = {microcontroller,Standards,Atmospheric measurements,microcontrollers,Temperature sensors,autonomous aerial vehicles,attitude estimation,attitude measurement,Estimation,Data Fusion,distance measurement,Distance measurement,entry; descent and landing (spacecraft),Flight Mechanics,frequency 2 Hz,helicopters,infrared detectors,infrared sensor,Infrared sensor,Integration,low-cost sensor fusion,low-cost sensors,minimisation,mobile application,multiple distance measurement,obstacle detection,ranging sensor integration,sensor fusion,short distance measurement,small quad-rotor,small rotary wing UAS,small unmanned aircraft system,sonic ranging sensor,Sonic sensor,SRS-IRS sensor,Temperature measurement,UAS landing procedure,UAV/UAS,variance minimization,Voltage measurement},
  pages = {571-577},
  file = {/home/yuri/Zotero/storage/QMD4BHCY/7999533.html}
}
% == BibTeX quality report for papaObstacleDetectionRanging2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{sansoneRelativeNavigationSensor2017,
  title = {A Relative Navigation Sensor for {{CubeSats}} Based on Retro-Reflective Markers},
  abstract = {Over the last years, nanosatellites based on the CubeSat standard have been increasingly exploited by both academic institutions and commercial companies. The low cost and short development time of CubeSats make such miniature spacecraft interesting for a variety of applications. A particularly appealing future field of application for nanosatellites is On-Orbit Servicing of existing orbital assets. A number of mission concepts foresee the employment of miniature spacecraft to perform a variety of operations on orbital vehicles, ranging from inspection and monitoring to assembly and repairing. In this framework, the development of technologies that enable proximity navigation and/or docking between a nanosatellite chaser and a target object is of interest. In particular, miniaturized navigation sensors for the estimation of relative position and attitude of the involved vehicles are required. The work presented here focuses on the development of an IR-based relative navigation sensor compatible with CubeSat standard nanosatellites. The system estimates the relative pose and position of the target by taking images of the object, which is equipped with retro-reflecting fiducial markers illuminated by an array of IR LEDs on the chaser. The system architecture and operation are described, and preliminary laboratory test results are presented.},
  booktitle = {2017 {{IEEE International Workshop}} on {{Metrology}} for {{AeroSpace}} ({{MetroAeroSpace}})},
  doi = {10.1109/MetroAeroSpace.2017.7999529},
  author = {Sansone, F. and Branz, F. and Francesconi, A.},
  month = jun,
  year = {2017},
  keywords = {artificial satellites,Space vehicles,Standards,Navigation,attitude estimation,attitude measurement,Estimation,Cameras,infrared detectors,infrared sensor,aerospace navigation,CubeSat,CubeSat standard,IR LEDs array,IR-based relative navigation sensor,light emitting diodes,Light emitting diodes,Machine vision,nanosatellite,on-orbit servicing,orbital vehicles,position measurement,proximity navigation,relative navigation,relative navigation sensor,relative position estimation,retroreflecting fiducial marker,retroreflective marker,spacecraft miniaturization},
  pages = {550-555},
  file = {/home/yuri/Zotero/storage/BWX5ZYC9/7999529.html}
}
% == BibTeX quality report for sansoneRelativeNavigationSensor2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{goldbergPrimerNeuralNetwork2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.00726},
  primaryClass = {cs},
  title = {A {{Primer}} on {{Neural Network Models}} for {{Natural Language Processing}}},
  abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
  journal = {arXiv:1510.00726 [cs]},
  author = {Goldberg, Yoav},
  month = oct,
  year = {2015},
  keywords = {Computer Science - Computation and Language},
  file = {/home/yuri/Zotero/storage/FZ7YRM28/Goldberg - 2015 - A Primer on Neural Network Models for Natural Lang.pdf;/home/yuri/Zotero/storage/RGPURXJ6/1510.html}
}
% == BibTeX quality report for goldbergPrimerNeuralNetwork2015:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1510.00726 [cs]
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{fenManagementOperationCommunication2016,
  title = {Management and {{Operation}} of {{Communication Equipment Based}} on {{Big Data}}},
  abstract = {Big Data is a hot spot of the current field of information technology, describing the characteristics of Big Data technologies, analyzes the status and problems of communication equipment management, based on building a new security management system of communications equipment, investigate Big Data application in communication equipment management. Ander the background of Big Data, Research on the way how to improve the communication equipment operation and maintenance effective by three sides, such as improving Big Data infrastructure, data of qualified personnel, security tripartite.},
  booktitle = {2016 {{International Conference}} on {{Robots Intelligent System}} ({{ICRIS}})},
  doi = {10/gfz7mx},
  author = {Fen, Z. and Yanqin, Z. and Chong, C. and Ling, S.},
  month = aug,
  year = {2016},
  keywords = {Real-time systems,Monitoring,Big Data,Big data,Big Data application,Big Data technology characteristics,Communication equipment,Communication Equipment,communication equipment management problems,communication equipment operation,Effectiveness,information technology,Maintenance engineering,Management and Operation,Personnel,security management system,telecommunication computing},
  pages = {246-248},
  file = {/home/yuri/Zotero/storage/PC6G4RNQ/Fen et al_2016_Management and Operation of Communication Equipment Based on Big Data.pdf;/home/yuri/Zotero/storage/3ZDD9L5R/7757119.html}
}
% == BibTeX quality report for fenManagementOperationCommunication2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{geibPayloadStateHealth2002,
  title = {Payload State of Health Monitoring Design for next Generation Satellite Constellations},
  volume = {1},
  abstract = {The authors have developed a Payload State of Health (PSOH) application that monitors the State of Health (SOH) of a non-homogeneous set of payloads in a satellite constellation. This application is part of a satellite ground station that processes sensor data from payloads and monitors the SOH of the ground station itself. The first payload of the constellation was launched in 2001. The PSOH application was used successfully for Early Orbit Turn-on (EOT) testing, and it is currently processing data autonomously. The PSOH application was developed with three types of users in mind. It accommodates the needs of payload designers who predominately use it during EOT. It uses a highly intuitive graphical Human Computer Interface (HCI) that allows a non-expert ground station operator to monitor all payloads of the constellation with a minimal chance of human error. It generates messages that provide a persistent record of any anomalies for use by the sensor data analyst. This paper shows how early development of the ground station PSOH application can benefit the three main users the sensor designer, the operator, and the data analyst.},
  booktitle = {Proceedings, {{IEEE Aerospace Conference}}},
  doi = {10/ct68pr},
  author = {Geib, P. L. and Cox, D. D. and Tomasi, A. M.},
  month = mar,
  year = {2002},
  keywords = {artificial satellites,aerospace computing,Testing,ground support systems,Monitoring,Data analysis,Payloads,condition monitoring,Application software,Computer interfaces,Computer displays,computerised monitoring,early orbit turn-on testing,graphical human computer interface,graphical user interfaces,Human computer interaction,monitoring,next generation satellite constellations,payload state of health monitoring design,Satellite constellations,satellite ground station,Satellite ground stations},
  pages = {1-1},
  file = {/home/yuri/Zotero/storage/GT6DRXWU/Geib et al_2002_Payload state of health monitoring design for next generation satellite.pdf;/home/yuri/Zotero/storage/S8X4AMDB/Geib et al_2002_Payload state of health monitoring design for next generation satellite.pdf;/home/yuri/Zotero/storage/W9776T4U/1036841.html;/home/yuri/Zotero/storage/Y7TJH28S/1036841.html}
}
% == BibTeX quality report for geibPayloadStateHealth2002:
% Missing required field 'publisher'

@inproceedings{ningResearchWarshipCommunication2014,
  title = {Research on {{Warship Communication Operation}} and {{Maintenance Management Based}} on {{Big Data}}},
  abstract = {Big data as another revolution of information technology is happening and has a huge impact on many fields of the society. In the military communication field, massive and multi-source data with explosive growth of operational command, operation and maintenance, meteorological environment provides the information source for data analysis in "information war" era. In order to improve the operational command efficiency in actual combat and enable to real-time predict and analyze battlefield situation and network performance and fault needs the support of intelligent and automatic communication operation and maintenance. In this paper, on a basis of related technologies of big data, focusing on the requirement of big data in the warship communication operation and maintenance management, and proposing a management architecture of warship communication operation and maintenance management based on big data, and the advantages of big data in the application of warship communication operation and maintenance management system are analyzed. Finally several issues of big data hold in the application of warship communication operation and maintenance management are elaborated.},
  booktitle = {2014 {{International Conference}} on {{Cloud Computing}} and {{Big Data}}},
  doi = {10/gfz7mw},
  author = {Ning, D. and Chen, P. and Yuan, G. and Xu, J. and Xu, L.},
  month = nov,
  year = {2014},
  keywords = {Computer architecture,Real-time systems,data analysis,Servers,Big Data,Data analysis,Big data,Business,big data,information technology,Maintenance engineering,information source,information war era,maintenance engineering,maintenance management,meteorological environment,military communication field,military vehicles,multisource data,operation and maintenance management,operational command efficiency,ships,warship communication,warship communication operation},
  pages = {126-129},
  file = {/home/yuri/Zotero/storage/4TCFMXVL/Ning et al_2014_Research on Warship Communication Operation and Maintenance Management Based on.pdf;/home/yuri/Zotero/storage/EVMFLLQF/7062883.html}
}
% == BibTeX quality report for ningResearchWarshipCommunication2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{gonzaloChallengeCenturyLifespan2014,
  title = {On the Challenge of a Century Lifespan Satellite},
  volume = {70},
  issn = {03760421},
  abstract = {This paper provides a review of the main issues affecting satellite survivability, including a discussion on the technologies to mitigate the risks and to enhance system reliability. The feasibility of a 100-year lifespan space mission is taken as the guiding thread for the discussion. Such a mission, defined with a few preliminary requirements, could be used to deliver messages to our descendants regardless of the on-ground contingencies. After the analysis of the main threats for long endurance in space, including radiation, debris and micrometeoroids, atmospheric drag and thermal environment, the available solutions are investigated. A trade-off study analyses orbital profiles from the point of view of radiation, thermal stability and decay rate, providing best locations to maximize lifespan. Special attention is also paid to on-board power, in terms of energy harvesting and accumulation, highlighting the limitations of current assets, i.e. solar panels and batteries, and revealing possible future solutions. Furthermore, the review includes electronics, non-volatile memories and communication elements, which need extra hardening against radiation and thermal cycling if extra-long endurance is required. As a result of the analysis, a century-lifetime mission is depicted by putting together all the reviewed concepts. The satellite, equipped with reliability enhanced elements and system-level solutions such as smart hibernation policies, could provide limited but still useful performance after a 100-year flight.},
  language = {en},
  journal = {Progress in Aerospace Sciences},
  doi = {10/f6fdx4},
  author = {Gonzalo, Jes{\'u}s and Dom{\'i}nguez, Diego and L{\'o}pez, Deibi},
  month = oct,
  year = {2014},
  pages = {28-41},
  file = {/home/yuri/Zotero/storage/ZJP77DN3/Gonzalo et al. - 2014 - On the challenge of a century lifespan satellite.pdf}
}
% == BibTeX quality report for gonzaloChallengeCenturyLifespan2014:
% Missing required field 'number'

@inproceedings{dischnerCYGNSSMOCMeeting2016,
  title = {{{CYGNSS MOC}}; {{Meeting}} the Challenge of Constellation Operations in a Cost-Constrained World},
  abstract = {The Cyclone Global Navigation Satellite System (CYGNSS), selected as part of NASA's Earth Venture program and launching in 2016, will use an 8 observatory micro-satellite constellation to enhance the understanding of tropical cyclone intensity development. Operating the CYGNSS constellation in a cost effective manner is a fundamental cornerstone of achieving significant science return in a cost-constrained environment. The Southwest Research Institute (SwRI) has developed an innovative Mission Operations Center (MOC) designed to accommodate the constellation's significant planning and data downlink requirements [2]. The large number of data downlink opportunities necessitates a MOC that can easily identify and schedule ground contacts, and autonomously run the ground contact events from initiation of acquisition to delivery of the L0 science data. This paper will discuss the commercial off-the-shelf (COTS) and Government off-the-shelf (GOTS) tools leveraged, as well as in-house developed software that allow the CYGNSS MOC to meet the constellation operational challenge in a cost effective manner.},
  booktitle = {2016 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7k9},
  author = {Dischner, Z. and Redfern, J. and Rose, D. and Rose, R. and Ruf, C. and Vincent, M.},
  month = mar,
  year = {2016},
  keywords = {artificial satellites,Software,Schedules,scheduling,Orbits,Downlink,Observatories,commercial off-the-shelf tools,constellation operations,constellation significant planning,cost-constrained environment,COTS tools,Cyclone global navigation satellite system,Cyclones,CYGNSS MOC,data downlink requirements,GOTS tools,government off-the-shelf tools,ground contact scheduling,innovative Mission Operations Center,L0 science data,MOC,NASA Earth Venture program,observatory microsatellite constellation,satellite navigation,Southwest Research Institute,space vehicle navigation,SwRI,tropical cyclone intensity development},
  pages = {1-8},
  file = {/home/yuri/Zotero/storage/GGZHDBS5/Dischner et al_2016_CYGNSS MOC\; Meeting the challenge of constellation operations in a.pdf;/home/yuri/Zotero/storage/D4F5IMBG/7500676.html}
}
% == BibTeX quality report for dischnerCYGNSSMOCMeeting2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{nagCostRiskAnalysis2014,
  title = {Cost and Risk Analysis of Small Satellite Constellations for Earth Observation},
  abstract = {Distributed Space Missions (DSMs) are gaining momentum in their application to Earth science missions owing to their ability to increase observation sampling in spatial, spectral, temporal and angular dimensions. Past literature from academia and industry have proposed and evaluated many cost models for spacecraft as well as methods for quantifying risk. However, there have been few comprehensive studies quantifying the cost for multiple spacecraft, for small satellites and the cost risk for the operations phase of the project which needs to be budgeted for when designing and building efficient architectures. This paper identifies the three critical problems with the applicability of current cost and risk models to distributed small satellite missions and uses data-based modeling to suggest changes that can be made in some of them to improve applicability. Learning curve parameters to make multiple copies of the same unit, technological complexity based costing and COTS enabled small satellite costing have been studied and insights provided.},
  booktitle = {2014 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mv},
  author = {Nag, S. and LeMoigne, J. and de Weck, O.},
  month = mar,
  year = {2014},
  keywords = {artificial satellites,Satellites,Space vehicles,Earth,NASA,Payloads,spacecraft,Complexity theory,cost analysis,COTS,data-based modeling,distributed small satellite missions,distributed space missions,DSM,earth observation,Modeling,observation sampling,risk analysis,small satellite constellations},
  pages = {1-16},
  file = {/home/yuri/Zotero/storage/37KDSV67/Nag et al_2014_Cost and risk analysis of small satellite constellations for earth observation.pdf;/home/yuri/Zotero/storage/V3ITG3KJ/6836396.html}
}
% == BibTeX quality report for nagCostRiskAnalysis2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{blahutSpaceTechnologyST52003,
  title = {The Space Technology 5 ({{ST}}-5) Mission Constellation Control System and Operations Approach},
  volume = {7},
  booktitle = {2003 {{IEEE Aerospace Conference Proceedings}} ({{Cat}}. {{No}}.{{03TH8652}})},
  doi = {10/ccdbj8},
  author = {Blahut, K. and Bibyk, I.},
  month = mar,
  year = {2003},
  keywords = {Satellites,Space vehicles,NASA,Space missions,Control systems,Space technology,System testing,Batteries,Automation,Power system management},
  pages = {3435-3441},
  file = {/home/yuri/Zotero/storage/GVNDBACY/Blahut_Bibyk_2003_The space technology 5 (ST-5) mission constellation control system and.pdf;/home/yuri/Zotero/storage/ERKUE2SK/1234187.html}
}
% == BibTeX quality report for blahutSpaceTechnologyST52003:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{tsogIntelligentDataProcessing2018,
  title = {Intelligent Data Processing Using In-Orbit Advanced Algorithms on Heterogeneous System Architecture},
  abstract = {In recent years, commercial exploitation of small satellites and CubeSats has rapidly increased. Time to market of processed customer data products is becoming an important differentiator between solution providers and satellite constellation operators. Timely and accurate data dissemination is the key to success in the commercial usage of small satellite constellations which is ultimately dependent on a high degree of autonomous fleet management and automated decision support. The traditional way for disseminating data is limited by on the communication capability of the satellite and the ground terminal availability. Even though cloud computing solutions on the ground offer high analytical performance, getting the data from the space infrastructure to the ground servers poses a bottleneck of data analysis and distribution. On the other hand, adopting advanced and intelligent algorithms onboard offers the ability of autonomy, tasking of operations, and fast customer generation of low latency conclusions, or even real-time communication with assets on the ground or other sensors in a multi-sensor configuration. In this paper, the advantages of intelligent onboard processing using advanced algorithms for Heterogeneous System Architecture (HSA) compliant onboard data processing systems are explored. The onboard data processing architecture is designed to handle a large amount of high-speed streaming data and provides hardware redundancy to be qualified for the space mission application domain. We conduct an experimental study to evaluate the performance analysis by using image recognition algorithms based on an open source intelligent machine library ``MIOpen'' and an open standard ``OpenVX''. OpenVX is a cross-platform computer vision library.},
  booktitle = {2018 {{IEEE Aerospace Conference}}},
  doi = {10/gd8xch},
  author = {Tsog, N. and Behnam, M. and Sj{\"o}din, M. and Bruhn, F.},
  month = mar,
  year = {2018},
  keywords = {artificial satellites,Field programmable gate arrays,aerospace computing,Computer architecture,Sensors,data analysis,sensor fusion,accurate data dissemination,automated decision support,autonomous fleet management,computer vision,cross-platform computer vision library,CubeSats,data dissemination,Data processing,Graphics processing units,ground servers,ground terminal availability,heterogeneous system architecture,Heterogeneous System Architecture compliant onboard data,high-speed streaming data,image recognition,image recognition algorithms,in-orbit advanced algorithms,intelligent algorithms,intelligent data processing,intelligent onboard processing,MIOpen,multisensor configuration,open source intelligent machine library,OpenVX,satellite constellation operators,satellites,space infrastructure,space mission application domain},
  pages = {1-8},
  file = {/home/yuri/Zotero/storage/XED8D259/Tsog et al_2018_Intelligent data processing using in-orbit advanced algorithms on heterogeneous.pdf;/home/yuri/Zotero/storage/BV5HUYET/8396536.html}
}
% == BibTeX quality report for tsogIntelligentDataProcessing2018:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{lazregDesignArchitecturePicosatellites2016,
  title = {Design and Architecture of {{Pico}}-Satellites Network for Earth Coverage},
  abstract = {This paper presents a comparison between conventional satellites and Pico-satellites constellation and the benefits of Pico-satellites constellation at the coverage and data collection versus a classic big satellite at high altitude by showing who a several number of Pico-Satellite constellation can replace traditional and large satellites.},
  booktitle = {2016 2nd {{International Conference}} on {{Advanced Technologies}} for {{Signal}} and {{Image Processing}} ({{ATSIP}})},
  doi = {10/gfz7mt},
  author = {Lazreg, N. and Besbes, K.},
  month = mar,
  year = {2016},
  keywords = {Satellite broadcasting,Low earth orbit satellites,Earth,Orbits,remote sensing,classic big satellite,Constellation,Coverage,Data collection,Earth coverage,geophysical equipment,Imaging,Pico-Satellites,Pico-satellites constellation,Pico-satellites network architecture,Pico-satellites network design},
  pages = {601-605},
  file = {/home/yuri/Zotero/storage/3EGG44JN/Lazreg_Besbes_2016_Design and architecture of Pico-satellites network for earth coverage.pdf;/home/yuri/Zotero/storage/WLI2QPK2/7523152.html}
}
% == BibTeX quality report for lazregDesignArchitecturePicosatellites2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{yinjinDistributedCooperativeStorage2018,
  series = {Communications in {{Computer}} and {{Information Science}}},
  title = {Distributed {{Cooperative Storage Management Framework}} for {{Big Data}} in {{Satellite Network Operation}} and {{Maintenance}}},
  isbn = {978-981-10-7877-4},
  abstract = {In recent years, China's rapid growth on the demand for satellite communication application, not only impels the continuous expanding on the management scale of network operation and maintenance (O\&M) system, but also puts forward higher demand for intelligent network management and control. Under this circumstance, the data management function, which services as the core of the satellite network O\&M system, is faced with the serious management challenges brought by the huge amount and complex datasets. In this paper, we study the distributed cooperative storage management technologies for big data management issue in satellite network O\&M. We propose a distributed cooperative big data storage model for the satellite network O\&M, and further study the intra-site hybrid database management strategy and inter-site fast data synchronization technology, to improve the scalability and disaster tolerance of data service in the O\&M application. Finally, we evaluate the hybrid database architecture based on Oracle and HBase using the benchmark, and compare the theoretical network traffic with the actual flow measured by GoldenGate, then perform the quantitative analysis on the system disaster tolerance of data services.},
  language = {en},
  booktitle = {Space {{Information Networks}}},
  publisher = {{Springer Singapore}},
  author = {Yinjin, Fu and Rui, Hou and Jun, Xie},
  editor = {Yu, Quan},
  year = {2018},
  keywords = {\#nosource,Big data,Data synchronization,Disaster tolerance,Distributed storage,Hybrid database,Operation and maintenance,Satellite network},
  pages = {93-104}
}
% == BibTeX quality report for yinjinDistributedCooperativeStorage2018:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{codispotiValidationGroundTechnologies2018,
  title = {Validation of Ground Technologies for Future {{Q}}/{{V}} Band Satellite Systems: {{The QV}} \textemdash{} {{LIFT}} Project},
  shorttitle = {Validation of Ground Technologies for Future {{Q}}/{{V}} Band Satellite Systems},
  abstract = {With the aim of supporting satellite Terabit connectivity, the future generations of High Throughput Systems - HTS rely on the exploitation of Q/V band frequencies. These frequency bands offer the possibility to have larger bandwidth availability with respect to Ka-band systems, freeing the portion of the Ka frequency band that is currently allocated to feeder links and offering bigger portion of spectrum for specific services, such as the aeronautical in-flight entertainment and connectivity (IFEC) services. The design of a Ground Segment for Q/V band satellite communications presents however several technological challenges. The design of antennas, power amplifiers with high efficiency and Low Noise receivers are critical, nevertheless they are fundamental to support high data rate transmissions. Furthermore, to counteract atmospheric impairments, a system able to implement and manage a handover mechanism between gateways is also needed. A Ground Segment for Q/V band high throughput system has been conceived in the project ``Q/V band earth segment LInk for Future high Throughput space systems'' (QV-LIFT), funded by the European Commission in the framework of the Horizon 2020 program. The consolidation of crucial technologies for new generation satellite communications is addressed, with the objective to ensure space accessibility to Europe and, in particular, to foster technology readiness of European industries in space related sectors. This paper provides the description of key hardware and software developments for next generation HTS systems operating in Q/V band, based on core technologies for both ground and user segments currently under development for the QV-LIFT project. The system test architecture which will be used to validate the developed technology and functionalities is also presented, together with the overview of the project status and validation plan.},
  booktitle = {2018 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7ms},
  author = {Codispoti, G. and Parca, G. and Sanctis, M. De and Ruggieri, M. and Rossi, T. and Amendola, G. and Luini, C. R. Lorenzo and Massaro, F.},
  month = mar,
  year = {2018},
  keywords = {Satellites,Europe,Earth,internetworking,Payloads,aeronautical in-flight entertainment and connectivity services,atmospheric impairments,future Q/V band satellite systems,Ground Segment,ground technologies,handover mechanism,high data rate transmissions,High Throughput Systems,High-temperature superconductors,Ka frequency band,Ka-band systems,larger bandwidth availability,Logic gates,Low Noise receivers,millimetre wave communication,next generation HTS systems,next generation networks,next generation satellite communications,Q/V band earth segment LInk for future high throughput space systems,Q/V band frequencies,Q/V band high throughput system,Q/V band satellite communications presents,QV-LIFT project,Radio frequency,satellite ground stations,satellite Terabit connectivity,specific services,system test architecture,technology readiness,user segments},
  pages = {1-9},
  file = {/home/yuri/Zotero/storage/4S2B9GMK/Codispoti et al_2018_Validation of ground technologies for future Q-V band satellite systems.pdf;/home/yuri/Zotero/storage/8ECH27XD/8396830.html}
}
% == BibTeX quality report for codispotiValidationGroundTechnologies2018:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{nehlUpdateConceptOperation2007,
  title = {Update: {{Concept}} and {{Operation}} of the {{Performance Data Analysis}} and {{Reporting System}} ({{PDARS}})},
  shorttitle = {Update},
  abstract = {This paper contains a factual update to the concept and operation of the performance data analysis and reporting system (PDARS) paper originally presented at the SAE conference, Montreal Canada, 2003 by den Braven and Schade. Since 1999 the Federal Aviation Administration (FAA) has been operating a system for the collection, analysis, and reporting of performance-related data from the National Airspace System (NAS). This performance data analysis and reporting system (PDARS) has been installed at twenty Air Route Traffic Control Centers (ARTCCs), nineteen Terminal Radar Approach Control facilities (TRACONs), three service area offices, the FAA's Air Traffic Control System Command Center in Herndon, Virginia and FAA Headquarters offices in Washington, DC. The system generates and distributes close to 1000 reports daily for these facilities. PDARS calculates a range of performance measures, including traffic counts, travel times, travel distances, traffic flows, and in-trail separations. It turns these measurement data into information useful to FAA facilities through a\textsuperscript{2}n architecture that features (1) automatic collection and analysis of radar tracks and flight plans, (2) automatic generation and distribution of daily morning reports, (3) sharing of data and reports among facilities, and (4) support for exploratory and causal analysis. PDARS applications at FAA facilities include performance measurement, route and airspace design, noise abatement analysis, traffic flow management initiative assessment and design, training, and support for search and rescue. PDARS has also been used in a range of FAA and NASA studies. Examples are the measurement of actual benefits of the Dallas/Fort Worth (DFW) Metroplex airspace, an analysis of the Los Angeles Arrival Enhancement Procedure (AEP), an analysis of the Phoenix Dryheat departure procedure, measurement of navigation accuracy of aircraft using area navigation (RNAV) en route, a study on the detection and analysis of in-close approach changes, an evaluation of the benefits of domestic reduced vertical separation minimum implementation, and a baseline study for the airspace flow program.},
  booktitle = {2007 {{IEEE Aerospace Conference}}},
  doi = {10/dch7dv},
  author = {Nehl, R. and Schade, J.},
  month = mar,
  year = {2007},
  keywords = {aerospace computing,data analysis,Area measurement,Data analysis,Information analysis,air traffic control,Air traffic control,Aircraft navigation,airspace design,Dallas/Fort Worth Metroplex airspace,domestic reduced vertical separation minimum,FAA,flight plans,Fluid flow measurement,in-trail separations,Los Angeles Arrival Enhancement Procedure,Management training,navigation accuracy,noise abatement analysis,PDARS,Performance analysis,performance data analysis and reporting system,performance-related data,Phoenix Dryheat departure procedure,Radar tracking,radar tracks,traffic flow management initiative assessment,traffic flows,travel distances,travel times},
  pages = {1-16},
  file = {/home/yuri/Zotero/storage/HDCZYBHD/Nehl_Schade_2007_Update.pdf;/home/yuri/Zotero/storage/8BG6SIAG/4161691.html}
}
% == BibTeX quality report for nehlUpdateConceptOperation2007:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{ivancicSecureAutonomousIntelligent2013,
  title = {Secure, {{Autonomous}}, {{Intelligent Controller}} for {{Integrating Distributed Emergency Response Satellite Operations}}},
  abstract = {This report describes a Secure, Autonomous, and Intelligent Controller for Integrating Distributed Emergency Response Satellite Operations. It includes a description of current improvements to existing Virtual Mission Operations Center technology being used by US Department of Defense and originally developed under NASA funding. The report also highlights a technology demonstration performed in partnership with the United States Geological Service for Earth Resources Observation and Science using DigitalGlobe\textregistered{} satellites to obtain space-based sensor data.},
  booktitle = {2013 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mr},
  author = {Ivancic, W. D. and Paulsen, P. E. and Miller, E. M. and Sage, S. P.},
  month = mar,
  year = {2013},
  keywords = {artificial satellites,Satellites,satellite communication,Internet,Schedules,aerospace control,Monitoring,NASA,security of data,geophysical techniques,remote sensing,geophysics computing,intelligent control,autonomous controller,disasters,distributed emergency response satellite operations,earth resources observation,intelligent controller,secure controller,space based sensor data,United States geological service,US Department of Defense,virtual mission operations center technology},
  pages = {1-12},
  file = {/home/yuri/Zotero/storage/KQILA7II/Ivancic et al_2013_Secure, Autonomous, Intelligent Controller for Integrating Distributed.pdf;/home/yuri/Zotero/storage/FFGHRNFQ/6496888.html}
}
% == BibTeX quality report for ivancicSecureAutonomousIntelligent2013:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{atkinsAutonomousSatelliteFormation2002,
  title = {Autonomous Satellite Formation Assembly and Reconfiguration with Gravity Fields},
  volume = {2},
  abstract = {Spacecraft formation flight may increase data coverage area and accuracy for a myriad of space-based experiments. To prevent ground operations support from scaling with number of satellites, we propose a control architecture that describes a formation as a virtual body, such that the operator controls the group as if it were a single entity. We overview the components of a satellite formation flying architecture then outline a constrained multi-agent planning approach to decompose the specified formation geometry into an optimized set of synchronized satellite waypoint sequences. To illustrate our approach, we describe a two-satellite planar Earth-orbiting formation for far-field interferometry and show results from path optimization for circular and elliptical orbits.},
  booktitle = {Proceedings, {{IEEE Aerospace Conference}}},
  doi = {10/fwrzd2},
  author = {Atkins, E. and Pennecot, Y.},
  month = mar,
  year = {2002},
  keywords = {artificial satellites,Satellite broadcasting,Space vehicles,path planning,Orbits,ground support systems,aerospace control,Trajectory,Assembly,Astronomy,Atmospheric waves,circular orbits,constrained multi-agent planning approach,control architecture,data coverage area,elliptical orbits,far-field interferometry,Fuels,Geometry,Gravity,gravity fields,ground operations support,multi-agent systems,path optimization,radiowave interferometry,satellite formation flying architecture,space-based experiments,spacecraft formation flight,synchronized satellite waypoint sequences,two-satellite planar Earth-orbiting formation,virtual body},
  pages = {2-2},
  file = {/home/yuri/Zotero/storage/J3H6KDE8/Atkins_Pennecot_2002_Autonomous satellite formation assembly and reconfiguration with gravity fields.pdf;/home/yuri/Zotero/storage/L4EY4Y9F/1035631.html}
}
% == BibTeX quality report for atkinsAutonomousSatelliteFormation2002:
% Missing required field 'publisher'

@article{zhaoweiBoardSoftwareArchitecture2009,
  title = {The On-board Software Architecture and Distributed Testbed of Micro-satellite Clusters},
  volume = {81},
  issn = {0002-2667},
  number = {3},
  journal = {Aircraft Engineering and Aerospace Technology},
  doi = {10/d8xtmd},
  author = {Zhaowei, Sun and Xiande, Wu and Hui, Li},
  month = may,
  year = {2009},
  keywords = {Artificial satellites,Cluster analysis,Systems software,Test equipment},
  pages = {234-238},
  file = {/home/yuri/Zotero/storage/24TLBZHZ/Zhaowei et al_2009_The onâboard software architecture and distributed testbed of microâsatellite.pdf;/home/yuri/Zotero/storage/EI4WL4H4/00022660910954745.html}
}

@inproceedings{ibrahimIntegratedTelemetryAnalysis2018,
  title = {Integrated Telemetry Analysis Using Human Expert Knowledge and the Logical Analysis of Data},
  abstract = {Telemetry data received from satellites during in-orbit operation are typically analyzed by experts to identify faults that might lead to potential subsystem failures. A typical method is to apply limit-checking procedure to locate off-nominal features which do not fall within the expected normal range of values. Human-experts' analysis is usually limited. It focuses on analyzing selected group of features that would interpret a specific situation that might be taking place onboard the satellite. When the size of telemetry features and number of observations are in the order of hundreds or thousands, a full expert-based analysis is almost impossible to achieve. In this paper, expert-based analysis is leveraged by the automation of the telemetry analysis process based on a machine learning technique called the Logical Analysis of Data (LAD). LAD is a pattern recognition and classification approach that combines ideas and concepts from optimization, combinatorics and Boolean functions. One of the main advantages of LAD is its explanatory power, which offers a classification and an interpretation of the root causes of the events under study. Patterns generated via LAD are easily understood by experts as they are constructed from the features within the set of observations. Consequently, LAD is used in numerous practical applications. We are applying LAD in the process of fault diagnosis and prognosis in satellites by combining the domain expert's knowledge and the knowledge extracted by LAD. The procedure begins by performing a fault tree analysis (FTA) and a list of corrective actions, by domain human experts. This analysis is usually limited by the expert's knowledge to represent known faulty states of the system. However, operation data collected over a period of time can introduce meaningful hidden knowledge about faulty states that were not represented in the original FTA. We apply LAD to find this hidden knowledge in data collected through simulation, testing and in-flight operation. The results reflect significant interpretation power and an effective leverage of the obtained knowledge by the integrated telemetry analysis tool when compared to traditional limit-checking method.},
  booktitle = {2018 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mq},
  author = {Ibrahim, M. M. and Ahmed, A. M. and Akah, H. E. and Mohamed, M. I. and Yacout, S.},
  month = mar,
  year = {2018},
  keywords = {Satellites,aerospace computing,learning (artificial intelligence),telemetry,telemetry data,data analysis,Telemetry,failure analysis,data mining,optimization,pattern recognition,condition monitoring,fault diagnosis,Machine learning,pattern classification,satellites,Logic gates,Boolean functions,combinatorics,domain expert,domain human experts,expert-based analysis,fault tree analysis,fault trees,Fault trees,human expert knowledge,in-flight operation,integrated telemetry analysis tool,knowledge acquisition,LAD,limit-checking procedure,logical analysis of data,machine learning technique,off-nominal features,operation data,Software reliability,subsystem failures,telemetry analysis process,telemetry features},
  pages = {1-14},
  file = {/home/yuri/Zotero/storage/MSMV43Z7/Ibrahim et al_2018_Integrated telemetry analysis using human expert knowledge and the logical.pdf;/home/yuri/Zotero/storage/FVSPYZLW/8396740.html}
}
% == BibTeX quality report for ibrahimIntegratedTelemetryAnalysis2018:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{witkowskiCassiniRiskManagement2003,
  title = {Cassini Risk Management during Mission Operations and Data Analysis - Application Amp; Lessons Learned},
  volume = {2},
  booktitle = {2003 {{IEEE Aerospace Conference Proceedings}} ({{Cat}}. {{No}}.{{03TH8652}})},
  doi = {10/ddc4v9},
  author = {Witkowski, M. M.},
  month = mar,
  year = {2003},
  keywords = {Space vehicles,NASA,Saturn,Moon,Probes,Relays,Data analysis,Propulsion,Laboratories,Risk management},
  pages = {2\_799-2\_806},
  file = {/home/yuri/Zotero/storage/I9MT5U3J/Witkowski_2003_Cassini risk management during mission operations and data analysis -.pdf;/home/yuri/Zotero/storage/4AS8WMF5/1235491.html}
}
% == BibTeX quality report for witkowskiCassiniRiskManagement2003:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{al-zaidyNovelApproachSatellite2017,
  title = {Novel Approach of Satellite Health Monitoring, Diagnosis and Prediction via {{PLS}} Batch Modelling},
  abstract = {Satellite health analysis executed to guarantee that service systems would keep up satellite's functionality all over the mission time. Monitoring, early anomaly detection and diagnosis are the main concerns of space operations. Service systems analysis specialist deals with massive amount of telemetry data that downloaded from satellite. The ground segment has multiple means to perform the task of telemetry monitoring and analysis. Multivariate analysis introduced as a powerful method in this field. In this paper, a survey on multivariate techniques (PCA-PLS) explained. Then the technique of batch modeling applied for first time to the telemetry of the thermal control system of Nickel-Hydrogen batteries in the power supply system of real satellite during test of batteries. Batch modeling is a partial least squares projections to latent structure (PLS) technique. This technique based on developing a model of evolution for normal batches; the model used for monitoring the new batches, and classifies them as normal or faulty ones. In addition, control charts such as CUSUM and Shewhart used for monitoring and early detection of anomalies. The discussion shows that using these techniques could avoid the failures that had happened if it used. In addition, contribution and variable patch plots used in diagnosis and physical interpretation.},
  booktitle = {2017 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mp},
  author = {{Al-Zaidy}, A. M. and Hussein, W. M. and {El-Sherif}, I.},
  month = mar,
  year = {2017},
  keywords = {artificial satellites,Satellites,Space vehicles,Data models,space operations,telemetry data,Monitoring,Telemetry,space telemetry,statistical analysis,least squares approximations,principal component analysis,Principal component analysis,Payloads,condition monitoring,fault diagnosis,space vehicle power plants,anomaly diagnosis,batch modeling,batch processing (industrial),control charts,CUSUM,early anomaly detection,ground segment,latent structure technique,multiple means,multivariate techniques,Nickel-Hydrogen batteries,normal batches,partial least square projections,PCA-PLS,PLS batch modelling,power supply system,process monitoring,regression analysis,satellite health analysis,satellite health monitoring,service system analysis specialist,statistical process control,telemetry monitoring,thermal control system},
  pages = {1-11},
  file = {/home/yuri/Zotero/storage/L432U64A/Al-Zaidy et al_2017_Novel approach of satellite health monitoring, diagnosis and prediction via PLS.pdf;/home/yuri/Zotero/storage/9S36HR2M/7943558.html}
}
% == BibTeX quality report for al-zaidyNovelApproachSatellite2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{jenkinsProcessingManagingKepler2016,
  title = {Processing and Managing the {{Kepler}} Mission's Treasure Trove of Stellar and Exoplanet Data},
  abstract = {The Kepler telescope launched into orbit in March 2009, initiating NASA's first mission to discover Earth-size planets orbiting Sun-like stars. Kepler simultaneously collected data for 160,000 target stars at a time over its four-year mission, identifying over 4700 planet candidates, 2300 confirmed or validated planets, and over 2100 eclipsing binaries. While Kepler was designed to discover exoplanets, the long term, ultra-high photometric precision measurements it achieved made it a premier observational facility for stellar astrophysics, especially in the field of asteroseismology, and for variable stars, such as RR Lyraes. The Kepler Science Operations Center (SOC) was developed at NASA Ames Research Center to process the data acquired by Kepler from pixel-level calibrations all the way to identifying transiting planet signatures and subjecting them to a suite of diagnostic tests to establish or break confidence in their planetary nature. Detecting small, rocky planets transiting Sun-like stars presents a variety of daunting challenges, from achieving an unprecedented photometric precision of 20 parts per million (ppm) on 6.5-hour timescales, supporting the science operations, management, processing, and repeated reprocessing of the accumulating data stream. This paper describes how the design of the SOC meets these varied challenges, discusses the architecture of the SOC and how the SOC pipeline is operated and is run on the NAS Pleiades supercomputer, and summarizes the most important pipeline features addressing the multiple computational, image and signal processing challenges posed by Kepler.},
  booktitle = {2016 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  doi = {10/gfz7mn},
  author = {Jenkins, J. M.},
  month = dec,
  year = {2016},
  keywords = {Extrasolar planets,Pipelines,Instruments,Astrophysics,astronomy,Extraterrestrial measurements,software architecture,AD 2009 03,astronomy: extrasolar planets,astrophysics,celestial mechanics,data processing,Earth-size planets orbiting Sun-like stars,exoplanet data,extrasolar planetary motion,high performance computing,Kepler missions treasure,Kepler Science Operations Center,NASA Ames Research Center,Oscillators,pixel-level calibrations,planetary nature,stellar astrophysics,ultra-high photometric precision measurements},
  pages = {3158-3167},
  file = {/home/yuri/Zotero/storage/ERN4FRDK/Jenkins_2016_Processing and managing the Kepler mission's treasure trove of stellar and.pdf;/home/yuri/Zotero/storage/KWCGUCYL/7840971.html}
}
% == BibTeX quality report for jenkinsProcessingManagingKepler2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{dickinsonCYGNSSCommandData2014,
  title = {{{CYGNSS}} Command and Data Subsystem and Electrical Power Subsystem Phase {{A}} and {{B}} Developments},
  abstract = {The Cyclone Global Navigation Satellite System (CYGNSS), which was selected as the Earth Venture-2 investigation by NASA's Earth Science System Pathfinder (ESSP) Program, measures the ocean surface wind field with unprecedented temporal resolution and spatial coverage, under all precipitating conditions, and over the full dynamic range of wind speeds experienced in a tropical cyclone (TC). The CYGNSS flight segment consists of 8 microsatellite-class observatories, which represent SwRI's first spacecraft bus design, installed on a Deployment Module for launch. The microsatellites (microsats) are identical in design but provide their own individual contribution to the CYGNSS science data set. Within the first year of the CYGNSS program (Phase A and B), the design has been analyzed, vetted, and reviewed which culminated in a succession of updates and design improvements. This paper will discuss relevant updates to the electrical systems of the microsat, specifically the command and data subsystem (CDS) and the electrical power subsystem (EPS). For the CDS, more detailed analysis of the communication link and maturation of the transceiver module design are presented. The link budget was reviewed by the team and verified via simulation. The transceiver module has moved to a dedicated box consisting of a radio frequency (RF) board and a digital signal processing (DSP) board. For the EPS, a detailed power analysis of the mission has led to an update in the solar array configuration. The details of the power analysis, which is performed in STK and Matlab, are presented.},
  booktitle = {2014 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mm},
  author = {Dickinson, J. R. and Alvarez, J. L. and McDaniel, L. T. and Pruitt, J. R. and Walls, B. J. and Zajicek, K. P.},
  month = mar,
  year = {2014},
  keywords = {artificial satellites,space vehicle electronics,NASA,Telemetry,EPS,Downlink,Analytical models,transceivers,electrical power subsystem,space vehicle power plants,satellite navigation,SwRI,Radio frequency,Antennas,CDS,command and data subsystem,Cyclone Global Navigation Satellite System,CYGNSS flight segment,CYGNSS program,CYGNSS science data set,deployment module,detailed power analysis,digital signal processing board,DSP board,Earth Science System Pathfinder Program,Earth Venture-2 investigation,electrical systems,ESSP Program,link budget,Matlab,microsatellite-class observatories,microsats,Nonvolatile memory,ocean surface wind field,precipitating conditions,radio frequency board,RF board,solar array configuration,spacecraft bus design,STK,transceiver module design,Transceivers,tropical cyclone,wind speeds},
  pages = {1-10},
  file = {/home/yuri/Zotero/storage/77BM6ZWD/Dickinson et al_2014_CYGNSS command and data subsystem and electrical power subsystem phase A and B.pdf;/home/yuri/Zotero/storage/DMVS6ACU/6836336.html}
}
% == BibTeX quality report for dickinsonCYGNSSCommandData2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{jasperDataProductionFuture2017,
  title = {Data Production on Past and Future {{NASA}} Missions},
  abstract = {Data return is a metric that is commonly publicized for all space science missions. In the early days of the Space Program, this figure was small, and could be described in bits or maybe even megabits. But now, missions are capable of returning data volumes two or three orders of magnitude larger. For example, Voyager 1 and 2 combined produced a little over 5 Terabits of data in 39 years of operation. In contrast, the Cassini mission, launched two decades after Voyager, produced about one and a half times those data volumes in half the time. NISAR, an Earth Science Mission currently in implementation, plans to produce over 28 Petabits of raw data in just 3 years. This means that NISAR will produce about as many data in 30 days as the combined data production of nearly all planetary missions to date. These increases in capability are a result of technology enhancements in two main areas: telecommunications architecture (both space and ground segments) and data storage technology. This paper describes the progression of these two technologies over the course of more than three decades of space missions and provides additional insight into the design of the end-to-end NISAR Data System Architecture. Trends in the data are briefly explored and compared to Moore's Law which provides only a qualitative model for memory growth but not for data production. In summary, early missions are found to be driven by unrefined processes while later missions, having utilized earlier lessons learned, focus more on improvements to flight and ground capabilities. Data return seems to fall into three categories. First, deep space missions are driven by the large distances that limit data return to the Earth. Next, the orbiter infrastructure around Mars helps these missions generate more data than other deep space spacecraft. Finally, near-Earth missions have the greatest capabilities for the studied metrics due to their close proximity to Earth and the ground network availability.},
  booktitle = {2017 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mk},
  author = {Jasper, L. E. Z. and Xaypraseuth, P.},
  month = mar,
  year = {2017},
  keywords = {Space vehicles,space communication links,Earth,Space missions,Image coding,Production,Telecommunications,synthetic aperture radar,Cassini mission,data production,data return,data storage technology,data volumes,deep space missions,earth science mission,ground network availability,Jupiter,magnitude larger,NASA missions,space program,space science missions},
  pages = {1-11},
  file = {/home/yuri/Zotero/storage/GRIKRPMG/Jasper_Xaypraseuth_2017_Data production on past and future NASA missions.pdf;/home/yuri/Zotero/storage/Z3H2WVDI/7943918.html}
}
% == BibTeX quality report for jasperDataProductionFuture2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{cookeKeplerEndtoendData2012,
  title = {The {{Kepler}} End-to-End Data Pipeline: {{From}} Photons to Far Away Worlds},
  shorttitle = {The {{Kepler}} End-to-End Data Pipeline},
  abstract = {Launched by NASA on 6 March 2009, the Kepler Mission has been observing more than 100,000 targets in a single patch of sky between the constellations Cygnus and Lyra almost continuously for the last two years looking for planetary systems using the transit method. As of October 2011, the Kepler spacecraft has collected and returned to Earth just over 290 GB of data, identifying 1235 planet candidates with 25 of these candidates confirmed as planets via ground observation. Extracting the telltale signature of a planetary system from stellar photometry where valid signal transients can be as small as a 40 ppm is a difficult and exacting task. The end-to-end process of determining planetary candidates from noisy, raw photometric measurements is discussed. The Kepler mission is described in overview and the Kepler technique for discovering exoplanets is discussed. The design and implementation of the Kepler spacecraft, tracing the data path from photons entering the telescope aperture through raw observation data transmitted to the ground operations team is described. The technical challenges of operating a large aperture photometer with an unprecedented 95 million pixel detector are addressed as well as the onboard technique for processing and reducing the large volume of data produced by the Kepler photometer. The technique and challenge of day-to-day mission operations that result in a very high percentage of time on target is discussed. This includes the day to day process for monitoring and managing the health of the spacecraft, the annual process for maintaining sun on the solar arrays while still keeping the telescope pointed at the fixed science target, the process for safely but rapidly returning to science operations after a spacecraft initiated safing event and the long term anomaly resolution process. The ground data processing pipeline, from the point that science data is received on the ground to the presentation of preliminary planetary candidates and supporting data to the science team for further evaluation is discussed. Ground management, control, exchange and storage of Kepler's large and growing data set is discussed as well as the process and techniques for removing noise sources and applying calibrations to intermediate data products.},
  booktitle = {2012 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mj},
  author = {Cooke, B. and Thompson, R. and Standley, S.},
  month = mar,
  year = {2012},
  keywords = {Space vehicles,Earth,space vehicles,NASA,Telemetry,Arrays,calibration,Apertures,calibrations,Charge coupled devices,Cygnus constellation,data path,day-to-day mission operations,end-to-end process,exoplanets,extrasolar planets,far away worlds,fixed science target,ground data processing pipeline,ground management,ground observation,ground operation team,intermediate data products,Kepler end-to-end data pipeline,Kepler Mission,Kepler photometer,Kepler spacecraft,Kepler technique,large aperture photometer,long term anomaly resolution process,Lyra constellation,noise sources,noisy raw photometric measurements,planetary system,planetary systems,preliminary planetary candidates,raw observation data,safing event,science data,science operations,signal transients,solar arrays,stellar photometry,telescope aperture,telltale signature,transit method},
  pages = {1-9},
  file = {/home/yuri/Zotero/storage/5PBZ8M5T/Cooke et al_2012_The Kepler end-to-end data pipeline.pdf;/home/yuri/Zotero/storage/39F6QRA4/6187170.html}
}
% == BibTeX quality report for cookeKeplerEndtoendData2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{highsmithSpaceLaunchSystem2015,
  title = {Space {{Launch System}} ({{SLS}}) Data Acquisition and Sensor System for Human Space Flight},
  abstract = {The Boeing Company is designing, developing and manufacturing the Core Stage and Avionics for NASA's heavy lift rocket, the Space Launch System (SLS), which will support missions beyond low earth orbit for the first time since the Apollo days. On the SLS vehicle many sensors are selected and used to provide critical information for testing, ground operations, and vehicle flight environments. Appropriate data acquisition systems that can interface and process the selected sensors is important to the success of the program. In this program, multiple data acquisition systems are used to interface with sensors at various development stages of the SLS vehicle, and at different sensor criticality levels. These phased configurations of data acquisition classes and complexity range from gathering sensor data during system level testing at Stennis Space Center (SSC), to transmitting flight information after liftoff at the Kennedy Space Center. One challenge that engineers are faced with pertains to how the data acquisition system is defined prior to understanding the quantity and type of sensors required for the application. This paper focuses mainly on the SLS flight system and provides a general overview of the system architecture. The paper also addresses the strategy used to define the data acquisition system, along with the methodology for selecting data acquisition cards that fit with the application.},
  booktitle = {2015 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mh},
  author = {Highsmith, H. and Brock, J. E. and Stephens, D. E.},
  month = mar,
  year = {2015},
  keywords = {Monitoring,system architecture,aerospace instrumentation,data acquisition,Downlink,Propulsion,sensors,Engines,avionics,aerospace testing,core stage,data acquisition cards,human space flight,Liquids,low earth orbit,NASA heavy lift rocket,sensor criticality levels,sensor system,Signal resolution,SLS data acquisition system,SLS flight system,SLS vehicle,space launch system,system level testing,vehicle flight environments},
  pages = {1-9},
  file = {/home/yuri/Zotero/storage/VSPYI7RY/Highsmith et al_2015_Space Launch System (SLS) data acquisition and sensor system for human space.pdf;/home/yuri/Zotero/storage/MKI4ID9J/7119024.html}
}
% == BibTeX quality report for highsmithSpaceLaunchSystem2015:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{zetochaBackroomMissionOperations2002,
  title = {A Backroom Mission Operations Center for {{TechSat}} 21},
  volume = {7},
  abstract = {The TechSat 21 satellite program is an Air Force Research Laboratory (AFRL) technology initiative which has an objective to demonstrate and validate microsatellite cluster system concepts and enabling technologies. The primary experimental objectives are to demonstrate formation flying algorithms and technologies for clustered satellites, and to demonstrate autonomous cluster and spacecraft operations. TechSat 21 consists of three satellites which will fly in various configurations with variable separation distances. Command and control of a cluster of satellites with multiple heterogeneous experimental objectives poses several challenges from a ground perspective. To assist in operating TechSat 21, AFRL is developing a backroom Mission Operations Center (MOC) which will be capable of performing, among other tasks: planning and scheduling; command generation; state-of-health (SOH) monitoring; telemetry playbacks; fault detection, isolation, and resolution (FDIR); data storage; and payload data analysis. The objective of this paper is to describe the MOC architecture, highlight the key components, and outline its planned operational use.},
  booktitle = {Proceedings, {{IEEE Aerospace Conference}}},
  doi = {10/cxkx88},
  author = {Zetocha, P.},
  month = mar,
  year = {2002},
  keywords = {Satellites,Space vehicles,planning,scheduling,ground support systems,Monitoring,Telemetry,satellite telemetry,Clustering algorithms,data storage,Space technology,ground support equipment,Fault detection,fault diagnosis,fault detection,Laboratories,satellite ground stations,Air Force Research Laboratory technology initiative,autonomous cluster operations,autonomous spacecraft operations,backroom mission operations center,clustered satellites,command and control,Command and control systems,command generation,fault isolation,fault resolution,formation flying algorithms,ground perspective,Memory,microsatellite cluster system,military equipment,MOC architecture,MOC operational use,multiple heterogeneous experimental objectives,payload data analysis,satellite configurations,satellite separation distances,state-of-health monitoring,strategic planning,TechSat 21 satellite program,telemetry playbacks},
  pages = {7-7},
  file = {/home/yuri/Zotero/storage/BRLF4KVB/Zetocha_2002_A backroom mission operations center for TechSat 21.pdf;/home/yuri/Zotero/storage/9Z8QVAE3/1035295.html}
}
% == BibTeX quality report for zetochaBackroomMissionOperations2002:
% Missing required field 'publisher'

@inproceedings{richMulticenterSpaceData2016,
  title = {A Multi-Center Space Data System Prototype Based on {{CCSDS}} Standards},
  abstract = {Deep space missions beyond earth orbit will require new methods of data communications in order to compensate for increasing Radio Frequency (RF) propagation delay. The Consultative Committee for Space Data Systems (CCSDS) standard protocols Spacecraft Monitor \& Control (SM\&C), Asynchronous Message Service (AMS), and Delay/Disruption Tolerant Networking (DTN) provide such a method. However, the maturity level of this protocol stack is insufficient for mission inclusion at this time. This Space Data System prototype is intended to provide experience which will raise the Technical Readiness Level (TRL) of this protocol set. In order to reduce costs, future missions can take advantage of these standard protocols, which will result in increased interoperability between control centers. This prototype demonstrates these capabilities by implementing a realistic space data system in which telemetry is published to control center applications at the Jet Propulsion Lab (JPL), the Marshall Space Flight Center (MSFC), and the Johnson Space Center (JSC). Reverse publishing paths for commanding from each control center are also implemented. The target vehicle consists of realistic flight computer hardware running Core Flight Software (CFS) in the integrated Power, Avionics, and Power (iPAS) Pathfinder Lab at JSC. This prototype demonstrates a potential upgrade path for future Deep Space Network (DSN) modification, in which the automatic error recovery and communication gap compensation capabilities of DTN would be exploited. In addition, SM\&C provides architectural flexibility by allowing new service providers and consumers to be added efficiently anywhere in the network using the common interface provided by SM\&C's Message Abstraction Layer (MAL). In FY 2015, this space data system was enhanced by adding telerobotic operations capability provided by the Robot API Delegate (RAPID) family of protocols developed at NASA. RAPID is one of several candidates for consideration and inclusion in a new international standard being developed by the CCSDS Telerobotic Operations Working Group. Software gateways for the purpose of interfacing RAPID messages with the existing SM\&C based infrastructure were developed. Telerobotic monitor, control, and bridge applications were written in the RAPID framework, which were then tailored to the NAO telerobotic test article hardware, a product of Aldebaran Robotics.},
  booktitle = {2016 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mg},
  author = {Rich, T. M.},
  month = mar,
  year = {2016},
  keywords = {Space vehicles,delay tolerant networks,protocols,Protocols,Software,space communication links,Telemetry,interoperability,DTN,Aerospace electronics,JPL,application program interfaces,deep space network,avionics,Earth orbit,Aldebaran robotics,AMS,asynchronous message service,automatic error recovery,CCSDS telerobotic operations working group,CFS,communication gap compensation capabilities,consultative committee for space data systems standard protocol spacecraft monitor \& control,core flight software,data communication,Data systems,delay-disruption tolerant networking,DSN,integrated power; avionics; and power pathfinder,iPAS pathfinder,jet propulsion lab,Johnson space center,JSC,MAL,marshall space flight center,MSFC,multicenter space data system prototype,NAO telerobotic test,Prototypes,radiofrequency propagation delay,RAPID,RF propagation delay,robot API delegate,SM\&c,SM\&C message abstraction layer,technical readiness level,telerobotics,TRL},
  pages = {1-6},
  file = {/home/yuri/Zotero/storage/KK52LC9U/Rich_2016_A multi-center space data system prototype based on CCSDS standards.pdf;/home/yuri/Zotero/storage/AFE9925U/7500798.html}
}
% == BibTeX quality report for richMulticenterSpaceData2016:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{iversonSystemHealthMonitoring2008,
  title = {System {{Health Monitoring}} for {{Space Mission Operations}}},
  abstract = {Many spacecraft provide an abundance of system status telemetry that is monitored in real time by ground personnel and archived to allow for further analysis. In the flight control room, controllers typically monitor these values using text or graphical displays that incorporate individual parameter limit checking or simple trend analysis. Recent developments in data mining techniques for anomaly detection make it possible to use the wealth of archived system data to produce more sophisticated system health monitoring applications. These "data driven" applications are capable of characterizing and monitoring interactions between multiple parameters and can complement existing practice to provide valuable decision support for mission controllers. Data driven software tools have been successfully applied to mission operations for both the Space Shuttle and the International Space Station. These tools have been applied to engineering analysis of spacecraft data to detect unusual events in the data, and to real-time system health monitoring in the flight control room. Augmenting traditional mission control software with advanced monitoring tools can provide controllers with greater insight into the health and performance of the space systems under their watch. Adding heuristic rule based methods that encode system knowledge obtained from seasoned mission controllers can also be helpful to less experienced personnel. We will describe how such techniques have been applied to NASA mission control operations and discuss plans for future mission control system health monitoring software systems.},
  booktitle = {2008 {{IEEE Aerospace Conference}}},
  doi = {10/dzwzb6},
  author = {Iverson, D. L.},
  month = mar,
  year = {2008},
  keywords = {Space vehicles,aerospace computing,aerospace engineering,space vehicles,aerospace control,Telemetry,Space missions,data mining,Control systems,condition monitoring,spacecraft,system health monitoring,space mission operations,Application software,Real time systems,Personnel,Aerospace control,Condition monitoring,data driven software tools,flight control room,International Space Station,mission control operations,software tools,Software tools,Space Shuttle,system status telemetry},
  pages = {1-8},
  file = {/home/yuri/Zotero/storage/FQLK5D87/Iverson_2008_System Health Monitoring for Space Mission Operations.pdf;/home/yuri/Zotero/storage/8ZK8XXAA/4526646.html}
}
% == BibTeX quality report for iversonSystemHealthMonitoring2008:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{eklundRealtimeOnlineHealth2014,
  title = {Real-Time Online Health Analytics for Interplanetary Space Missions},
  abstract = {Monitoring the health and well-being of human beings during any manned spaceflight is a critical aspect of the space mission. To date this has been done almost exclusively with telemetry to ground stations and monitoring of the physiological data by physicians. With lunar and Mars missions the transmission latency become problematic, and in additional to longer latency, Mars missions would also be subject to more restricted bandwidth availability and to blackout periods due to the normal rotation of Mars when the lander would be on the far side of the planet relative to earth for roughly half of each Mars solar day. While these blackout period could be reduced or removed with relay satellite in Martian orbit, this may not be practical for early missions to Mars. As such, Autonomous Medical Care has been identified by NASA as one of the top priority technologies to be developed for Mars missions, with seven Risk Categories with that area. Two of these are Monitoring and Prevention; and Medical Informatics, Technologies and Support Systems. In this paper, the Artemis platform, which was developed originally for real-time diagnostics in neonatal intensive care is presented as a solution to these areas of Autonomous Medical Care for Mars missions. This platform has the capability, in addition to real-time diagnostics, to provide supervisory medical monitoring, modularity in deployment of specialized diagnostic algorithms and also the offline (earth-based) development and deployment of new algorithms based on additional mission needs. The platform and these capabilities are described, along with examples of its use. Furthermore, its requirements in terms of resources within the inherently limited resources of a long space mission are also presented and analysed, and a proposed physical architecture for both the space and ground control components are proposed.},
  booktitle = {2014 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mf},
  author = {Eklund, J. M. and McGregor, C.},
  month = mar,
  year = {2014},
  keywords = {Space vehicles,Mars,Real-time systems,space vehicles,telemetry,Monitoring,Space missions,Aerospace electronics,artemis platform,autonomous medical care,Biomedical monitoring,biomedical telemetry,ground control components,ground stations,health monitoring,human beings,interplanetary matter,interplanetary space missions,long space mission,lunar missions,Mars missions,Mars solar day,Martian orbit,medical diagnostic computing,medical monitoring,neonatal intensive care,patient care,patient diagnosis,patient monitoring,physical architecture,physiological data,real-time online health analytics,real-time patient diagnosis,relay satellite,space control components,spaceflight},
  pages = {1-10},
  file = {/home/yuri/Zotero/storage/94TDZFFI/Eklund_McGregor_2014_Real-time online health analytics for interplanetary space missions.pdf;/home/yuri/Zotero/storage/VVKWDUUW/6836458.html}
}
% == BibTeX quality report for eklundRealtimeOnlineHealth2014:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{rabenauApproachChallengesScience2012,
  title = {Approach and Challenges to Science and Mission Planning for the {{European}} Orbiter {{Mars Express}}},
  abstract = {The paper focuses on the end-to-end science operations for the first and only European Mars mission to-date, describing the approaches to science and mission planning and the challenges imposed by the operations constraints. It includes the activities of the instrument and science planning teams to plan and process the collected data. The Mars Express spacecraft has been in orbit around Mars since late 2003. An elliptical, near polar orbit has been chosen as the best trade-off between low-cost launch with little remaining fuel, science operations close to the targets on the Red Planet, and long periods required for transmitting the science data with an affordable communications system. The chosen orbit precesses around Mars, allowing the mapping of the complete surface of Mars at low altitude and good resolution. A high-resolution stereo camera, a mineralogical mapping spectrometer, two atmospheric/surface thermal mapping spectrometers, an energetic neutral atoms analyser, a sub-surface sounding radar and communications equipment in support of lander operations are mounted on the payload face of the spacecraft and compete for observation time, pointing preference and downlink time. The science planning facility performs the trade-off between the competing instrument science requests, planning within an envelope imposed by the power, thermal and illumination resource constraints, as well as the on-board data storage capacity and ground station availability. Different seasonal constraints apply due to the geometry between Mars and Earth, Mars and Sun, but also because of anomalies on the spacecraft that impact the power resource. Once a consolidated science plan has been generated and distributed in the form of a pointing timeline and payload operations requests, the Mars Express mission planning and flight dynamics teams validate the feasibility of the plan. The analysis includes the detailed power usage, spacecraft slew rates, reaction wheel off-loading parameters, illumination and thermal constraint checks. Based on the pointing timeline, orbital events, like eclipses and occultations, and the allocated ground station antennas from the tracking network of the European Space Agency and the Deep Space Network of NASA, the mission planning team generates a communications plan which consists of the switching of the transmitter, downlink and commanding opportunity windows. Tools have been developed within the team to ease and automate the mission planning processes. The tools include the first-ever application based on artificial intelligence technology in mission operations at the European Space Operations Centre (for computing the irregular downlink and uplink plans). Several days prior to execution, the instrument operations requests are updated with detailed and fine-tuned operations parameters. Due to the planning of the instrument operations relative to an orbit event, the execution time uplinked to the instrument is aligned to the latest orbit prediction providing very good accuracy of the orbital events. Once the instrument observations have taken place, the data stored in the on-board mass memory is transferred to the control centre according to the `dump plan', from where the instrument teams pick up the science and housekeeping data for further processing, analysis and publication of the scientific results.},
  booktitle = {2012 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7md},
  author = {Rabenau, E. and Denis, M. and Altobelli, N.},
  month = mar,
  year = {2012},
  keywords = {Space vehicles,Mars,Planning,space communication links,Earth,artificial intelligence,Orbits,space vehicles,NASA,Instruments,Sun,Downlink,Payloads,antennas,artificial intelligence technology,atmospheric thermal mapping spectrometers,cameras,communication system,communications equipment,Deep Space Network,end-to-end science operations,energetic neutral atom analyser,European Mars mission,European Orbiter Mars Express,European Space Agency,European Space Operations Centre,fine-tuned operation parameters,flight dynamics,ground station antennas,high-resolution stereo camera,housekeeping data,Mars Express mission,Mars Express spacecraft,mineralogical mapping spectrometer,mission planning process,occultations,on-board communications,on-board data storage capacity,on-board mass memory,orbit prediction method,planetary surfaces,polar orbit,reaction wheel off-loading parameters,Red Planet,science planning facility,spacecraft anomalies,spacecraft payload face,spectrometers,subsurface sounding radar,surface thermal mapping spectrometers,wheels},
  pages = {1-9},
  file = {/home/yuri/Zotero/storage/6YDDKPN4/Rabenau et al_2012_Approach and challenges to science and mission planning for the European.pdf;/home/yuri/Zotero/storage/PWM5YGRW/6187172.html}
}
% == BibTeX quality report for rabenauApproachChallengesScience2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{hennawyTelemetryRangingUsing2015,
  title = {Telemetry Ranging Using Software-Defined Radios},
  abstract = {Telemetry ranging is a technique that inserts ranging data measured by the spacecraft into the downlink telemetry stream, thereby avoiding the need to allocate downlink power for a ranging signal. This technique has many benefits depending on the mission profile, including increased data return, operational simplification, and spectrum efficiency. The present study considers a variation to the ranging technique presented in [1] in order to facilitate implementation in a software-defined radio (SDR). This implementation tracks an uplink PN range code and measures the code phase coincident with the start of downlink telemetry frames. The phase is then embedded in subsequent telemetry frames. The method is implemented in the JHU/APL Frontier Radio and leverages the PN ranging design from the NASA New Horizons communications system. Initial test results are summarized and indicate that the method is viable for space exploration.},
  booktitle = {2015 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mc},
  author = {Hennawy, J. and Adams, N. and Sanchez, E. and Srinivasan, D. and Hamkins, J. and Vilnrotter, V. and Xie, H. and Kinman, P.},
  month = mar,
  year = {2015},
  keywords = {Space vehicles,Telemetry,Downlink,space exploration,Delays,pseudonoise codes,spacecraft,radiotelemetry,Extraterrestrial measurements,Distance measurement,code phase coincident,Decision support systems,downlink telemetry stream,JHU/APL frontier radio,mission profile,NASA new horizons communications system,pseudonoise ranging,ranging signal,SDR,software radio,software-defined radio,spectrum efficiency,telemetry ranging,uplink PN range code},
  pages = {1-14},
  file = {/home/yuri/Zotero/storage/VTRZA4LH/Hennawy et al_2015_Telemetry ranging using software-defined radios.pdf;/home/yuri/Zotero/storage/8XA9JDXF/7119178.html}
}
% == BibTeX quality report for hennawyTelemetryRangingUsing2015:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{edwardsRelayCommunicationsSupport2017,
  title = {Relay Communications Support to the {{ExoMars Schiaparelli}} Lander},
  abstract = {The European Space Agency's ExoMars Trace Gas Orbiter (TGO) arrived at Mars on October 19, 2016, three days after releasing the Schiaparelli Lander on a ballistic trajectory to Meridiani Planum. During the separation event, and subsequently during Schiaparelli's Entry, Descent, and Landing (EDL), the NASA-provided Electra Ultra-High Frequency (UHF) payload onboard TGO was used to record signals from the Schiaparelli Lander for post-processing on the ground to recover both tracking of the lander's carrier signal and reconstruction of the lander's 8 kb/s telemetry. In addition, ESA's Mars Express orbiter recorded the Schiaparelli signal, with ground post-processing providing independent tracking of the lander carrier signal, and the Giant Metrewave Radio Telescope near Pune, India was configured to provide real-time detection of the lander carrier signal. While an anomaly in the latter stages of EDL led to loss of the lander, these critical event data sets, and in particular the telemetry reconstruction enabled by the TGO Electra recording, proved essential in enabling detailed diagnosis of the anomaly. While the loss of the lander during EDL precluded the planned surface relay operations, the preparations for that activity provide important lessons learned for future Mars relay support scenarios.},
  booktitle = {2017 {{IEEE Aerospace Conference}}},
  doi = {10/gfz7mb},
  author = {Edwards, C. D. and Asmar, S. and Bruvold, K. N. and Chamberlain, N. F. and Esterhuizen, S. and Gladden, R. E. and Johnston, M. D. and Kuperman, I. and Mendoza, R. and Potts, C. L. and Pugh, M. P. and Wenkert, D. and Denis, M. and Schmitz, P. and Wood, S. and Bayle, O. and Winton, A. and Montagna, M.},
  month = mar,
  year = {2017},
  keywords = {Space vehicles,space communication links,Orbits,NASA,Telemetry,Relays,Payloads,entry; descent and landing (spacecraft),Transceivers,ballistic trajectory,bit rate 8 kbit/s,critical event data sets,EDL,electra ultra-high frequency payload onboard TGO,ESA Mars Express orbiter,European Space Agency ExoMars trace gas orbiter,ExoMars Schiaparelli lander,Giant Metrewave radio telescope,ground post-processing,India,lander carrier signal detection,lander carrier signal tracking,lander telemetry reconstruction,Meridiani Planum,planned surface relay operations,Pune,relay communications,relay networks (telecommunication),Schiaparelli entry-descent and landing,signal detection,TGO Electra recording,UHF},
  pages = {1-14},
  file = {/home/yuri/Zotero/storage/4XWSL7EW/Edwards et al_2017_Relay communications support to the ExoMars Schiaparelli lander.pdf;/home/yuri/Zotero/storage/RX4LRAXD/7943620.html}
}
% == BibTeX quality report for edwardsRelayCommunicationsSupport2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{kampgenInteractingStatisticalLinked2015,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Interacting with {{Statistical Linked Data}} via {{OLAP Operations}}},
  isbn = {978-3-662-46641-4},
  abstract = {Online Analytical Processing (OLAP) promises an interface to analyse Linked Data containing statistics going beyond other interaction paradigms such as follow-your-nose browsers, faceted-search interfaces and query builders. Transforming statistical Linked Data into a star schema to populate a relational database and applying a common OLAP engine do not allow to optimise OLAP queries on RDF or to directly propagate changes of Linked Data sources to clients. Therefore, as a new way to interact with statistics published as Linked Data, we investigate the problem of executing OLAP queries via SPARQL on an RDF store. First, we define projection, slice, dice and roll-up operations on single data cubes published as Linked Data reusing the RDF Data Cube vocabulary and show how a nested set of operations lead to an OLAP query. Second, we show how to transform an OLAP query to a SPARQL query which generates all required tuples from the data cube. In a small experiment, we show the applicability of our OLAP-to-SPARQL mapping in answering a business question in the financial domain.},
  language = {en},
  booktitle = {The {{Semantic Web}}: {{ESWC}} 2012 {{Satellite Events}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {K{\"a}mpgen, Benedikt and O'Riain, Se{\'a}n and Harth, Andreas},
  editor = {Simperl, Elena and Norton, Barry and Mladenic, Dunja and Della Valle, Emanuele and Fundulaki, Irini and Passant, Alexandre and Troncy, Rapha{\"e}l},
  year = {2015},
  keywords = {OLAP,\#nosource,Linked data,Query,SPARQL,Statistics,XBRL},
  pages = {87-101}
}
% == BibTeX quality report for kampgenInteractingStatisticalLinked2015:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{jiMiningFrequentClosed2006,
  series = {{{VLDB}} '06},
  title = {Mining {{Frequent Closed Cubes}} in {{3D Datasets}}},
  abstract = {In this paper, we introduce the concept of frequent closed cube (FCC), which generalizes the notion of 2D frequent closed pattern to 3D context. We propose two novel algorithms to mine FCCs from 3D datasets. The first scheme is a Representative Slice Mining (RSM) framework that can be used to extend existing 2D FCP mining algorithms for FCC mining. The second technique, called CubeMiner, is a novel algorithm that operates on the 3D space directly. We have implemented both schemes, and evaluated their performance on both real and synthetic datasets. The experimental results show that the RSM-based scheme is efficient when one of the dimensions is small, while CubeMiner is superior otherwise.},
  booktitle = {Proceedings of the {{32Nd International Conference}} on {{Very Large Data Bases}}},
  publisher = {{VLDB Endowment}},
  author = {Ji, Liping and Tan, Kian-Lee and Tung, Anthony K. H.},
  year = {2006},
  keywords = {â No DOI found},
  pages = {811--822},
  file = {/home/yuri/Zotero/storage/3WM8MK4I/Ji et al. - Mining Frequent Closed Cubes in 3D Datasets.pdf}
}
% == BibTeX quality report for jiMiningFrequentClosed2006:
% ? Title looks like it was stored in title-case in Zotero

@misc{PDFContrastSubgraph,
  title = {(7) ({{PDF}}) {{Contrast Subgraph Mining}} from {{Coherent Cores}}},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  language = {en},
  journal = {ResearchGate},
  howpublished = {https://www.researchgate.net/publication/323276320\_Contrast\_Subgraph\_Mining\_from\_Coherent\_Cores},
  file = {/home/yuri/Zotero/storage/KGCIC3UA/323276320_Contrast_Subgraph_Mining_from_Coherent_Cores.html}
}
% == BibTeX quality report for PDFContrastSubgraph:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{zhangMiTexCubeMicroTextClusterCube2013,
  title = {{{MiTexCube}}: {{MicroTextCluster Cube}} for Online Analysis of Text Cells and Its Applications},
  volume = {6},
  copyright = {Copyright \textcopyright{} 2012 Wiley Periodicals, Inc., A Wiley Company},
  issn = {1932-1872},
  shorttitle = {{{MiTexCube}}},
  abstract = {A fundamental problem of multidimensional text database analysis is efficient and effective support of various kinds of online applications, such as summarizing the content of a text cell or comparing the contents across multiple text cells. In this paper, we propose a new infrastructure called MicroTextCluster Cube (or MiTexCube) to support efficient online text analysis on multidimensional text databases by introducing micro-clusters of text documents as a compact representation of text content. Experimental results on real multidimensional text databases show that (i) MiTexCube can be materialized efficiently with reasonable overhead in space, and (ii) applications based on the proposed materialized MiTexCube are more efficient than the baseline method of direct analysis based on document units in each cell, without sacrificing much quality of analysis, and MiTexCube naturally accommodates flexible trade-off between efficiency and quality of analysis. \textcopyright{} 2012 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 6: 243\textendash{}259, 2013},
  language = {en},
  number = {3},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  doi = {10/gfz7nq},
  author = {Zhang, Duo and Zhai, ChengXiang and Han, Jiawei},
  year = {2013},
  keywords = {MiTexCube,multidimensional text database,text mining},
  pages = {243-259},
  file = {/home/yuri/Zotero/storage/5LBEZCBJ/Zhang et al_2013_MiTexCube.pdf;/home/yuri/Zotero/storage/N8RHDSAQ/sam.html}
}

@article{tangMultidimensionalSensorData2012,
  title = {Multidimensional {{Sensor Data Analysis}} in {{Cyber}}-{{Physical System}}: {{An Atypical Cube Approach}}},
  volume = {8},
  issn = {1550-1477, 1550-1477},
  shorttitle = {Multidimensional {{Sensor Data Analysis}} in {{Cyber}}-{{Physical System}}},
  language = {en},
  number = {4},
  journal = {International Journal of Distributed Sensor Networks},
  doi = {10/gb7m2c},
  author = {Tang, Lu-An and Yu, Xiao and Kim, Sangkyum and Han, Jiawei and Peng, Wen-Chih and Sun, Yizhou and Leung, Alice and La Porta, Thomas},
  month = apr,
  year = {2012},
  pages = {724846},
  file = {/home/yuri/Zotero/storage/3NVQ6APB/Tang et al_2012_Multidimensional Sensor Data Analysis in Cyber-Physical System.pdf}
}
% == BibTeX quality report for tangMultidimensionalSensorData2012:
% ? Title looks like it was stored in title-case in Zotero

@article{liuMethodsMiningFrequent2011,
  title = {Methods for Mining Frequent Items in Data Streams: An Overview},
  volume = {26},
  issn = {0219-3116},
  shorttitle = {Methods for Mining Frequent Items in Data Streams},
  abstract = {In many real-world applications, information such as web click data, stock ticker data, sensor network data, phone call records, and traffic monitoring data appear in the form of data streams. Online monitoring of data streams has emerged as an important research undertaking. Estimating the frequency of the items on these streams is an important aggregation and summary technique for both stream mining and data management systems with a broad range of applications. This paper reviews the state-of-the-art progress on methods of identifying frequent items from data streams. It describes different kinds of models for frequent items mining task. For general models such as cash register and Turnstile, we classify existing algorithms into sampling-based, counting-based, and hashing-based categories. The processing techniques and data synopsis structure of each algorithm are described and compared by evaluation measures. Accordingly, as an extension of the general data stream model, four more specific models including time-sensitive model, distributed model, hierarchical and multi-dimensional model, and skewed data model are introduced. The characteristics and limitations of the algorithms of each model are presented, and open issues waiting for study and improvement are discussed.},
  language = {en},
  number = {1},
  journal = {Knowledge and Information Systems},
  doi = {10/b23k46},
  author = {Liu, Hongyan and Lin, Yuan and Han, Jiawei},
  month = jan,
  year = {2011},
  keywords = {Data mining,Data stream,Frequent items,Mining methods and algorithms},
  pages = {1-30},
  file = {/home/yuri/Zotero/storage/23YI5GT7/Liu et al_2011_Methods for mining frequent items in data streams.pdf;/home/yuri/Zotero/storage/LQVTNLS8/Liu et al_2011_Methods for mining frequent items in data streams.pdf}
}

@inproceedings{yintaoKeywordSearchText2010,
  title = {Keyword Search in Text Cube: {{Finding}} Top-k Relevant Cells},
  shorttitle = {Keyword Search in Text Cube},
  abstract = {We study the problem of keyword search in a data cube with text-rich dimension(s) (so-called text cube). The text cube is built on a multidimensional text database, where each row is associated with some text data (e.g., a document) and other structural dimensions (attributes). A cell in the text cube aggregates a set of documents with matching attribute values in a subset of dimensions. A cell document is the concatenation of all documents in a cell. Given a keyword query, our goal is to find the top-k most relevant cells (ranked according to the relevance scores of cell documents w.r.t. the given query) in the text cube. We define a keyword-based query language and apply IR-style relevance model for scoring and ranking cell documents in the text cube. We propose two efficient approaches to find the top-k answers. The proposed approaches support a general class of IR-style relevance scoring formulas that satisfy certain basic and common properties. One of them uses more time for pre-processing and less time for answering online queries; and the other one is more efficient in pre-processing and consumes more time for online queries. Experimental studies on the ASRS dataset are conducted to verify the efficiency and effectiveness of the proposed approaches.},
  booktitle = {{{CIDU}} 2010},
  author = {Yintao, Y. U. and Zhao, Bo and Lin, Cindy Xide and Han, Jiawei and Zhai, ChengXiang},
  year = {2010},
  keywords = {Data cube,â No DOI found,Concatenation,Cube Dosage Form,Estimation theory,Keyword,Preprocessor,Query language,Relevance,Score,Search algorithm,Software performance testing,Subgroup,Subject-matter expert,Text corpus},
  file = {/home/yuri/Zotero/storage/GCHK6B59/Yintao et al_2010_Keyword search in text cube.pdf}
}
% == BibTeX quality report for yintaoKeywordSearchText2010:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{liSurveyTruthDiscovery2016,
  title = {A {{Survey}} on {{Truth Discovery}}},
  volume = {17},
  issn = {1931-0145},
  abstract = {Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains.},
  number = {2},
  journal = {SIGKDD Explor. Newsl.},
  doi = {10/gfz7nn},
  author = {Li, Yaliang and Gao, Jing and Meng, Chuishi and Li, Qi and Su, Lu and Zhao, Bo and Fan, Wei and Han, Jiawei},
  month = feb,
  year = {2016},
  pages = {1--16},
  file = {/home/yuri/Zotero/storage/DVDQ6ZPU/Li et al_2016_A Survey on Truth Discovery.pdf}
}
% == BibTeX quality report for liSurveyTruthDiscovery2016:
% ? Possibly abbreviated journal title SIGKDD Explor. Newsl.
% ? Title looks like it was stored in title-case in Zotero

@article{shangContrastSubgraphMining2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06189},
  primaryClass = {cs},
  title = {Contrast {{Subgraph Mining}} from {{Coherent Cores}}},
  abstract = {Graph pattern mining methods can extract informative and useful patterns from large-scale graphs and capture underlying principles through the overwhelmed information. Contrast analysis serves as a keystone in various fields and has demonstrated its effectiveness in mining valuable information. However, it has been long overlooked in graph pattern mining. Therefore, in this paper, we introduce the concept of contrast subgraph, that is, a subset of nodes that have significantly different edges or edge weights in two given graphs of the same node set. The major challenge comes from the gap between the contrast and the informativeness. Because of the widely existing noise edges in real-world graphs, the contrast may lead to subgraphs of pure noise. To avoid such meaningless subgraphs, we leverage the similarity as the cornerstone of the contrast. Specifically, we first identify a coherent core, which is a small subset of nodes with similar edge structures in the two graphs, and then induce contrast subgraphs from the coherent cores. Moreover, we design a general family of coherence and contrast metrics and derive a polynomial-time algorithm to efficiently extract contrast subgraphs. Extensive experiments verify the necessity of introducing coherent cores as well as the effectiveness and efficiency of our algorithm. Real-world applications demonstrate the tremendous potentials of contrast subgraph mining.},
  journal = {arXiv:1802.06189 [cs]},
  author = {Shang, Jingbo and Shi, Xiyao and Jiang, Meng and Liu, Liyuan and Hanratty, Timothy and Han, Jiawei},
  month = feb,
  year = {2018},
  keywords = {â No DOI found,Computer Science - Social and Information Networks},
  file = {/home/yuri/Zotero/storage/4RQQNVEV/Shang et al_2018_Contrast Subgraph Mining from Coherent Cores.pdf;/home/yuri/Zotero/storage/6K7RQW9M/1802.html}
}
% == BibTeX quality report for shangContrastSubgraphMining2018:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1802.06189 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{yuINextCubeInformationNetworkenhanced2009,
  title = {{{iNextCube}}: {{Information Network}}-Enhanced {{Text Cube}}},
  volume = {2},
  issn = {2150-8097},
  shorttitle = {{{iNextCube}}},
  abstract = {Nowadays, most business, administration, and/or scientific databases contain both structured attributes and text attributes. We call a database that consists of both multidimensional structured data and narrative text data as multidimensional text database. Searching, OLAP, and mining such databases pose many research challenges. To enhance the power of data analysis, interesting entities and relationships can be extracted from such databases to derive heterogeneous information networks, which in turn will substantially increase the power and flexibility of data exploration in such databases. Based on our previous studies on TextCube [1], TopicCube [2], and information network analysis, such as RankClus [3] and NetClus [4], we construct iNextCube, an information-Network-enhanced text Cube. In this demo, we show the power of iNextCube in the search and analysis of two multidimensional text databases: (i) a DBLP-based CS bibliographic database, and (ii) an online news database.},
  number = {2},
  journal = {Proc. VLDB Endow.},
  doi = {10/gfz7np},
  author = {Yu, Yintao and Lin, Cindy X. and Sun, Yizhou and Chen, Chen and Han, Jiawei and Liao, Binbin and Wu, Tianyi and Zhai, ChengXiang and Zhang, Duo and Zhao, Bo},
  month = aug,
  year = {2009},
  pages = {1622--1625},
  file = {/home/yuri/Zotero/storage/NG85B6TK/Yu et al_2009_iNextCube.pdf}
}
% == BibTeX quality report for yuINextCubeInformationNetworkenhanced2009:
% ? Possibly abbreviated journal title Proc. VLDB Endow.

@article{chenGraphOLAPMultidimensional2009,
  title = {Graph {{OLAP}}: A Multi-Dimensional Framework for Graph Data Analysis},
  volume = {21},
  issn = {0219-3116},
  shorttitle = {Graph {{OLAP}}},
  abstract = {Databases and data warehouse systems have been evolving from handling normalized spreadsheets stored in relational databases, to managing and analyzing diverse application-oriented data with complex interconnecting structures. Responding to this emerging trend, graphs have been growing rapidly and showing their critical importance in many applications, such as the analysis of XML, social networks, Web, biological data, multimedia data and spatiotemporal data. Can we extend useful functions of databases and data warehouse systems to handle graph structured data? In particular, OLAP (On-Line Analytical Processing) has been a popular tool for fast and user-friendly multi-dimensional analysis of data warehouses. Can we OLAP graphs? Unfortunately, to our best knowledge, there are no OLAP tools available that can interactively view and analyze graph data from different perspectives and with multiple granularities. In this paper, we argue that it is critically important to OLAP graph structured data and propose a novel Graph OLAP framework. According to this framework, given a graph dataset with its nodes and edges associated with respective attributes, a multi-dimensional model can be built to enable efficient on-line analytical processing so that any portions of the graphs can be generalized/specialized dynamically, offering multiple, versatile views of the data. The contributions of this work are three-fold. First, starting from basic definitions, i.e., what are dimensions and measures in the Graph OLAP scenario, we develop a conceptual framework for data cubes on graphs. We also look into different semantics of OLAP operations, and classify the framework into two major subcases: informational OLAP and topological OLAP. Second, we show how a graph cube can be materialized by calculating a special kind of measure called aggregated graph and how to implement it efficiently. This includes both full materialization and partial materialization where constraints are enforced to obtain an iceberg cube. As we can see, due to the increased structural complexity of data, aggregated graphs that depend on the underlying ``network'' properties of the graph dataset are much harder to compute than their traditional OLAP counterparts. Third, to provide more flexible, interesting and informative OLAP of graphs, we further propose a discovery-driven multi-dimensional analysis model to ensure that OLAP is performed in an intelligent manner, guided by expert rules and knowledge discovery processes. We outline such a framework and discuss some challenging research issues for discovery-driven Graph OLAP.},
  language = {en},
  number = {1},
  journal = {Knowledge and Information Systems},
  doi = {10/cf7kqx},
  author = {Chen, Chen and Yan, Xifeng and Zhu, Feida and Han, Jiawei and Yu, Philip S.},
  month = oct,
  year = {2009},
  keywords = {Discovery-driven analysis,Efficient computation,Graph OLAP,Multi-dimensional model},
  pages = {41-63},
  file = {/home/yuri/Zotero/storage/DZX4VQY8/Chen et al_2009_Graph OLAP.pdf}
}

@inproceedings{ahmadiSparseRegressionCube2011,
  title = {The {{Sparse Regression Cube}}: {{A Reliable Modeling Technique}} for {{Open Cyber}}-{{Physical Systems}}},
  shorttitle = {The {{Sparse Regression Cube}}},
  abstract = {Understanding the end-to-end behavior of complex systems where computing technology interacts with physical world properties is a core challenge in cyber-physical computing. This paper develops a hierarchical modeling methodology for open cyber-physical systems that combines techniques in estimation theory with those in data mining to reliably capture complex system behavior at different levels of abstraction. Our technique is also novel in the sense that it provides a measure of confidence in predictions. An application to green transportation is discussed, where the goal is to reduce vehicular fuel consumption and carbon footprint. First-principle models of cyber-physical systems can be very complex and include a large number of parameters, whereas empirical regression models are often unreliable when a high number of parameters is involved. Our new modeling technique, called the Sparse Regression Cube, simultaneously (i) partitions sparse, high-dimensional measurements into subspaces within which reliable linear regression models apply and (ii) determines the best reliable model for each partition, quantifying uncertainty in output prediction. Evaluation results show that the framework significantly improves modeling accuracy compared to previous approaches and correctly quantifies prediction error, while maintaining high efficiency and scalability.},
  booktitle = {2011 {{IEEE}}/{{ACM Second International Conference}} on {{Cyber}}-{{Physical Systems}}},
  doi = {10/b5jx5q},
  author = {Ahmadi, H. and Abdelzaher, T. and Han, J. and Pham, N. and Ganti, R. K.},
  month = apr,
  year = {2011},
  keywords = {Roads,Predictive models,Data models,Computational modeling,open systems,Data Cube,data mining,Fuels,regression analysis,carbon footprint,complex system,cyber-physical computing technology,Cyber-physical System,empirical regression model,estimation theory,Green products,green transportation,hierarchical modeling methodology,high-dimensional measurement,Linear Regression,modeling technique,open cyber-physical system,physical world property,prediction error,Reliability,reliable linear regression model,reliable modeling technique,Sparse Data,sparse regression cube,vehicular fuel consumption},
  pages = {87-96},
  file = {/home/yuri/Zotero/storage/CQ7S3DD8/Ahmadi et al_2011_The Sparse Regression Cube.pdf;/home/yuri/Zotero/storage/99N59ELL/5945424.html}
}
% == BibTeX quality report for ahmadiSparseRegressionCube2011:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{xinComputingIcebergCubes2007,
  title = {Computing {{Iceberg Cubes}} by {{Top}}-{{Down}} and {{Bottom}}-{{Up Integration}}: {{The StarCubing Approach}}},
  volume = {19},
  issn = {1041-4347},
  shorttitle = {Computing {{Iceberg Cubes}} by {{Top}}-{{Down}} and {{Bottom}}-{{Up Integration}}},
  abstract = {Data cube computation is one of the most essential but expensive operations in data warehousing. Previous studies have developed two major approaches, top-down versus bottom-up. The former, represented by the multiway array cube (called the multiway) algorithm, aggregates simultaneously on multiple dimensions; however, it cannot take advantage of a priori pruning when computing iceberg cubes (cubes that contain only aggregate cells whose measure values satisfy a threshold, called the iceberg condition). The latter, represented by BUC, computes the iceberg cube bottom-up and facilitates a priori pruning. BUC explores fast sorting and partitioning techniques; however, it does not fully explore multidimensional simultaneous aggregation. In this paper, we present a new method, star-cubing, that integrates the strengths of the previous two algorithms and performs aggregations on multiple dimensions simultaneously. It utilizes a star-tree structure, extends the simultaneous aggregation methods, and enables the pruning of the group-bys that do not satisfy the iceberg condition. Our performance study shows that star-cubing is highly efficient and outperforms the previous methods},
  number = {1},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  doi = {10/bf9ctr},
  author = {Xin, D. and Han, J. and Li, X. and Shao, Z. and Wah, B. W.},
  month = jan,
  year = {2007},
  keywords = {Data mining,data mining,data warehouses,data warehousing,Aggregates,data cube computation,Data analysis,Sorting,Costs,a priori pruning,bottom-up integration,Data warehouse,Database systems,iceberg cube computation,Multidimensional systems,multiway array cube algorithm,online analytical processing (OLAP).,Partitioning algorithms,Regression analysis,star-tree structure,starcubing approach,top-down integration,Warehousing},
  pages = {111-126},
  file = {/home/yuri/Zotero/storage/JZNIVYHZ/Xin et al_2007_Computing Iceberg Cubes by Top-Down and Bottom-Up Integration.pdf;/home/yuri/Zotero/storage/NV5RKBXA/4016519.html}
}
% == BibTeX quality report for xinComputingIcebergCubes2007:
% ? Title looks like it was stored in title-case in Zotero

@article{liuTopdownMiningFrequent2009,
  title = {Top-down Mining of Frequent Closed Patterns from Very High Dimensional Data},
  volume = {179},
  issn = {0020-0255},
  abstract = {Frequent pattern mining is an essential theme in data mining. Existing algorithms usually use a bottom-up search strategy. However, for very high dimensional data, this strategy cannot fully utilize the minimum support constraint to prune the rowset search space. In this paper, we propose a new method called top-down mining together with a novel row enumeration tree to make full use of the pruning power of the minimum support constraint. Furthermore, to efficiently check if a rowset is closed, we develop a method called the trace-based method. Based on these methods, an algorithm called TD-Close is designed for mining a complete set of frequent closed patterns. To enhance its performance further, we improve it by using new pruning strategies and new data structures that lead to a new algorithm TTD-Close. Our performance study shows that the top-down strategy is effective in cutting down search space and saving memory space, while the trace-based method facilitates the closeness-checking. As a result, the algorithm TTD-Close outperforms the bottom-up search algorithms such as Carpenter and FPclose in most cases. It also runs faster than TD-Close.},
  number = {7},
  journal = {Information Sciences},
  doi = {10/cwx9mq},
  author = {Liu, Hongyan and Wang, Xiaoyu and He, Jun and Han, Jiawei and Xin, Dong and Shao, Zheng},
  month = mar,
  year = {2009},
  keywords = {Data mining,Association rules,Frequent patterns,High dimensional data},
  pages = {899-924},
  file = {/home/yuri/Zotero/storage/LEG483LT/Liu et al_2009_Top-down mining of frequent closed patterns from very high dimensional data.pdf;/home/yuri/Zotero/storage/DNKNVJML/S0020025508005136.html}
}

@article{dokaBrownDwarfFullydistributed2011,
  title = {Brown {{Dwarf}}: {{A}} Fully-Distributed, Fault-Tolerant Data Warehousing System},
  volume = {71},
  issn = {0743-7315},
  shorttitle = {Brown {{Dwarf}}},
  abstract = {In this paper we present the Brown Dwarf, a distributed data analytics system designed to efficiently store, query and update multidimensional data over commodity network nodes, without the use of any proprietary tool. Brown Dwarf distributes a centralized indexing structure among peers on-the-fly, reducing cube creation and querying times by enforcing parallelization. Analytical queries are naturally performed on-line through cooperating nodes that form an unstructured Peer-to-Peer overlay. Updates are also performed on-line, eliminating the usually costly over-night process. Moreover, the system employs an adaptive replication scheme that adjusts to the workload skew as well as the network churn by expanding or shrinking the units of the distributed data structure. Our system has been thoroughly evaluated on an actual testbed: it manages to accelerate cube creation up and querying up to several tens of times compared to the centralized solution by exploiting the capabilities of the available network nodes working in parallel. It also manages to quickly adapt even after sudden bursts in load and remains unaffected with a considerable fraction of frequent node failures. These advantages are even more apparent for dense and skewed data cubes and workloads.},
  number = {11},
  journal = {Journal of Parallel and Distributed Computing},
  doi = {10/b5mhxn},
  author = {Doka, Katerina and Tsoumakos, Dimitrios and Koziris, Nectarios},
  month = nov,
  year = {2011},
  keywords = {Data cube,Data warehousing,Peer-to-Peer},
  pages = {1434-1446},
  file = {/home/yuri/Zotero/storage/RYFQREP7/Doka et al_2011_Brown Dwarf.pdf;/home/yuri/Zotero/storage/VL8Q36KI/S0743731511001432.html}
}

@article{strubellIntroductionQuantumAlgorithms2011,
  title = {An {{Introduction}} to {{Quantum Algorithms}}},
  language = {en},
  author = {Strubell, Emma},
  year = {2011},
  keywords = {â No DOI found},
  pages = {35},
  file = {/home/yuri/Zotero/storage/JXUFKP5K/Strubell - An Introduction to Quantum Algorithms.pdf}
}
% == BibTeX quality report for strubellIntroductionQuantumAlgorithms2011:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@book{jeromeUtilizingBigData2018,
  title = {Utilizing {{Big Data Paradigms}} for {{Business Intelligence}}},
  isbn = {978-1-5225-4964-2},
  abstract = {Because efficient compilation of information allows managers and business leaders to make the best decisions for the financial solvency of their organizations, data analysis is an important part of modern business administration. Understanding the use of analytics, reporting, and data mining in everyday business environments is imperative to the success of modern businesses. Utilizing Big Data Paradigms for Business Intelligence is a pivotal reference source that provides vital research on how to address the challenges of data extraction in business intelligence using the five ``Vs'' of big data: velocity, volume, value, variety, and veracity. This book is ideally designed for business analysts, investors, corporate managers, entrepreneurs, and researchers in the fields of computer science, data science, and business intelligence.},
  language = {en},
  publisher = {{IGI Global}},
  author = {J{\'e}r{\^o}me, Darmont and Sabine, Loudcher},
  month = aug,
  year = {2018},
  keywords = {Business \& Economics / General,Business \& Economics / Information Management,Computers / Data Processing}
}
% == BibTeX quality report for jeromeUtilizingBigData2018:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{zhaoClosedFragShellsCubing2018,
  address = {{New York, NY, USA}},
  series = {{{IMCOM}} '18},
  title = {A {{Closed Frag}}-{{Shells Cubing Algorithm}} on {{High Dimensional}} and {{Non}}-{{Hierarchical Data Sets}}},
  isbn = {978-1-4503-6385-3},
  abstract = {In view of high-dimensional and non-hierarchical large data sets, an improved CFSC (Closed Frag-Shells Cube) method is proposed based on the Frag-Shells method in this paper. When the Data Cube is generated, the high-dimensional data is divided into several low-dimensional data fragments by using the idea of partitioning cubes into dimension attributes. For each dimension data segment, the closed cubes of each dimension data segment are calculated using the closed cube calculation. A query bitmap is added to each fragment, and a query index table of closed segments is constructed by using bit map index technology to reduce the storage space occupied by the result set and to increase the query efficiency. Based on the application of multidimensional analysis of water conservancy census data, it is proved that this method can effectively reduce the storage space of cube data of water conservancy census data and improve the efficiency of OLAP (online analytical processing) query.},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Ubiquitous Information Management}} and {{Communication}}},
  publisher = {{ACM}},
  doi = {10/gfz7nm},
  author = {Zhao, Qun and Zhu, Yuelong and Wan, Dingsheng and Tang, Shanshan},
  year = {2018},
  keywords = {Bitmap Index,Closed Cube,Date Cube,Inverted Index,Shell Segment Cube},
  pages = {6:1--6:8},
  file = {/home/yuri/Zotero/storage/65IL5G2X/Zhao et al_2018_A Closed Frag-Shells Cubing Algorithm on High Dimensional and Non-Hierarchical.pdf}
}
% == BibTeX quality report for zhaoClosedFragShellsCubing2018:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{neumayrSemanticEnrichmentOLAP2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Semantic {{Enrichment}} of {{OLAP Cubes}}: {{Multi}}-Dimensional {{Ontologies}} and {{Their Representation}} in {{SQL}} and {{OWL}}},
  isbn = {978-3-642-41030-7},
  shorttitle = {Semantic {{Enrichment}} of {{OLAP Cubes}}},
  abstract = {A multi-dimensional ontology (MDO) enriches an OLAP cube with concepts that represent business terms in the context of data analysis. The formal representation of the meaning of business terms facilitates the unambiguous interpretation of query results as well as the sharing of knowledge among business analysts. In contrast to traditional ontologies, an MDO captures the multi-dimensional, hierarchical world view of business analysts. In this paper, we introduce a translation of MDO concepts to SQL in order to allow for the querying of a closed-world OLAP cube. We introduce a representation in OWL in order to determine subsumption hierarchies of MDO concepts using off-the-shelf reasoners.},
  language = {en},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}}: {{OTM}} 2013 {{Conferences}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Neumayr, Bernd and Sch{\"u}tz, Christoph and Schrefl, Michael},
  editor = {Meersman, Robert and Panetto, Herv{\'e} and Dillon, Tharam and Eder, Johann and Bellahsene, Zohra and Ritter, Norbert and De Leenheer, Pieter and Dou, Deijing},
  year = {2013},
  keywords = {OLAP,\#nosource,Business Intelligence,Data Warehouse,Knowledge Representation and Reasoning},
  pages = {624-641}
}

@inproceedings{brahmiConstrainedClosedNon2012,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Constrained {{Closed Non Derivable Data Cubes}}},
  isbn = {978-3-642-35527-1},
  abstract = {It is well recognized that data cubes often produce huge outputs. Several efforts were devoted to this problem through Constrained Cubes allowing the user to focus on a particular set of interesting tuples. In this paper, we investigate reduced representations for the Constrained Cube (e.g., Constrained Closed Cube and Constrained Quotient Cube). That is why we introduce a new and concise representation of data cubes: the Constrained Closed Non Derivable Data Cube (CCND - Cube). The latter captures all the tuples of a data cube fulfilling a combination of monotone/anti-monotone constraints. It can be represented in a very compact way in order to optimize both computation time and required storage space. The results of our experiments confirm the relevance of our proposal.},
  language = {en},
  booktitle = {Advanced {{Data Mining}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Brahmi, Hanen and Ben Yahia, Sadok},
  editor = {Zhou, Shuigeng and Zhang, Songmao and Karypis, George},
  year = {2012},
  keywords = {\#nosource,Data warehouses,Closed patterns,Constrained cubes,Data cubes,Minimal generators,Non derivable patterns},
  pages = {766-778}
}
% == BibTeX quality report for brahmiConstrainedClosedNon2012:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{heinePopUpCubingAlgorithmEfficiently2017,
  address = {{New York, NY, USA}},
  series = {{{BDCAT}} '17},
  title = {{{PopUp}}-{{Cubing}}: {{An Algorithm}} to {{Efficiently Use Iceberg Cubes}} in {{Data Streams}}},
  isbn = {978-1-4503-5549-0},
  shorttitle = {{{PopUp}}-{{Cubing}}},
  abstract = {Data streams become more and more important in modern computer infrastructures. The amount of data processed by computer systems continuously increased in the past years, in a way that storing all of the data is not effective or even impossible. Consequently the most feasible way to get information out of the data is to compute it in a stream processing manner.  This paper proposes an iceberg cube algorithm which is able to compute an incremental iceberg cube with every new data record in a window of a stream. An iceberg cube is a data cube in which all attribute value combinations have to fulfill an aggregation condition. In contrast to many algorithms which use a combination of sorting and partitioning, the new PopUp-Cubing algorithm uses hashing to find the right cells for aggregation. PopUp-Cubing supports pruning in the incremental iceberg cube and is able to perform multi-dimensional simultaneous aggregation. Additionally, PopUp-Cubing has an indicator for cells that pop up or submerge at the metaphorical water surface of an iceberg cube. This happens, if the aggregated value of cells changes over time. This indicator can help to identify important changes in a data stream. The algorithm is evaluated with a network flow dataset. As the results will show, the new PopUp-Cubing algorithm outperforms a batch like algorithm that computes every window by a factor of about 360.},
  booktitle = {Proceedings of the {{Fourth IEEE}}/{{ACM International Conference}} on {{Big Data Computing}}, {{Applications}} and {{Technologies}}},
  publisher = {{ACM}},
  doi = {10/gfz7nk},
  author = {Heine, Felix and Rohde, Marius},
  year = {2017},
  keywords = {data cube,olap,data stream,iceberg cube},
  pages = {11--20},
  file = {/home/yuri/Zotero/storage/3LYUAN4W/Heine and Rohde - 2017 - PopUp-Cubing An Algorithm to Efficiently Use Iceb.pdf;/home/yuri/Zotero/storage/YUJA8EPK/Heine_Rohde_2017_PopUp-Cubing.pdf}
}
% == BibTeX quality report for heinePopUpCubingAlgorithmEfficiently2017:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{youDoubleTableSwitch2010,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Double {{Table Switch}}: {{An Efficient Partitioning Algorithm}} for {{Bottom}}-{{Up Computation}} of {{Data Cubes}}},
  isbn = {978-3-642-17313-4},
  shorttitle = {Double {{Table Switch}}},
  abstract = {Bottom-up computation of data cubes is an efficient approach which is adopted and developed by many other cubing algorithms such as H-Cubing, Quotient Cube and Closed Cube, etc. The main cost of bottom-up computation is recursively sorting and partitioning the base table in a worse way where large amount of auxiliary spaces are frequently allocated and released. This paper proposed a new partitioning algorithm, called Double Table Switch (DTS). It sets up two table spaces in the memory at the beginning, where the partitioned results in one table are copied into another table alternatively during the bottom-up computation. Thus DTS avoids the costly space management and achieves the constant memory usage. Further, we improve the DTS algorithm by adjusting the dimension order, etc. The experimental results demonstrate the efficiency of DTS.},
  language = {en},
  booktitle = {Advanced {{Data Mining}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {You, Jinguo and Jia, Lianying and Hu, Jianhua and Huang, Qingsong and Xi, Jianqing},
  editor = {Cao, Longbing and Zhong, Jiang and Feng, Yong},
  year = {2010},
  keywords = {\#nosource,Data cube,Data warehouse,Bottom-up computation,Double Table Switch,Partitioning algorithm},
  pages = {183-190}
}
% == BibTeX quality report for youDoubleTableSwitch2010:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{gomezAnalyzingContinuousFields2011,
  title = {Analyzing Continuous Fields with {{OLAP}} Cubes},
  isbn = {978-1-4503-0963-9},
  booktitle = {Proceedings of the {{ACM}} 14th International Workshop on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/c4d59t},
  author = {G{\'o}mez, Leticia I. and G{\'o}mez, Silvia A. and Vaisman, Alejandro A.},
  month = oct,
  year = {2011},
  pages = {89-94},
  file = {/home/yuri/Zotero/storage/JR9NK54K/GÃ³mez et al_2011_Analyzing continuous fields with OLAP cubes.pdf;/home/yuri/Zotero/storage/GRLW5NIW/citation.html}
}
% == BibTeX quality report for gomezAnalyzingContinuousFields2011:
% ? Unsure about the formatting of the booktitle

@inproceedings{andResearchDataCube2009,
  title = {Research on {{Data Cube}} Technology of {{Dwarf}} Based Semantic {{OLAP}}},
  abstract = {The computation of high-dimension Data Cube in data warehouse is of much importance. Dwarf is a highly compressed structure for computing and storing data cubes which can be materialized completely. During the constructing process, each closed node is stored in disk. While the computation of aggregation units needs to access the closed nodes in the disk frequently. For avoid accessing the unnecessary closed nodes in disk, in this paper, we propose an optimized algorithm named Q-Dwarf. The property of this algorithm guarantees that once the closed nodes are written to disk, they will not be read out again, and query algorithm and update algorithm are both based on files. Experimental results show that the performance of the new algorithm outperforms that of the Dwarf algorithm, and the query and update algorithms are efficient for data warehousing.},
  booktitle = {2009 {{International Conference}} on {{Future BioMedical Information Engineering}} ({{FBIE}})},
  doi = {10/cbxzmh},
  author = {{and} and Zhao, {and} Y.},
  month = dec,
  year = {2009},
  keywords = {optimisation,data mining,data warehouses,cube,online analytical processing,query processing,data compression,Data compression,data warehouse,Data warehouses,Data structures,Data engineering,Space technology,Warehousing,Biomedical computing,Biomedical engineering,data cube technology,disc storage,dwarf,Dwarf based semantic OLAP,Fuses,high-dimension,Q-Dwarf,query algorithm,Spatial databases,update algorithm},
  pages = {540-543},
  file = {/home/yuri/Zotero/storage/68GFV8HV/and_Zhao_2009_Research on Data Cube technology of Dwarf based semantic OLAP.pdf;/home/yuri/Zotero/storage/9DP4PFCB/5405777.html}
}
% == BibTeX quality report for andResearchDataCube2009:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{brahmiClosedNonDerivable2009,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Closed {{Non Derivable Data Cubes Based}} on {{Non Derivable Minimal Generators}}},
  isbn = {978-3-642-03348-3},
  abstract = {It is well recognized that data cubes often produce huge outputs. Several efforts were devoted to this problem through closed cubes, where cells preserving aggregation semantics are losslessly reduced to one cell. In this paper, we introduce the concept of closed non derivable data cube, denoted CNDCND\textbackslash{}mathcal\{CND\} - CC\textbackslash{}mathcal\{C\}ube, which generalizes the notion of bi-dimensional frequent closed non derivable patterns to the multidimensional context. We propose a novel algorithm to mine CNDCND\textbackslash{}mathcal\{CND\} - CC\textbackslash{}mathcal\{C\}ube from multidimensional databases considering three anti-monotone constraints, namely ``to be frequent'', ``to be non derivable'' and ``to be minimal generator''. Experiments show that our proposal provides the smallest representation of a data cube and thus is the most efficient for saving storage space.},
  language = {en},
  booktitle = {Advanced {{Data Mining}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Brahmi, Hanen and Hamrouni, Tarek and Ben Messaoud, Riadh and Ben Yahia, Sadok},
  editor = {Huang, Ronghuai and Yang, Qiang and Pei, Jian and Gama, Jo{\~a}o and Meng, Xiaofeng and Li, Xue},
  year = {2009},
  keywords = {data cube,\#nosource,Data warehouse,closed pattern,minimal generator,non derivable pattern},
  pages = {55-66}
}
% == BibTeX quality report for brahmiClosedNonDerivable2009:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{casaliLosslessReductionDatacubes2009,
  title = {Lossless {{Reduction}} of {{Datacubes}} Using {{Partitions}}},
  volume = {5},
  copyright = {Access limited to members},
  issn = {1548-3924},
  language = {en},
  number = {1},
  journal = {International Journal of Data Warehousing and Mining (IJDWM)},
  doi = {10/bhsqrd},
  author = {Casali, Alain and Nedjar, S{\'e}bastien and Cicchetti, Rosine and Lakhal, Lotfi and Novelli, No{\"e}l},
  year = {2009},
  pages = {18-35},
  file = {/home/yuri/Zotero/storage/MJYI4YFN/1821.html}
}

@inproceedings{missaouiWhatCanFormal2009,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {What {{Can Formal Concept Analysis Do}} for {{Data Warehouses}}?},
  isbn = {978-3-642-01815-2},
  abstract = {Formal concept analysis (FCA) has been successfully used in several Computer Science fields such as databases, software engineering, and information retrieval, and in many domains like medicine, psychology, linguistics and ecology. In data warehouses, users exploit data hypercubes (i.e., multi-way tables) mainly through online analytical processing (OLAP) techniques to extract useful information from data for decision support purposes.Many topics have attracted researchers in the area of data warehousing: data warehouse design and multidimensional modeling, efficient cube materialization (pre-computation), physical data organization, query optimization and approximation, discovery-driven data exploration as well as cube compression and mining. Recently, there has been an increasing interest to apply or adapt data mining approaches and advanced statistical analysis techniques for extracting knowledge (e.g., outliers, clusters, rules, closed n-sets) from multidimensional data. Such approaches or techniques cover (but are not limited to) FCA, cluster analysis, principal component analysis, log-linear modeling, and non-negative multi-way array factorization. Since data cubes are generally large and highly dimensional, and since cells contain consolidated (e.g., mean value), multidimensional and temporal data, such facts lead to challenging research issues in mining data cubes. In this presentation, we will give an overview of related work and show how FCA theory (with possible extensions) can be used to extract valuable and actionable knowledge from data warehouses.},
  language = {en},
  booktitle = {Formal {{Concept Analysis}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Missaoui, Rokia and Kwuida, L{\'e}onard},
  editor = {Ferr{\'e}, S{\'e}bastien and Rudolph, Sebastian},
  year = {2009},
  keywords = {Data Cube,\#nosource,Approximate Query,Association Rule,Formal Concept Analysis,Galois Lattice},
  pages = {58-65}
}
% == BibTeX quality report for missaouiWhatCanFormal2009:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{chandranCubicityBipartiteGraphs2009,
  title = {On the Cubicity of Bipartite Graphs},
  volume = {109},
  issn = {0020-0190},
  abstract = {A unit cube in k-dimension (or a k-cube) is defined as the Cartesian product R1\texttimes{}R2\texttimes{$\cdots\times$}Rk, where each Ri is a closed interval on the real line of the form [ai,ai+1]. The cubicity of G, denoted as cub(G), is the minimum k such that G is the intersection graph of a collection of k-cubes. Many NP-complete graph problems can be solved efficiently or have good approximation ratios in graphs of low cubicity. In most of these cases the first step is to get a low dimensional cube representation of the given graph. It is known that for a graph G, cub(G){$\leqslant\lfloor$}2n3{$\rfloor$}. Recently it has been shown that for a graph G, cub(G){$\leqslant$}4({$\Delta$}+1)lnn, where n and {$\Delta$} are the number of vertices and maximum degree of G, respectively. In this paper, we show that for a bipartite graph G=(A{$\cup$}B,E) with |A|=n1, |B|=n2, n1{$\leqslant$}n2, and {$\Delta{'}$}=min\{{$\Delta$}A,{$\Delta$}B\}, where {$\Delta$}A=maxa{$\in$}Ad(a) and {$\Delta$}B=maxb{$\in$}Bd(b), d(a) and d(b) being the degree of a and b in G, respectively, cub(G){$\leqslant$}2({$\Delta{'}$}+2){$\lceil$}lnn2{$\rceil$}. We also give an efficient randomized algorithm to construct the cube representation of G in 3({$\Delta{'}$}+2){$\lceil$}lnn2{$\rceil$} dimensions. The reader may note that in general {$\Delta{'}$} can be much smaller than {$\Delta$}.},
  number = {9},
  journal = {Information Processing Letters},
  doi = {10/c8vhbw},
  author = {Chandran, L. Sunil and Das, Anita and Sivadasan, Naveen},
  month = apr,
  year = {2009},
  keywords = {Algorithms,Cubicity,Intersection graphs},
  pages = {432-435},
  file = {/home/yuri/Zotero/storage/QPYEUM5G/Chandran et al_2009_On the cubicity of bipartite graphs.pdf;/home/yuri/Zotero/storage/M6ATCHFN/S0020019008003864.html}
}

@inproceedings{sheng-enResearchClosedData2004,
  title = {Research on {{Closed Data Cube Technology}}},
  abstract = {There is a lot of redundant information in a data cube. Removing redundancy from a data cube can not only reduce the storage space but also accelerate the computation. Tuples of a data cube can be divided into closed-tuples and non-closed tuples. For any non-closed tuple, there exists a closed-tuple, and both are aggregated from the same set of tuples in a base table and have the same aggregated value. By removing all non-closed tuples, a data cube can be translated to a closed data cube. The algorithm of computing a closed data cube is given, answering a query and maintaining the closed data cube incrementally. The results of experiments are also presented by using both the synthetic and real-world data sets. The experimental results show that the closed data cube technique is effective.},
  author = {{Sheng-en}, Li and Shan, Wang},
  year = {2004},
  keywords = {Data cube,Algorithm,â No DOI found,Cube Dosage Form,Experiment,Synthetic intelligence},
  file = {/home/yuri/Zotero/storage/VX5BWIIL/Sheng-en_Shan_2004_Research on Closed Data Cube Technology.pdf}
}
% == BibTeX quality report for sheng-enResearchClosedData2004:
% Missing required field 'booktitle'
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Title looks like it was stored in title-case in Zotero

@incollection{stonebrakerMakingDatabasesWork2019,
  address = {{New York, NY, USA}},
  title = {Making {{Databases Work}}},
  isbn = {978-1-947487-19-2},
  abstract = {In previous papers [SC05, SBC+07], some of us predicted the end of "one size fits all" as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by 1--2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets. Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T. to a popular RDBMS on the standard transactional benchmark, TPC-C. We conclude that the current RDBMS code lines, while attempting to be a "one size fits all" solution, infact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of "from scratch" specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for yesterday's needs.},
  publisher = {{Association for Computing Machinery and Morgan \& Claypool}},
  author = {Stonebraker, Michael and Madden, Samuel and Abadi, Daniel J. and Harizopoulos, Stavros and Hachem, Nabil and Helland, Pat},
  editor = {Brodie, Michael L.},
  year = {2019},
  keywords = {\#nosource},
  pages = {463--489},
  doi = {10.1145/3226595.3226637}
}
% == BibTeX quality report for stonebrakerMakingDatabasesWork2019:
% Missing required field 'booktitle'
% ? Title looks like it was stored in title-case in Zotero

@article{liSemiClosedCubeEffective2005,
  title = {Semi-{{Closed Cube}}: {{An Effective Approach}} to {{Trading Off Data Cube Size}} and {{Query Response Time}}},
  volume = {20},
  issn = {1860-4749},
  shorttitle = {Semi-{{Closed Cube}}},
  abstract = {The results of data cube will occupy huge amount of disk space when the base table is of a large number of attributes. A new type of data cube, compact data cube like condensed cube and quotient cube, was proposed to solve the problem. It compresses data cube dramatically. However, its query cost is so high that it cannot be used in most applications. This paper introduces the semi-closed cube to reduce the size of data cube and achieve almost the same query response time as the data cube does. Semi-closed cube is a generalization of condensed cube and quotient cube and is constructed from a quotient cube. When the query cost of quotient cube is higher than a given threshold, semi-closed cube selects some views and picks a fellow for each of them. All the tuples of those views are materialized except those closed by their fellows. To find a tuple of those views, users only need to scan the view and its fellow. Thus, their query performance is improved. Experiments were conducted using a real-world data set. The results show that semi-closed cube is an effective approach of data cube.},
  language = {en},
  number = {3},
  journal = {Journal of Computer Science and Technology},
  doi = {10/bmq7p4},
  author = {Li, Sheng-En and Wang, Shan},
  month = may,
  year = {2005},
  keywords = {data cube,OLAP (on-line analytical processing),warehousing},
  pages = {367-372},
  file = {/home/yuri/Zotero/storage/BUKZPANX/Li_Wang_2005_Semi-Closed Cube.pdf}
}
% == BibTeX quality report for liSemiClosedCubeEffective2005:
% ? Title looks like it was stored in title-case in Zotero

@article{zhangBuildingQuotientCube2013,
  title = {Building {{Quotient Cube}} with {{MapReduce}} in {{Hadoop}}},
  volume = {765-767},
  issn = {1662-8985},
  abstract = {In order to solve the problem that how to improve the efficiency of query and calculation in massive data, a method of building quotient cubes in Hadoop plateform which combined the advantage of the quotient cube and MapReduce model is proposed in this paper. At first, all cubes will be established and their aggregate value will be calculated in the Mapping stage. All the key/value pair formed in Mapping stage will be passed to Reducing stage. Equivalence partitioning will be carried out In this stage, and the minimum aggregation cube of each equivalence partitioning will be the key with its aggregate value. According to the minimum aggregation cubes, we can get the quotient cubes. In order to improve the speed of parallel computing and reduce network traffic, equivalence class division will be executed locally after the Map stage, it is named as combiner stage. In this paper, MapReduce model is used to improve the efficiency of building quotient cube because of its ability of parallel computing in a large amount of data. In addition, the experiment proved that, under certain circumstances, increasing the number of Mapper/Reducer task can reduce the building time effectively, and improve the construction efficiency.},
  journal = {Advanced Materials Research},
  doi = {10/gfz7m2},
  author = {Zhang, Juan and Zhang, Jiong Min},
  month = sep,
  year = {2013},
  keywords = {\#nosource},
  pages = {1031-1035}
}
% == BibTeX quality report for zhangBuildingQuotientCube2013:
% Missing required field 'number'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{casaliExtractingSemanticsData2003,
  address = {{New York, NY, USA}},
  series = {{{KDD}} '03},
  title = {Extracting {{Semantics}} from {{Data Cubes Using Cube Transversals}} and {{Closures}}},
  isbn = {978-1-58113-737-8},
  abstract = {In this paper we propose a lattice-based approach intended for extracting semantics from datacubes: borders of version spaces for supervised classification, closed cube lattice to summarize the semantics of datacubes w.r.t. COUNT, SUM, and covering graph of the quotient cube as a visualization tool of minimal multidimensional associations. With this intention, we introduce two novel concepts: the cube transversals and the cube closures over the cube lattice of a categorical database relation. We propose a levelwise merging algorithm for mining minimal cube transversals with a single database scan. We introduce the cube connection, show that it is a Galois connection and derive a closure operator over the cube lattice. Using cube transversals and closures, we define a new characterization of boundary sets which provide a condensed representation of version spaces used to enhance supervised classification. The algorithm designed for computing such borders improves the complexity of previous proposals. We also introduce the concept of closed cube lattice and show that it is isomorph to on one hand the Galois lattice and on the other hand the quotient cube w.r.t. COUNT, SUM. Proposed in [16], the quotient cube is a succinct summary of a datacube preserving the Rollup/Drilldown semantics. We show that the quotient cube w.r.t. COUNT, SUM and the closed cube lattice have a similar expression power but the latter has the smallest possible size. Finally we focus on the multidimensional association issue and introduce the covering graph of the quotient cube which provides the user with a visualization tool of minimal multidimensional associations.},
  booktitle = {Proceedings of the {{Ninth ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{ACM}},
  doi = {10/c7fxbz},
  author = {Casali, Alain and Cicchetti, Rosine and Lakhal, Lotfi},
  year = {2003},
  keywords = {algorithm,closures,datacubes,hypergraph transversals,lattices,version spaces},
  pages = {69--78},
  file = {/home/yuri/Zotero/storage/9HFQXHLG/Casali et al_2003_Extracting Semantics from Data Cubes Using Cube Transversals and Closures.pdf}
}
% == BibTeX quality report for casaliExtractingSemanticsData2003:
% ? Title looks like it was stored in title-case in Zotero

@article{patonAutomatingDataPreparation2019,
  title = {Automating {{Data Preparation}}: {{Can We}}? {{Should We}}? {{Must We}}?},
  language = {en},
  author = {Paton, Norman W},
  year = {2019},
  keywords = {â No DOI found},
  pages = {5},
  file = {/home/yuri/Zotero/storage/8DFLAGHL/Paton - Automating Data Preparation Can We Should We Mu.pdf}
}
% == BibTeX quality report for patonAutomatingDataPreparation2019:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@article{gallinucciVarietyAwareOLAPDocumentOriented2018,
  title = {Variety-{{Aware OLAP}} of {{Document}}-{{Oriented Databases}}},
  abstract = {Schemaless databases, and document-oriented databases in particular, are preferred to relational ones for storing heterogeneous data with variable schemas and structural forms. However, the absence of a unique schema adds complexity to analytical applications, in which a single analysis often involves large sets of data with different schemas. In this paper we propose an original approach to OLAP on collections stored in document-oriented databases. The basic idea is to stop fighting against schema variety and welcome it as an inherent source of information wealth in schemaless sources. Our approach builds on four stages: schema extraction, schema integration, FD enrichment, and querying; these stages are discussed in detail in the paper. To make users aware of the impact of schema variety, we propose a set of indicators related for instance to query completeness and precision.},
  language = {en},
  author = {Gallinucci, Enrico and Golfarelli, Matteo and Rizzi, Stefano},
  year = {2018},
  keywords = {â No DOI found},
  pages = {10},
  file = {/home/yuri/Zotero/storage/PMLZAZ4Q/Gallinucci et al. - Variety-Aware OLAP of Document-Oriented Databases.pdf}
}
% == BibTeX quality report for gallinucciVarietyAwareOLAPDocumentOriented2018:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@article{franciaAugmentedBusinessIntelligence2019,
  title = {Augmented {{Business Intelligence}}},
  abstract = {Augmented reality allows users to superimpose digital information (typically, of operational type) upon real world entities. The synergy of analytical frameworks and augmented reality opens the door to a new wave of situated OLAP, in which users within a physical environment are provided with immersive analyses of local contextual data. In this paper we propose an approach that, based on the sensed augmented context (provided by wearable and smart devices), proposes a set of relevant analytical queries to the user. This is done by relying on a mapping between the entities that can be recognized by the devices and the elements of the enterprise data, and also taking into account the queries preferred by users during previous interactions that occurred in similar contexts. A set of experimental tests evaluates the proposed approach in terms of efficiency and effectiveness.},
  language = {en},
  author = {Francia, Matteo and Golfarelli, Matteo and Rizzi, Stefano},
  year = {2019},
  keywords = {â No DOI found},
  pages = {10},
  file = {/home/yuri/Zotero/storage/5P38G3QV/Francia et al. - Augmented Business Intelligence.pdf}
}
% == BibTeX quality report for franciaAugmentedBusinessIntelligence2019:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{andersenAdvancedDataWarehouse2014,
  address = {{New York, NY, USA}},
  series = {{{DOLAP}} '14},
  title = {An {{Advanced Data Warehouse}} for {{Integrating Large Sets}} of {{GPS Data}}},
  isbn = {978-1-4503-0999-8},
  abstract = {GPS data recorded from driving vehicles is available from many sources and is a very good data foundation for answering traffic related queries. However, most approaches so far have not considered combining GPS data from many sources into a single data warehouse. Further, the integration of GPS data with fuel consumption data (from the so-called CAN bus in the vehicles) and weather data has not been done. In this paper, we propose a data warehouse design for handling GPS data, fuel consumption data, and weather data. The design is fully implemented in a running system using the PostgreSQL DBMS. The system has been in production since March 2011 and the main fact table contains today approximately 3.4 billion rows from 16 different data sources. We show that the system can be used for a number of novel traffic related analyses such as relating the fuel consumption of vehicles with the road network and road congestion.},
  booktitle = {Proceedings of the 17th {{International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/gfz7nh},
  author = {Andersen, Ove and Krogh, Benjamin Bjerre and Thomsen, Christian and Torp, Kristian},
  year = {2014},
  keywords = {data warehousing,can bus,GPS,weather},
  pages = {13--22},
  file = {/home/yuri/Zotero/storage/Y5E8HK8X/Andersen et al_2014_An Advanced Data Warehouse for Integrating Large Sets of GPS Data.pdf}
}
% == BibTeX quality report for andersenAdvancedDataWarehouse2014:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{aligonHolisticApproachOLAP2014,
  address = {{New York, NY, USA}},
  series = {{{DOLAP}} '14},
  title = {A {{Holistic Approach}} to {{OLAP Sessions Composition}}: {{The Falseto Experience}}},
  isbn = {978-1-4503-0999-8},
  shorttitle = {A {{Holistic Approach}} to {{OLAP Sessions Composition}}},
  abstract = {OLAP is the main paradigm for flexible and effective exploration of multidimensional cubes in data warehouses. During an OLAP session the user analyzes the results of a query and determines a new query that will give her a better understanding of information. Given the huge size of the data space, this exploration process is often tedious and may leave the user disoriented and frustrated. This paper presents an OLAP tool named Falseto (Former AnalyticaL Sessions for lEss Tedious Olap), that is meant to assist query and session composition, by letting the user summarize, browse, query, and reuse former analytical sessions. Falseto's implementation on top of a formal framework is detailed. We also report the experiments we run to obtain and analyze real OLAP sessions and assess Falseto with them. Finally, we discuss how Falseto can be seen as a starting point for bridging OLAP with exploratory search, a search paradigm centered on the user and the evolution of her knowledge.},
  booktitle = {Proceedings of the 17th {{International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/gfz7ng},
  author = {Aligon, Julien and Boulil, Kamal and Marcel, Patrick and Peralta, Veronika},
  year = {2014},
  keywords = {exploratory search,focused search,log mining,recommendation,summary,tool,user support},
  pages = {37--46},
  file = {/home/yuri/Zotero/storage/BEQSU6QM/Aligon et al_2014_A Holistic Approach to OLAP Sessions Composition.pdf}
}
% == BibTeX quality report for aligonHolisticApproachOLAP2014:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{gkesoulisCineCubesCubesMovie2013,
  address = {{New York, NY, USA}},
  series = {{{DOLAP}} '13},
  title = {{{CineCubes}}: {{Cubes As Movie Stars}} with {{Little Effort}}},
  isbn = {978-1-4503-2412-0},
  shorttitle = {{{CineCubes}}},
  abstract = {In this paper we investigate how we can exploit the existence of a star schema in order to answer user OLAP queries with CineCube movies. Our method, implemented in an actual system, includes the following steps. The user submits a query over an underlying star schema. Taking this query as input, the system comes up with a set of queries complementing the information content of the original query, and executes them. Then, the system visualizes the query results and accompanies this presentation with a text commenting on the result highlights. Moreover, via a text-to-speech conversion the system automatically produces audio for the constructed text. Each combination of visualization, text and audio practically constitutes a cube movie, which is wrapped as a PowerPoint presentation and returned to the user.},
  booktitle = {Proceedings of the {{Sixteenth International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/gfz7nj},
  author = {Gkesoulis, Dimitrios and Vassiliadis, Panos},
  year = {2013},
  keywords = {olap,management of query results,query recommendation},
  pages = {3--10},
  file = {/home/yuri/Zotero/storage/F6R9EYU3/Gkesoulis_Vassiliadis_2013_CineCubes.pdf}
}
% == BibTeX quality report for gkesoulisCineCubesCubesMovie2013:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{cuzzocreaDataWarehousingOLAP2013,
  address = {{New York, NY, USA}},
  series = {{{DOLAP}} '13},
  title = {Data {{Warehousing}} and {{OLAP}} over {{Big Data}}: {{Current Challenges}} and {{Future Research Directions}}},
  isbn = {978-1-4503-2412-0},
  shorttitle = {Data {{Warehousing}} and {{OLAP}} over {{Big Data}}},
  abstract = {In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.},
  booktitle = {Proceedings of the {{Sixteenth International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/gfz7nf},
  author = {Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol},
  year = {2013},
  keywords = {data warehousing,big data,big multidimensional data,olap},
  pages = {67--70},
  file = {/home/yuri/Zotero/storage/FRAUWCHQ/Cuzzocrea et al_2013_Data Warehousing and OLAP over Big Data.pdf;/home/yuri/Zotero/storage/RAACE855/Cuzzocrea et al. - 2013 - Data Warehousing and OLAP over Big Data Current C.pdf}
}
% == BibTeX quality report for cuzzocreaDataWarehousingOLAP2013:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{marcelIntensionalAnswersOLAP2012,
  address = {{New York, NY, USA}},
  series = {{{DOLAP}} '12},
  title = {Towards {{Intensional Answers}} to {{OLAP Queries}} for {{Analytical Sessions}}},
  isbn = {978-1-4503-1721-4},
  abstract = {One of the problems in analyzing large multidimensional databases through OLAP sessions is that decision makers can be overwhelmed by the size of query answers, while they need a concise summary of data. Intensional query answering can help by providing a concise description of extensional answers (i.e., the sets of retrieved facts), generally relying on knowledge like integrity constraints, taxonomies, or patterns discovered from data. This paper proposes a framework for computing an intensional answer to an OLAP query by leveraging on the previous queries in the current session. Such intensional answer is concise and semantically rich, and allows the size of the extensional answers returned to be reduced, so as to achieve an effective trade-off between conciseness and informational content. After describing the general framework, we propose a specific instantiation that relies on previous contributions in cube modeling and intensional query answering.},
  booktitle = {Proceedings of the {{Fifteenth International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/gfz7nd},
  author = {Marcel, Patrick and Missaoui, Rokia and Rizzi, Stefano},
  year = {2012},
  keywords = {olap,intensional query answering},
  pages = {49--56},
  file = {/home/yuri/Zotero/storage/65563TSB/Marcel et al_2012_Towards Intensional Answers to OLAP Queries for Analytical Sessions.pdf}
}
% == BibTeX quality report for marcelIntensionalAnswersOLAP2012:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{derougemontApproximateAnswersOLAP2012,
  address = {{New York, NY, USA}},
  series = {{{DOLAP}} '12},
  title = {Approximate {{Answers}} to {{OLAP Queries}} on {{Streaming Data Warehouses}}},
  isbn = {978-1-4503-1721-4},
  abstract = {We study streaming data for a data warehouse, which combines different sources. We consider the relative answers to OLAP queries on a schema, as distributions with the L1 distance and approximate the answers without storing the entire data warehouse. We first study how to sample each source and combine the samples to approximate any OLAP query. We then consider a streaming context, where a data warehouse is built by streams of different sources. We first show a lower bound on the size of the memory necessary to approximate queries and then consider a statistical hypothesis where some attributes determine fixed distributions of the measure. We use the sampling methods to learn the statistical model and approximate OLAP queries. In this case, we approximate OLAP queries with a finite memory. We apply the method to a dataset which simulates the data of sensors, which provide weather parameters over time and locations from different sources.},
  booktitle = {Proceedings of the {{Fifteenth International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/gfz7nc},
  author = {De Rougemont, Michel and Cao, Phuong Thao},
  year = {2012},
  keywords = {approximation of olap query answering,data exchange,sampling algorithm,streaming data},
  pages = {121--128},
  file = {/home/yuri/Zotero/storage/Q2DE86I4/De Rougemont_Cao_2012_Approximate Answers to OLAP Queries on Streaming Data Warehouses.pdf}
}
% == BibTeX quality report for derougemontApproximateAnswersOLAP2012:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{cuzzocreaEnhancedClusteringComplex2012,
  address = {{New York, NY, USA}},
  series = {{{DOLAP}} '12},
  title = {Enhanced {{Clustering}} of {{Complex Database Objects}} in the {{Clustcube Framework}}},
  isbn = {978-1-4503-1721-4},
  abstract = {This paper significantly extends our previous research contribution [1], where we introduced the OLAP-based ClustCube framework for clustering and mining complex database objects extracted from distributed database settings. In particular, in this research we provide the following two novel contributions over [1]. First, we provide an innovative tree-based distance function over complex objects that takes into account the typical tree-like nature of these objects in distributed database settings. This novel distance is a relevant contribution over the simpler low-level-field-based distance presented in [1]. Second, we provide a comprehensive experimental campaign of ClustCube algorithms for computing ClustCube cubes, according to both performance metrics and accuracy metrics, against a well-known benchmark data set, and in comparison with a state-of-the-art subspace clustering algorithm for high-dimensional data. Retrieved results clearly demonstrate the superiority of our approach.},
  booktitle = {Proceedings of the {{Fifteenth International Workshop}} on {{Data Warehousing}} and {{OLAP}}},
  publisher = {{ACM}},
  doi = {10/gfz7nb},
  author = {Cuzzocrea, Alfredo and Serafino, Paolo},
  year = {2012},
  keywords = {integration of olap and data mining,knowledge discovery from olap data cubes,olap mining},
  pages = {129--136},
  file = {/home/yuri/Zotero/storage/UTTPW6LS/Cuzzocrea_Serafino_2012_Enhanced Clustering of Complex Database Objects in the Clustcube Framework.pdf}
}
% == BibTeX quality report for cuzzocreaEnhancedClusteringComplex2012:
% ? Title looks like it was stored in title-case in Zotero

@article{fosterOrbitDeterminationDifferentialdrag2015,
  title = {Orbit {{Determination}} and {{Differential}}-Drag {{Control}} of {{Planet Labs Cubesat Constellations}}},
  abstract = {We present methodology and mission results from orbit determination of Planet
Labs nanosatellites and differential-drag control of their relative motion.
Orbit determination (OD) is required on Planet Labs satellites to accurately
predict the positioning of satellites during downlink passes and we present a
scalable OD solution for large fleets of small satellites utilizing two-way
ranging. In the second part of this paper, we present mission results from
relative motion differential-drag control of a constellation of satellites
deployed in the same orbit.},
  language = {en},
  author = {Foster, Cyrus and Hallam, Henry and Mason, James},
  month = sep,
  year = {2015},
  keywords = {â No DOI found},
  file = {/home/yuri/Zotero/storage/34FMSR22/Foster et al_2015_Orbit Determination and Differential-drag Control of Planet Labs Cubesat.pdf;/home/yuri/Zotero/storage/7AUVJQHI/1509.html}
}
% == BibTeX quality report for fosterOrbitDeterminationDifferentialdrag2015:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@article{brandtDataReductionPipeline2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03067},
  primaryClass = {astro-ph},
  title = {Data {{Reduction Pipeline}} for the {{CHARIS Integral}}-{{Field Spectrograph I}}: {{Detector Readout Calibration}} and {{Data Cube Extraction}}},
  shorttitle = {Data {{Reduction Pipeline}} for the {{CHARIS Integral}}-{{Field Spectrograph I}}},
  abstract = {We present the data reduction pipeline for CHARIS, a high-contrast integral-field spectrograph for the Subaru Telescope. The pipeline constructs a ramp from the raw reads using the measured nonlinear pixel response, and reconstructs the data cube using one of three extraction algorithms: aperture photometry, optimal extraction, or \$\textbackslash{}chi\^2\$ fitting. We measure and apply both a detector flatfield and a lenslet flatfield and reconstruct the wavelength- and position-dependent lenslet point-spread function (PSF) from images taken with a tunable laser. We use these measured PSFs to implement a \$\textbackslash{}chi\^2\$-based extraction of the data cube, with typical residuals of \textasciitilde{}5\% due to imperfect models of the undersampled lenslet PSFs. The full two-dimensional residual of the \$\textbackslash{}chi\^2\$ extraction allows us to model and remove correlated read noise, dramatically improving CHARIS' performance. The \$\textbackslash{}chi\^2\$ extraction produces a data cube that has been deconvolved with the line-spread function, and never performs any interpolations of either the data or the individual lenslet spectra. The extracted data cube also includes uncertainties for each spatial and spectral measurement. CHARIS' software is parallelized, written in Python and Cython, and freely available on github with a separate documentation page. Astrometric and spectrophotometric calibrations of the data cubes and PSF subtraction will be treated in a forthcoming paper.},
  journal = {arXiv:1706.03067 [astro-ph]},
  author = {Brandt, Timothy D. and Rizzo, Maxime and Groff, Tyler and Chilcote, Jeffrey and Greco, Johnny P. and Kasdin, N. Jeremy and Limbach, Mary Anne and Galvin, Michael and Loomis, Craig and Knapp, Gillian and McElwain, Michael W. and Jovanovic, Nemanja and Currie, Thayne and Mede, Kyle and Tamura, Motohide and Takato, Naruhisa and Hayashi, Masahiko},
  month = jun,
  year = {2017},
  keywords = {â No DOI found,Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {/home/yuri/Zotero/storage/AT9TK3KH/Brandt et al_2017_Data Reduction Pipeline for the CHARIS Integral-Field Spectrograph I.pdf;/home/yuri/Zotero/storage/9PISEZJD/1706.html}
}
% == BibTeX quality report for brandtDataReductionPipeline2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1706.03067 [astro-ph]
% ? Title looks like it was stored in title-case in Zotero

@article{delmoroJP3DCompressionSolar2017,
  title = {{{JP3D}} Compression of Solar Data-Cubes: {{Photospheric}} Imaging and Spectropolarimetry},
  volume = {43},
  issn = {1572-9508},
  shorttitle = {{{JP3D}} Compression of Solar Data-Cubes},
  abstract = {Hyperspectral imaging is an ubiquitous technique in solar physics observations and the recent advances in solar instrumentation enabled us to acquire and record data at an unprecedented rate. The huge amount of data which will be archived in the upcoming solar observatories press us to compress the data in order to reduce the storage space and transfer times. The correlation present over all dimensions, spatial, temporal and spectral, of solar data-sets suggests the use of a 3D base wavelet decomposition, to achieve higher compression rates. In this work, we evaluate the performance of the recent JPEG2000 Part 10 standard, known as JP3D, for the lossless compression of several types of solar data-cubes. We explore the differences in: a) The compressibility of broad-band or narrow-band time-sequence; I or V Stokes profiles in spectropolarimetric data-sets; b) Compressing data in [x,y, {$\lambda$}] packages at different times or data in [x,y,t] packages of different wavelength; c) Compressing a single large data-cube or several smaller data-cubes; d) Compressing data which is under-sampled or super-sampled with respect to the diffraction cut-off.},
  language = {en},
  number = {1},
  journal = {Experimental Astronomy},
  doi = {10/gfz7m9},
  author = {Del Moro, Dario and Giovannelli, Luca and Pietropaolo, Ermanno and Berrilli, Francesco},
  month = feb,
  year = {2017},
  keywords = {Remote sensing,Hyperspectral compression,JP3D,JPEG2000,Lossless compression,Solar data},
  pages = {23-37},
  file = {/home/yuri/Zotero/storage/AAWKYQ99/Del Moro et al_2017_JP3D compression of solar data-cubes.pdf}
}

@article{sundararajanSimpleEfficientMapReduce2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.10072},
  primaryClass = {cs},
  title = {A {{Simple}} and {{Efficient MapReduce Algorithm}} for {{Data Cube Materialization}}},
  abstract = {Data cube materialization is a classical database operator introduced in Gray et al.\textasciitilde{}(Data Mining and Knowledge Discovery, Vol.\textasciitilde{}1), which is critical for many analysis tasks. Nandi et al.\textasciitilde{}(Transactions on Knowledge and Data Engineering, Vol.\textasciitilde{}6) first studied cube materialization for large scale datasets using the MapReduce framework, and proposed a sophisticated modification of a simple broadcast algorithm to handle a dataset with a 216GB cube size within 25 minutes with 2k machines in 2012. We take a different approach, and propose a simple MapReduce algorithm which (1) minimizes the total number of copy-add operations, (2) leverages locality of computation, and (3) balances work evenly across machines. As a result, the algorithm shows excellent performance, and materialized a real dataset with a cube size of 35.0G tuples and 1.75T bytes in 54 minutes, with 0.4k machines in 2014.},
  journal = {arXiv:1709.10072 [cs]},
  author = {Sundararajan, Mukund and Yan, Qiqi},
  month = sep,
  year = {2017},
  keywords = {â No DOI found,Computer Science - Databases},
  file = {/home/yuri/Zotero/storage/FQV2LWQ5/Sundararajan_Yan_2017_A Simple and Efficient MapReduce Algorithm for Data Cube Materialization.pdf;/home/yuri/Zotero/storage/2MSM5GDM/1709.html}
}
% == BibTeX quality report for sundararajanSimpleEfficientMapReduce2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1709.10072 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{etcheverryEfficientAnalyticalQueries2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.07213},
  primaryClass = {cs},
  title = {Efficient {{Analytical Queries}} on {{Semantic Web Data Cubes}}},
  abstract = {The amount of multidimensional data published on the semantic web (SW) is constantly increasing, due to initiatives such as Open Data and Open Government Data, among other ones. Models, languages, and tools, that allow to obtain valuable information efficiently, are thus required. Multidimensional data are typically represented as data cubes, and exploited using Online Analytical Processing (OLAP) techniques. The RDF Data Cube Vocabulary, also denoted QB, is the current W3C standard to represent statistical data on the SW.Since QB does not include key features needed for OLAP analysis, in previous work we have proposed an extension, denoted QB4OLAP, to overcome this problem without the need of modifying already published data. Once data cubes are represented on the SW, we need tools to analyze them. However, writing efficient analytical queries over SW cubes demands a deep knowledge of RDF and SPARQL. These skills are not common in typical analytical users. Also, OLAP languages like MDX are far from being easily understood by the final user. The lack of friendly tools to exploit multidimensional data on the SW is a barrier that needs to be broken to promote the publication of such data. We address this problem in this paper. Our approach is based on allowing analytical users to write queries using OLAP operations over cubes, without dealing with SW standards. For this, we devised CQL (standing for Cube Query Language), a simple, high-level query language that operates over cubes. Using the metadata provided by QB4OLAP, we translate CQL queries into SPARQL. Then, we propose query improvement strategies to produce efficient SPARQL queries, adapting SPARQL query optimization techniques. We evaluate our approach using the Star-Schema benchmark, showing that our proposal outperforms others. A web application that allows querying SW data cubes using CQL, completes our contributions.},
  journal = {arXiv:1703.07213 [cs]},
  author = {Etcheverry, Lorena and Vaisman, Alejandro A.},
  month = mar,
  year = {2017},
  keywords = {â No DOI found,Computer Science - Databases},
  file = {/home/yuri/Zotero/storage/XFK7C69P/Etcheverry_Vaisman_2017_Efficient Analytical Queries on Semantic Web Data Cubes.pdf;/home/yuri/Zotero/storage/2NNZSNKN/1703.html}
}
% == BibTeX quality report for etcheverryEfficientAnalyticalQueries2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1703.07213 [cs]
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{hilalSuperimposedMultidimensionalSchemas2017,
  title = {Superimposed Multidimensional Schemas for {{RDF}} Data Analysis},
  abstract = {Traditional data analysis employs online analytical processing (OLAP) systems operating on multidimensional (MD) data. The Resource Description Framework (RDF) serves as the foundation for the publication of a growing amount of semantic web data still largely untapped by data analysis. RDF data, however, do not typically follow an MD structure and, therefore, elude traditional OLAP. We propose an approach for superimposing MD structures over arbitrary RDF datasets. On top of that, we present a high-level querying mechanism to express MD queries, which can be automatically translated into SPARQL queries over the source data. As a consequence, data analysts that are unfamiliar with SPARQL may still incorporate RDF data sources into the analysis. Superimposed MD schemas also serve as foundation for Semantic Web Analysis Graphs which capture analysis processes for increased self-service capabilities.},
  booktitle = {2017 {{IEEE}} 14th {{International Scientific Conference}} on {{Informatics}}},
  doi = {10/gfz7m8},
  author = {Hilal, M. and Schuetz, C. G. and Schrefl, M.},
  month = nov,
  year = {2017},
  keywords = {Data models,data analysis,data mining,data warehouses,OLAP,query processing,Data analysis,multidimensional data,Resource description framework,arbitrary RDF datasets,capture analysis processes,data analysts,Database languages,graph theory,high-level querying mechanism,MD queries,MD structure,online analytical processing systems,RDF data analysis,RDF data sources,Resource Description Framework,semantic Web,Semantic Web Analysis Graphs,semantic web data,source data,SPARQL queries,superimposed MD schemas,superimposed multidimensional schemas,superimposing MD structures,Task analysis,Vocabulary},
  pages = {104-110},
  file = {/home/yuri/Zotero/storage/X72XJC4F/Hilal et al_2017_Superimposed multidimensional schemas for RDF data analysis.pdf;/home/yuri/Zotero/storage/4TIJ2TQP/8327230.html}
}
% == BibTeX quality report for hilalSuperimposedMultidimensionalSchemas2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{hurtadoCapturingSummarizabilityIntegrity2005,
  title = {Capturing {{Summarizability}} with {{Integrity Constraints}} in {{OLAP}}},
  volume = {30},
  issn = {0362-5915},
  abstract = {In multidimensional data models intended for online analytic processing (OLAP), data are viewed as points in a multidimensional space. Each dimension has structure, described by a directed graph of categories, a set of members for each category, and a child/parent relation between members. An important application of this structure is to use it to infer summarizability, that is, whether an aggregate view defined for some category can be correctly derived from a set of precomputed views defined for other categories. A dimension is called structurally heterogeneous if two members in a given category are allowed to have ancestors in different categories. In this article, we propose a class of integrity constraints, dimension constraints, that allow us to reason about summarizability in heterogeneous dimensions. We introduce the notion of frozen dimensions which are minimal homogeneous dimension instances representing the different structures that are implicitly combined in a heterogeneous dimension. Frozen dimensions provide the basis for efficiently testing the implication of dimension constraints and are a useful aid to understanding heterogeneous dimensions. We give a sound and complete algorithm for solving the implication of dimension constraints that uses heuristics based on the structure of the dimension and the constraints to speed up its execution. We study the intrinsic complexity of the implication problem and the running time of our algorithm.},
  number = {3},
  journal = {ACM Trans. Database Syst.},
  doi = {10/d8mbpk},
  author = {Hurtado, Carlos A. and Gutierrez, Claudio and Mendelzon, Alberto O.},
  month = sep,
  year = {2005},
  keywords = {data warehousing,OLAP,integrity constraints,query-optimization,summarizability},
  pages = {854--886},
  file = {/home/yuri/Zotero/storage/9WVALJLR/Hurtado et al_2005_Capturing Summarizability with Integrity Constraints in OLAP.pdf}
}
% == BibTeX quality report for hurtadoCapturingSummarizabilityIntegrity2005:
% ? Possibly abbreviated journal title ACM Trans. Database Syst.
% ? Title looks like it was stored in title-case in Zotero

@article{ciferriCubeAlgebraGeneric2013,
  title = {Cube {{Algebra}}: {{A Generic User}}-{{Centric Model}} and {{Query Language}} for {{OLAP Cubes}}},
  volume = {9},
  copyright = {Access limited to members},
  issn = {1548-3924},
  shorttitle = {Cube {{Algebra}}},
  language = {en},
  number = {2},
  journal = {International Journal of Data Warehousing and Mining (IJDWM)},
  doi = {10/gfz7mz},
  author = {Ciferri, Cristina and Ciferri, Ricardo and G{\'o}mez, Leticia and Schneider, Markus and Vaisman, Alejandro and Zim{\'a}nyi, Esteban},
  year = {2013},
  keywords = {\#nosource},
  pages = {39-65}
}
% == BibTeX quality report for ciferriCubeAlgebraGeneric2013:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{aziraniEfficientOLAPOperations2015,
  title = {Efficient {{OLAP}} Operations for {{RDF}} Analytics},
  abstract = {RDF is the leading data model for the Semantic Web, and dedicated query languages such as SPARQL 1.1, featuring in particular aggregation, allow extracting information from RDF graphs. A framework for analytical processing of RDF data was introduced in [1], where analytical schemas and analytical queries (cubes) are fully re-designed for heterogeneous, semantic-rich RDF graphs. In this novel analytical setting, we consider the following optimization problem: how to reuse the materialized result of a given RDF analytical query (cube) in order to compute the answer to another cube. We provide view-based rewriting algorithms for these cube transformations, and demonstrate experimentally their practical interest.},
  booktitle = {2015 31st {{IEEE International Conference}} on {{Data Engineering Workshops}}},
  doi = {10/gfz7m7},
  author = {Azirani, E. A. and Goasdou{\'e}, F. and Manolescu, I. and Roati{\c s}, A.},
  month = apr,
  year = {2015},
  keywords = {data analysis,data mining,Data warehouses,Context,Resource description framework,SQL,Semantics,Q measurement,graph theory,semantic Web,analytical schemas,Blogs,Cities and towns,cube transformation,data model,data models,dedicated query languages,heterogeneous graph,information extraction,OLAP operation,RDF analytical query,RDF analytics,RDF data,semantic-rich RDF graph,SPARQL 1.1,view-based rewriting algorithm},
  pages = {71-76},
  file = {/home/yuri/Zotero/storage/3JUEFWEI/Azirani et al_2015_Efficient OLAP operations for RDF analytics.pdf;/home/yuri/Zotero/storage/F77ZP3G6/7129548.html}
}
% == BibTeX quality report for aziraniEfficientOLAPOperations2015:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{etcheverryEnhancingOLAPAnalysis2012,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Enhancing {{OLAP Analysis}} with {{Web Cubes}}},
  isbn = {978-3-642-30284-8},
  abstract = {Traditional OLAP tools have proven to be successful in analyzing large sets of enterprise data. For today's business dynamics, sometimes these highly curated data is not enough. External data (particularly web data), may be useful to enhance local analysis. In this paper we discuss the extraction of multidimensional data from web sources, and their representation in RDFS. We introduce Open Cubes, an RDFS vocabulary for the specification and publication of multidimensional cubes on the Semantic Web, and show how classical OLAP operations can be implemented over Open Cubes using SPARQL 1.1, without the need of mapping the multidimensional information to the local database (the usual approach to multidimensional analysis of Semantic Web data). We show that our approach is plausible for the data sizes that can usually be retrieved to enhance local data repositories.},
  language = {en},
  booktitle = {The {{Semantic Web}}: {{Research}} and {{Applications}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Etcheverry, Lorena and Vaisman, Alejandro A.},
  editor = {Simperl, Elena and Cimiano, Philipp and Polleres, Axel and Corcho, Oscar and Presutti, Valentina},
  year = {2012},
  keywords = {Resource Description Framework,Open Cube,Resource Description Framework Data,Resource Description Framework Triple,SPARQL Query},
  pages = {469-483},
  file = {/home/yuri/Zotero/storage/EDNKNJ34/Etcheverry_Vaisman_2012_Enhancing OLAP Analysis with Web Cubes.pdf}
}
% == BibTeX quality report for etcheverryEnhancingOLAPAnalysis2012:
% ? Title looks like it was stored in title-case in Zotero

@article{nadvornikTimeSeriesCube2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.01393},
  primaryClass = {astro-ph},
  title = {Time {{Series Cube Data Model}}},
  abstract = {The purpose of this document is to create a data model and its serialization for expressing generic time series data. Already existing IVOA data models are reused as much as possible. The model is also made as generic as possible to be open to new extensions but at the same time closed for modifications. This enables maintaining interoperability throughout different versions of the data model. We define the necessary building blocks for metadata discovery, serialization of time series data and understanding it by clients. We present several categories of time series science cases with examples of implementation. We also take into account the most pressing topics for time series providers like tracking original images for every individual point of a light curve or time-derived axes like frequency for gravitational wave analysis. The main motivation for the creation of a new model is to provide a unified time series data publishing standard - not only for light curves but also more generic time series data, e.g., radial velocity curves, power spectra, hardness ratio, provenance linkage, etc. The flexibility is the most crucial part of our model - we are not dependent on any physical domain or frame models. While images or spectra are already stable and standardized products, the time series related domains are still not completely evolved and new ones will likely emerge in near future. That is why we need to keep models like Time Series Cube DM independent of any underlying physical models. In our opinion, this is the only correct and sustainable way for future development of IVOA standards.},
  journal = {arXiv:1702.01393 [astro-ph]},
  author = {Nadvornik, Jiri and Skoda, Petr and Morris, Dave and Tvrdik, Pavel},
  month = feb,
  year = {2017},
  keywords = {â No DOI found,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Databases},
  file = {/home/yuri/Zotero/storage/84CCG7Z9/Nadvornik et al_2017_Time Series Cube Data Model.pdf;/home/yuri/Zotero/storage/U3AXT9PT/1702.html}
}
% == BibTeX quality report for nadvornikTimeSeriesCube2017:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1702.01393 [astro-ph]
% ? Title looks like it was stored in title-case in Zotero

@article{etcheverryModelingQueryingData2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.06080},
  primaryClass = {cs},
  title = {Modeling and {{Querying Data Cubes}} on the {{Semantic Web}}},
  abstract = {The web is changing the way in which data warehouses are designed, used, and queried. With the advent of initiatives such as Open Data and Open Government, organizations want to share their multidimensional data cubes and make them available to be queried online. The RDF data cube vocabulary (QB), the W3C standard to publish statistical data in RDF, presents several limitations to fully support the multidimensional model. The QB4OLAP vocabulary extends QB to overcome these limitations, allowing to im- plement the typical OLAP operations, such as rollup, slice, dice, and drill-across using standard SPARQL queries. In this paper we introduce a formal data model where the main object is the data cube, and define OLAP operations using this model, independent of the underlying representation of the cube. We show then that a cube expressed using our model can be represented using the QB4OLAP vocabulary, and finally we provide a SPARQL implementation of OLAP operations over data cubes in QB4OLAP.},
  journal = {arXiv:1512.06080 [cs]},
  author = {Etcheverry, Lorena and Gomez, Silvia Silvia and Vaisman, Alejandro},
  month = dec,
  year = {2015},
  keywords = {â No DOI found,Computer Science - Databases},
  file = {/home/yuri/Zotero/storage/S58SRM6G/Etcheverry et al_2015_Modeling and Querying Data Cubes on the Semantic Web.pdf;/home/yuri/Zotero/storage/G8LZDWVI/1512.html}
}
% == BibTeX quality report for etcheverryModelingQueryingData2015:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1512.06080 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{menezesTreatmentProcedureVLT2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.02933},
  title = {A Treatment Procedure for {{VLT}}/{{SINFONI}} Data Cubes: Application to {{NGC}} 5643},
  volume = {450},
  issn = {0035-8711, 1365-2966},
  shorttitle = {A Treatment Procedure for {{VLT}}/{{SINFONI}} Data Cubes},
  abstract = {In this second paper of a series, we present a treatment procedure for data cubes obtained with the Spectrograph for Integral Field Observations in the Near Infrared of the Very Large Telescope. We verified that the treatment procedure improves significantly the quality of the images of the data cubes, allowing a more detailed analysis. The images of the Br\$\textbackslash{}gamma\$ and H\$\_2 \textbackslash{}lambda 21218\$ emission lines from the treated data cube of the nuclear region of NGC 5643 reveal the existence of ionized and molecular-gas clouds around the nucleus, which cannot be seen clearly in the images from the non-treated data cube of this galaxy. The ionized-gas clouds represent the narrow-line region, in the form of a bicone. We observe a good correspondence between the positions of the ionized-gas clouds in the Br\$\textbackslash{}gamma\$ image and in an [O III] image, obtained with the Hubble Space Telescope, of the nuclear region of this galaxy convolved with an estimate of the point-spread function of the data cube of NGC 5643. The morphologies of the ionized and molecular gas seem to be compatible with the existence of a molecular torus/disc that collimates the active galactic nucleus (AGN) emission. The molecular gas may also flow along this torus/disc, feeding the AGN. This scenario is compatible with the unified model for AGNs.},
  number = {1},
  journal = {Monthly Notices of the Royal Astronomical Society},
  doi = {10/gfz7m6},
  author = {Menezes, R. B. and {da Silva}, Patr{\'i}cia and Ricci, T. V. and Steiner, J. E. and May, D. and Borges, B. W.},
  month = jun,
  year = {2015},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Astrophysics - Astrophysics of Galaxies},
  pages = {369-396},
  file = {/home/yuri/Zotero/storage/5RBAFVEU/Menezes et al_2015_A treatment procedure for VLT-SINFONI data cubes.pdf;/home/yuri/Zotero/storage/WI4RVET4/1504.html}
}

@article{wangScalableDataCube2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.5663},
  primaryClass = {cs},
  title = {Scalable {{Data Cube Analysis}} over {{Big Data}}},
  abstract = {Data cubes are widely used as a powerful tool to provide multidimensional views in data warehousing and On-Line Analytical Processing (OLAP). However, with increasing data sizes, it is becoming computationally expensive to perform data cube analysis. The problem is exacerbated by the demand of supporting more complicated aggregate functions (e.g. CORRELATION, Statistical Analysis) as well as supporting frequent view updates in data cubes. This calls for new scalable and efficient data cube analysis systems. In this paper, we introduce HaCube, an extension of MapReduce, designed for efficient parallel data cube analysis on large-scale data by taking advantages from both MapReduce (in terms of scalability) and parallel DBMS (in terms of efficiency). We also provide a general data cube materialization algorithm which is able to facilitate the features in MapReduce-like systems towards an efficient data cube computation. Furthermore, we demonstrate how HaCube supports view maintenance through either incremental computation (e.g. used for SUM or COUNT) or recomputation (e.g. used for MEDIAN or CORRELATION). We implement HaCube by extending Hadoop and evaluate it based on the TPC-D benchmark over billions of tuples on a cluster with over 320 cores. The experimental results demonstrate the efficiency, scalability and practicality of HaCube for cube analysis over a large amount of data in a distributed environment.},
  journal = {arXiv:1311.5663 [cs]},
  author = {Wang, Zhengkui and Chu, Yan and Tan, Kian-Lee and Agrawal, Divyakant and Abbadi, Amr EI and Xu, Xiaolong},
  month = nov,
  year = {2013},
  keywords = {â No DOI found,Computer Science - Databases},
  file = {/home/yuri/Zotero/storage/UMHAHH5Y/Wang et al_2013_Scalable Data Cube Analysis over Big Data.pdf;/home/yuri/Zotero/storage/QGW7J6BH/1311.html}
}
% == BibTeX quality report for wangScalableDataCube2013:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1311.5663 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{aouicheWebOLAPData2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0905.2657},
  primaryClass = {cs},
  title = {Web 2.0 {{OLAP}}: {{From Data Cubes}} to {{Tag Clouds}}},
  volume = {18},
  shorttitle = {Web 2.0 {{OLAP}}},
  abstract = {Increasingly, business projects are ephemeral. New Business Intelligence tools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds are a popular community-driven visualization technique. Hence, we investigate tag-cloud views with support for OLAP operations such as roll-ups, slices, dices, clustering, and drill-downs. As a case study, we implemented an application where users can upload data and immediately navigate through its ad hoc dimensions. To support social networking, views can be easily shared and embedded in other Web sites. Algorithmically, our tag-cloud views are approximate range top-k queries over spontaneous data cubes. We present experimental evidence that iceberg cuboids provide adequate online approximations. We benchmark several browser-oblivious tag-cloud layout optimizations.},
  journal = {arXiv:0905.2657 [cs]},
  doi = {10/c9ks49},
  author = {Aouiche, Kamel and Lemire, Daniel and Godin, Robert},
  year = {2009},
  keywords = {Computer Science - Databases},
  pages = {51-64},
  file = {/home/yuri/Zotero/storage/JGCC4BWS/Aouiche et al_2009_Web 2.pdf;/home/yuri/Zotero/storage/QF8DSMWL/0905.html}
}
% == BibTeX quality report for aouicheWebOLAPData2009:
% Missing required field 'number'
% ? Possibly abbreviated journal title arXiv:0905.2657 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{grayDataCubeRelational2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cs/0701155},
  title = {Data {{Cube}}: {{A Relational Aggregation Operator Generalizing Group}}-{{By}}, {{Cross}}-{{Tab}}, and {{Sub}}-{{Totals}}},
  shorttitle = {Data {{Cube}}},
  abstract = {Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, cross-tabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.},
  journal = {arXiv:cs/0701155},
  author = {Gray, Jim and Chaudhuri, Surajit and Bosworth, Adam and Layman, Andrew and Reichart, Don and Venkatrao, Murali and Pellow, Frank and Pirahesh, Hamid},
  month = jan,
  year = {2007},
  keywords = {â No DOI found,Computer Science - Databases},
  file = {/home/yuri/Zotero/storage/H3CWT27L/Gray et al_2007_Data Cube.pdf;/home/yuri/Zotero/storage/3HYWTFJG/0701155.html}
}
% == BibTeX quality report for grayDataCubeRelational2007:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Title looks like it was stored in title-case in Zotero

@article{webbPruningAttributeValues2008,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0805.0747},
  primaryClass = {cs},
  title = {Pruning {{Attribute Values From Data Cubes}} with {{Diamond Dicing}}},
  abstract = {Data stored in a data warehouse are inherently multidimensional, but most data-pruning techniques (such as iceberg and top-k queries) are unidimensional. However, analysts need to issue multidimensional queries. For example, an analyst may need to select not just the most profitable stores or--separately--the most profitable products, but simultaneous sets of stores and products fulfilling some profitability constraints. To fill this need, we propose a new operator, the diamond dice. Because of the interaction between dimensions, the computation of diamonds is challenging. We present the first diamond-dicing experiments on large data sets. Experiments show that we can compute diamond cubes over fact tables containing 100 million facts in less than 35 minutes using a standard PC.},
  journal = {arXiv:0805.0747 [cs]},
  author = {Webb, Hazel and Kaser, Owen and Lemire, Daniel},
  month = may,
  year = {2008},
  keywords = {â No DOI found,Computer Science - Databases,Computer Science - Data Structures and Algorithms},
  file = {/home/yuri/Zotero/storage/DDLCBRPX/Webb et al_2008_Pruning Attribute Values From Data Cubes with Diamond Dicing.pdf;/home/yuri/Zotero/storage/NA5VSFP3/0805.html}
}
% == BibTeX quality report for webbPruningAttributeValues2008:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:0805.0747 [cs]
% ? Title looks like it was stored in title-case in Zotero

@article{vanahalliEfficientParallelRow2018,
  title = {An Efficient Parallel Row Enumerated Algorithm for Mining Frequent Colossal Closed Itemsets from High Dimensional Datasets},
  issn = {0020-0255},
  abstract = {Mining colossal itemsets from high dimensional datasets have gained focus in recent times. The conventional algorithms expend most of the time in mining small and mid-sized itemsets, which do not enclose valuable and complete information for decision making. Mining Frequent Colossal Closed Itemsets (FCCI) from a high dimensional dataset play a highly significant role in decision making for many applications, especially in the field of bioinformatics. To mine FCCI from a high dimensional dataset, the existing preprocessing techniques fail to prune the complete set of irrelevant features and irrelevant rows. Besides, the state-of-the-art algorithms for the same are sequential and computationally expensive. The proposed work highlights an Effective Improved Parallel Preprocessing (EIPP) technique to prune the complete set of irrelevant features and irrelevant rows from high dimensional dataset and a novel efficient Parallel Frequent Colossal Closed Itemset Mining (PFCCIM) algorithm. Further, the PFCCIM algorithm is integrated with a novel Rowset Cardinality Table (RCT), an efficient method to check the closeness of a rowset and also an efficient pruning strategy to cut down the mining search space. The proposed PFCCIM algorithm is the first parallel algorithm to mine FCCI from a high dimensional dataset. The performance study shows the improved effectiveness of the proposed EIPP technique over the existing preprocessing techniques and the improved efficiency of the proposed PFCCIM algorithm over the existing algorithms.},
  journal = {Information Sciences},
  doi = {10/gfz7m5},
  author = {Vanahalli, Manjunath K and Patil, Nagamma},
  month = aug,
  year = {2018},
  keywords = {Bioinformatics,Colossal closed itemsets,High dimensional datasets,Parallel algorithm,Parallel preprocessing,Rowset cardinality table},
  file = {/home/yuri/Zotero/storage/U474QGFG/Vanahalli_Patil_2018_An efficient parallel row enumerated algorithm for mining frequent colossal.pdf;/home/yuri/Zotero/storage/FBASLJ4D/S0020025518306145.html}
}
% == BibTeX quality report for vanahalliEfficientParallelRow2018:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@article{dokaOnlineQueryingDdimensional2011,
  title = {Online Querying of D-Dimensional Hierarchies},
  volume = {71},
  issn = {0743-7315},
  abstract = {In this paper we describe a distributed system designed to efficiently store, query and update multidimensional data organized into concept hierarchies and dispersed over a network. Our system employs an adaptive scheme that automatically adjusts the level of indexing according to the granularity of the incoming queries, without assuming any prior knowledge of the workload. Efficient roll-up and drill-down operations take place in order to maximize the performance by minimizing query flooding. Updates are performed on-line, with minimal communication overhead, depending on the level of consistency needed. Extensive experimental evaluation shows that, on top of the advantages that a distributed storage offers, our method answers the vast majority of incoming queries, both point and aggregate ones, without flooding the network and without causing significant storage or load imbalance. Our scheme proves to be especially efficient in cases of skewed workloads, even when these change dynamically with time. At the same time, it manages to preserve the hierarchical nature of data. To the best of our knowledge, this is the first attempt towards the support of concept hierarchies in DHTs.},
  number = {3},
  journal = {Journal of Parallel and Distributed Computing},
  doi = {10/c4cwrv},
  author = {Doka, Katerina and Tsoumakos, Dimitrios and Koziris, Nectarios},
  month = mar,
  year = {2011},
  keywords = {Data warehousing,Peer-to-Peer,Concept hierarchies,Distributed hash tables},
  pages = {424-437},
  file = {/home/yuri/Zotero/storage/G7YPPD75/Doka et al_2011_Online querying of d-dimensional hierarchies.pdf;/home/yuri/Zotero/storage/NJPQZNZE/S0743731510002030.html}
}

@incollection{ramosDistributedSystemsPerformance2016,
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  title = {Distributed {{Systems Performance}} for {{Big Data}}},
  isbn = {978-3-319-32467-8},
  abstract = {This paper describes a methodology for working with distributed systems, and achieve performance in Big Data, through the framework Hadoop, Python programming language, and Apache Hive module. The efficiency of the proposed methodology is tested through a case study that addresses a real problem found in the supercomputing environment of the Center for Weather Forecasting and Climate Studies linked to the Brazilian Institute for Space Research (CPTEC / INPE), which provides Society a work able to predict disasters and save people lives. In all three experiments involving the issue, using the Cray XT-6 supercomputer: (i) the first issue involves programming in Python and a sequential and monoprocessed arquitecture; (ii) the second uses Python and Hadoop framework, over parallel and distributed arquitecture; (iii) the latter combines Hadoop and Hive in a parallel and distributed arquitecture. The main results of these experiments are compared, discussed, and topics beyond the scope in this research are exposed as recommendations and suggestions for future work.},
  language = {en},
  booktitle = {Information {{Technology}}: {{New Generations}}},
  publisher = {{Springer International Publishing}},
  author = {Ramos, Marcelo Paiva and Tasinaffo, Paulo Marcelo and {de Almeida}, Eugenio Sper and Achite, Luis Marcelo and {da Cunha}, Adilson Marques and Dias, Luiz Alberto Vieira},
  editor = {Latifi, Shahram},
  year = {2016},
  keywords = {Big Data,Hadoop,Climate prediction,Cluster HPC,Distributed systems,Hive,Python},
  pages = {733-744},
  file = {/home/yuri/Zotero/storage/XA9V3JIK/ITNG1_DistributedSystemsPerformanceForBigData.pdf}
}
% == BibTeX quality report for ramosDistributedSystemsPerformance2016:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{baptistaAircraftOnconditionReliability2017,
  address = {{Big Sky, MT, USA}},
  title = {Aircraft On-Condition Reliability Assessment Based on Data-Intensive Analytics},
  isbn = {978-1-5090-1613-6},
  abstract = {The implementation of condition-based maintenance continues to face several challenges especially in the aeronautics field. While it is true that time-based maintenance dominates the industry today, it is believed that condition monitoring could yield promising results with a better compromise on cost over effectiveness in the long run. The aim of condition-based monitoring in aeronautics is, based on the available system data (e.g., flight, event and maintenance data), to evaluate the current health state of an aircraft component and to estimate its remaining useful life. Several approaches have been studied in condition-based maintenance with the most promising being data-driven modeling. This paper proposes a comparison of a set of data-driven modeling techniques to perform prognostics on a critical component of the jet engine bleed system. The novelty of our work is twofold. First, we perform this comparative study on a real case study of a critical valve of the aircraft bleed system. Fielded data from different data sources are used in the models. To our knowledge, this is the first case study that merges data from the computer central maintenance system (fault messages), maintenance data, and flight data on a prognostics system. Second, a variety of data-driven techniques are compared from neural nets to regression support machines. The models are compared using the standard metrics of absolute, mean, and squared errors. A regressive accuracy curve is also used to compare the models along different prediction window sizes. The results show the best model comprised information from all data sources. The data that most contributed to the performance improvement was the maintenance, flight and fault data, in this order. This result comes to reinforce the notion that it is more difficult to extract quantitative information from fault events than flight data with data-driven regressive methods.},
  language = {en},
  booktitle = {2017 {{IEEE Aerospace Conference}}},
  publisher = {{IEEE}},
  doi = {10/gfz7m4},
  author = {Baptista, Marcia L. and {de Medeiros}, Ivo P. and Malere, Joao P. and Nascimento, Cairo L. and Prendinger, Helmut and Henriques, Elsa},
  month = mar,
  year = {2017},
  pages = {1-12},
  file = {/home/yuri/Zotero/storage/CYD5BVSG/Baptista et al. - 2017 - Aircraft on-condition reliability assessment based.pdf}
}
% == BibTeX quality report for baptistaAircraftOnconditionReliability2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{mariaApplyingScrumInterdisciplinary2015,
  address = {{Las Vegas, NV, USA}},
  title = {Applying {{Scrum}} in an {{Interdisciplinary Project Using Big Data}}, {{Internet}} of {{Things}}, and {{Credit Cards}}},
  isbn = {978-1-4799-8828-0},
  abstract = {This paper describes the use of Scrum in a collaborative software project. It applies agile software development, big data, and internet of things to reduce frauds within credit card operations. It reports the experience of students from three different courses taking the graduate program in Electronics and Computer Engineering at the Brazilian Aeronautics Institute of Technology (Instituto Tecnologico de Aeronautica - ITA) during the first semester of 2014. The major contribution of this work is the application of Interdisciplinary Problem Based Learning, where students have worked asynchronously and geographically dispersed to deliver valuable increments, aiming to solve related problems. This work was performed during four project sprints on seventeen weeks. The main output was working software, developed and tested using Scrum and its best practices. At the end, a big data environment was used as a transparent way of fulfilling the needs of financial transaction analysts.},
  language = {en},
  booktitle = {2015 12th {{International Conference}} on {{Information Technology}} - {{New Generations}}},
  publisher = {{IEEE}},
  doi = {10/gfz7m3},
  author = {Maria, Rene Esteves and Junior, Luiz Antonio Rodrigues and de Vasconcelos, Luiz Eduardo Guarino and Pinto, Adriano Fonseca Mancilha and Tsoucamoto, Paulo Takachi and Silva, Henrique Nunweiler Angelim and Lastori, Airton and da Cunha, Adilson Marques and Dias, Luiz Alberto Vieira},
  month = apr,
  year = {2015},
  pages = {67-72},
  file = {/home/yuri/Zotero/storage/WLSGWZZK/Maria et al. - 2015 - Applying Scrum in an Interdisciplinary Project Usi.pdf}
}
% == BibTeX quality report for mariaApplyingScrumInterdisciplinary2015:
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@phdthesis{monteiroFRAMEWORKTRAJECTORYDATA2017,
  title = {A {{FRAMEWORK FOR TRAJECTORY DATA MINING}}},
  language = {en},
  author = {Monteiro, Diego Vilela},
  year = {2017},
  keywords = {â No DOI found},
  file = {/home/yuri/Zotero/storage/MTB5PEQH/Monteiro - A FRAMEWORK FOR TRAJECTORY DATA MINING.pdf}
}
% == BibTeX quality report for monteiroFRAMEWORKTRAJECTORYDATA2017:
% Missing required field 'school'
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{SimoesCamaQuei:2018:DaAnMa,
  title = {Sits: Data Analysis and Machine Learning Using Satellite Image Time Series},
  language = {English},
  booktitle = {Resumos...},
  author = {Sim{\~o}es, Rolf Ezequiel de Oliveira and Camara, Gilberto and de Queiroz, Gilberto Ribeiro},
  year = {2018},
  keywords = {machine learning,\#nosource,â No DOI found,land cover   classification,open source software.,satellite image time series},
  pages = {18},
  organization = {{Workshop de Computa{\c c}{\~a}o Aplicada, 18. (WORCAP)}},
  affiliation = {Instituto Nacional de Pesquisas Espaciais (INPE) and Instituto Nacional de Pesquisas Espaciais (INPE) and Instituto Nacional de Pesquisas Espaciais (INPE)},
  conference-location = {S{\~a}o Jos{\'e} dos Campos, SP},
  conference-year = {21-23 ago.},
  targetfile = {simoesâits.pdf}
}
% == BibTeX quality report for SimoesCamaQuei:2018:DaAnMa:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{liHighdimensionalOLAPMinimal2004,
  series = {{{VLDB}} '04},
  title = {High-Dimensional {{OLAP}}: {{A Minimal Cubing Approach}}},
  isbn = {978-0-12-088469-8},
  shorttitle = {High-Dimensional {{OLAP}}},
  abstract = {Data cube has been playing an essential role in fast OLAP (online analytical processing) in many multi-dimensional data warehouses. However, there exist data sets in applications like bioinformatics, statistics, and text processing that are characterized by high dimensionality, e.g., over 100 dimensions, and moderate size, e.g., around 106 tuples. No feasible data cube can be constructed with such data sets. In this paper we will address the problem of developing an efficient algorithm to perform OLAP on such data sets. Experience tells us that although data analysis tasks may involve a high dimensional space, most OLAP operations are performed only on a small number of dimensions at a time. Based on this observation, we propose a novel method that computes a thin layer of the data cube together with associated value-list indices. This layer, while being manageable in size, will be capable of supporting flexible and fast OLAP operations in the original high dimensional space. Through experiments we will show that the method has I/O costs that scale nicely with dimensionality. Furthermore, the costs are comparable to that of accessing an existing data cube when full materialization is possible.},
  booktitle = {Proceedings of the {{Thirtieth International Conference}} on {{Very Large Data Bases}} - {{Volume}} 30},
  publisher = {{VLDB Endowment}},
  author = {Li, Xiaolei and Han, Jiawei and Gonzalez, Hector},
  year = {2004},
  keywords = {â No DOI found},
  pages = {528--539},
  file = {/home/yuri/Zotero/storage/36IDHGUH/Li et al. - High-Dimensional OLAP A Minimal Cubing Approach.pdf;/home/yuri/Zotero/storage/GAHI9ZXF/Li et al_2004_High-dimensional OLAP.pdf;/home/yuri/Zotero/storage/S79MCTEC/Li et al. - High-Dimensional OLAP A Minimal Cubing Approach.pdf}
}

@article{silvaComputingBIGData2016,
  title = {Computing {{BIG}} Data Cubes with Hybrid Memory},
  volume = {11},
  abstract = {Nowadays, analysis data volumes are reaching critical sizes challenging traditional data warehousing approaches. Cubing methods based on inverted indices, such as Frag-Cubing, are efficient alternatives to conventional approaches of computing OLAP data cubes over Big Data. However, similar to other memory-based cube solutions, the efficiency of such methods is constrained by available dynamic random-access memory (DRAM). In this paper, we implement and test the hybrid inverted cubing (HIC) method, which adopts a hybrid memory system, with main goal of able to compute and update BIG data cubes (with high dimensionality and high number of tuples). HIC stores the most frequent attribute values in DRAM; the remaining attribute values are retained in external memory. Tests using a relation with 480 dimensions and 107 tuples show that HIC is three times slower than Frag-Cubing when computing a data cube, and approximately 13 times faster than Frag-Cubing when answering complex cube queries. A BIG data cube with 60 dimensions and 109 tuples was computed by HIC using 110 GB of RAM and 286 GB of external memory, while Frag-Cubing could not compute such a cube in same machine.},
  language = {en},
  number = {1},
  journal = {Journal of Convergence Information Technology},
  author = {Silva, Rodrigo Rocha and Hirata, Celso Massaki and Lima, Joubert de Castro},
  month = jan,
  year = {2016},
  keywords = {â No DOI found},
  pages = {18},
  file = {/home/yuri/Zotero/storage/AAA24NU7/Silva et al. - Computing BIG data cubes with hybrid memory.pdf}
}

@article{zakiRAREMiningColossal2018,
  title = {{{RARE}}: {{Mining}} Colossal Closed Itemset in High Dimensional Data},
  volume = {161},
  issn = {0950-7051},
  shorttitle = {{{RARE}}},
  abstract = {The present society has been sculpted into a continuous data generator. In fact, the massive automatic data collection has generated a new genre of dataset, termed as `high-dimensional data', which is characterized by a relatively small number of rows, in comparison to that of large number of columns (or dimensions). Among the vast data mining tasks, association rules have been extensively employed so as to describe the correlations between the variables found in a dataset. The task of mining association rules highly relies on the efficiency of the algorithms to extract all frequent itemsets that exist in the database. The focus towards improving run time and memory consumption of algorithms is strongly influenced by search strategies, effective pruning strategies, and the method of closure checking. Neither depth- nor breadth-first search may exert any variance without these techniques, mainly because the search space appears similar. With that, this paper investigated the strategies implemented in both row and column enumeration-based algorithms, hence proposing the RARE; a breadth-first bottom-up row-enumeration algorithm, in mining colossal closed itemsets in high-dimensional data.},
  journal = {Knowledge-Based Systems},
  doi = {10/gf2cj9},
  author = {Zaki, Fatimah Audah Md. and Zulkurnain, Nurul Fariza},
  month = dec,
  year = {2018},
  keywords = {Data mining,Closed itemset,High-dimensional data},
  pages = {1-11},
  file = {/home/yuri/Zotero/storage/YBFFDNIE/Zaki_Zulkurnain_2018_RARE.pdf;/home/yuri/Zotero/storage/U8NF9ZKT/S0950705118303769.html}
}
% == BibTeX quality report for zakiRAREMiningColossal2018:
% Missing required field 'number'

@article{caniupanEfficientRepairDimension2015,
  title = {Efficient Repair of Dimension Hierarchies under Inconsistent Reclassification},
  volume = {95},
  issn = {0169-023X},
  abstract = {On-Line Analytical Processing (OLAP) dimensions are usually modeled as a set of elements connected by a hierarchical relationship. To ensure summarizability, a dimension is required to be strict, that is, every element of the dimension must have a unique ancestor in each of its ancestor categories. In practice, elements in a dimension are often reclassified, meaning that their rollups are changed. After this operation the dimension may become non-strict. To fix this problem, we propose to compute a set of minimal r-repairs for the new non-strict dimension. Each minimal r-repair is a strict dimension that keeps the result of the reclassification, and is obtained by performing a minimum number of insertions and deletions to the dimension graph. We show that, although in the general case finding an r-repair is NP-complete, for real-world hierarchy schemas, computing such repairs can be done in polynomial time. Further, we propose efficient heuristic-based algorithms for computing r-repairs, and discuss their computational complexity. We also perform experiments over synthetic and real-world dimensions to show the plausibility of our approach.},
  journal = {Data \& Knowledge Engineering},
  doi = {10/gf2cj8},
  author = {Caniup{\'a}n, M{\'o}nica and Vaisman, Alejandro and Arredondo, Ra{\'u}l},
  month = jan,
  year = {2015},
  keywords = {OLAP,Data warehousing,Dimension hierarchies,Repairs,Updates},
  pages = {1-22},
  file = {/home/yuri/Zotero/storage/KR2AMQ9B/CaniupÃ¡n et al_2015_Efficient repair of dimension hierarchies under inconsistent reclassification.pdf;/home/yuri/Zotero/storage/MXYN23BY/S0169023X15000026.html}
}
% == BibTeX quality report for caniupanEfficientRepairDimension2015:
% Missing required field 'number'

@inproceedings{rahmanFrameworkImplementingJoin2019,
  title = {A {{Framework}} for {{Implementing Join Operation}} between {{Multiple MOLAPs}}},
  abstract = {In this modern era, a data warehouse is keenly interested in storing data in the form of OLAP(Online Analytical Processing). OLAP is widely used nowadays as it extends special aid to a user, regarding viewing a set of data from various views. To facilitate the analysis further, a multidimensional database is required which is ably supported by MOLAP, a special kind of OLAP. We can extract data from a single MOLAP in different possible ways. There have been many kinds of research to make this analytical process more efficient. But the research area, of extracting data from multiple MOLAPs, has been explored hardly. However, we have proposed a new model for faster querying in multiple MOLAPs using snowflake schema. This model uses array index reference, for creating a MOLAP by joining two-dimensional databases. Thus we can derive summarized MOLAP from fundamental MOLAP. Then we have used summarized MOLAP for creating a fact table, which preserves the summarized data. Our model provides a satisfactory result after comparing it with the MySQL database result.},
  booktitle = {2019 {{International Conference}} on {{Electrical}}, {{Computer}} and {{Communication Engineering}} ({{ECCE}})},
  doi = {10/gf2cj7},
  author = {Rahman, M. D. and Hossain, M. A. and Hasan, K. M. A.},
  month = feb,
  year = {2019},
  keywords = {Data models,Computational modeling,Indexes,Data Cube,data mining,data warehouses,OLAP,Arrays,online analytical processing,Production,data warehouse,Data warehouses,SQL,array index reference,MDBMS,MOLAP,multidimensional database,MySQL,MySQL database,ROLAP,snowflake schema,Snowflake schema},
  pages = {1-6},
  file = {/home/yuri/Zotero/storage/Y56P7ZZ9/Rahman et al_2019_A Framework for Implementing Join Operation between Multiple MOLAPs.pdf;/home/yuri/Zotero/storage/RKII2953/8679373.html}
}
% == BibTeX quality report for rahmanFrameworkImplementingJoin2019:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@article{toniniContributionQualityMaturity2008,
  title = {Contribution of Quality and Maturity Models to Software Process Improvement},
  volume = {18},
  issn = {0103-6513},
  number = {2},
  journal = {Production},
  doi = {10/fp6zp4},
  author = {Tonini, Antonio Carlos and de Carvalho, Marly Monteiro and Spinola, Mauro de Mesquita},
  year = {2008},
  pages = {275-286},
  file = {/home/yuri/Zotero/storage/B546WTQ5/Tonini et al_2008_Contribution of quality and maturity models to software process improvement.pdf;/home/yuri/Zotero/storage/G5FRRR3L/scielo.html}
}

@article{robertoOrganizacoesEconomicosociaisOs2007,
  title = {As Organiza{\c c}{\~o}es Econ{\'o}mico-Sociais e Os Seus Stakeholders},
  volume = {12},
  issn = {0873-7444},
  number = {2},
  journal = {Economia Global e Gest{\~a}o},
  author = {Roberto, Jos{\'e} Afonso and Serrano, Ant{\'o}nio},
  month = aug,
  year = {2007},
  keywords = {â No DOI found},
  pages = {73-93},
  file = {/home/yuri/Zotero/storage/75DRDPJ5/Roberto_Serrano_2007_As organizaÃ§Ãµes econÃ³mico-sociais e os seus stakeholders.pdf;/home/yuri/Zotero/storage/2UWLAMHC/scielo.html}
}

@article{bimonteOpenIssuesBig2016,
  title = {Open Issues in {{Big Data Warehouse}} Design},
  abstract = {Data Warehouse and OLAP systems allow analyzing huge volumes of data represented according to the multidimensional model. In the era of Big Data, NoSQL systems have been proved to be an effective Business Intelligence solution. Some works recently study warehousing and OLAPing Big Data. (Un)Lucky these works exclusively investigate time performance related to the Volume and Velocity features of Big Data. Therefore, in this paper we investigate the impact of other Big Data features: Variety, Veracity and Value on warehousing and OLAP analysis. Then, we go beyond computation performance and we highlight new Big Data Warehouses design issues.},
  language = {en},
  journal = {Revue des Nouvelles Technologies de l'Information},
  author = {Bimonte, Sandro},
  year = {2016},
  keywords = {â No DOI found},
  pages = {10},
  file = {/home/yuri/Zotero/storage/7KKSVPRJ/Bimonte - Open issues in Big Data Warehouse design.pdf}
}
% == BibTeX quality report for bimonteOpenIssuesBig2016:
% Missing required field 'number'
% Missing required field 'volume'

@article{floridiWhatFutureArtificial2019,
  title = {What the {{Near Future}} of {{Artificial Intelligence Could Be}}},
  volume = {32},
  issn = {2210-5441},
  language = {en},
  number = {1},
  journal = {Philosophy \& Technology},
  doi = {10/gfw86p},
  author = {Floridi, Luciano},
  month = mar,
  year = {2019},
  pages = {1-15},
  file = {/home/yuri/Zotero/storage/9XXAUANJ/Floridi_2019_What the Near Future of Artificial Intelligence Could Be.pdf}
}
% == BibTeX quality report for floridiWhatFutureArtificial2019:
% ? Title looks like it was stored in title-case in Zotero

@article{ponchateauEntrepotDonneesDans2016,
  title = {{Entrep{\^o}t de Donn{\'e}es dans l'{\`e}re Data Science : De la Donn{\'e}e au Mod{\`e}le}},
  language = {fr},
  author = {Ponchateau, Cyrille and Bellatreche, Ladjel and Baron, Mickael},
  year = {2016},
  keywords = {â No DOI found},
  pages = {16},
  file = {/home/yuri/Zotero/storage/BG6R9PKW/Ponchateau et al. - EntrepÃ´t de DonnÃ©es dans lâÃ¨re Data Science  De l.pdf}
}
% == BibTeX quality report for ponchateauEntrepotDonneesDans2016:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'volume'

@article{ponchateauConceptionExploitationBase2018,
  title = {{Conception et exploitation d'une base de mod{\`e}les : application aux data sciences}},
  shorttitle = {{Conception et exploitation d'une base de mod{\`e}les}},
  abstract = {Les sciences exp{\'e}rimentales font r{\'e}guli{\`e}rement usage de s{\'e}ries chronologiques, pour repr{\'e}senter certains des r{\'e}sultats exp{\'e}rimentaux, qui consistent en listes chronologiques de valeurs (index{\'e}es par le temps), g{\'e}n{\'e}ralement fournies par des capteurs reli{\'e}s {\`a} un syst{\`e}me (objet de l'exp{\'e}rience). Ces s{\'e}ries sont analys{\'e}es dans le but d'obtenir un mod{\`e}le math{\'e}matique permettant de d{\'e}crire les donn{\'e}es et ainsi comprendre et expliquer le comportement du syst{\`e}me {\'e}tudi{\'e}. De nos jours, les technologies de stockage et analyse de s{\'e}ries chronologiques sont nombreuses et matures, en revanche, quant au stockage et {\`a} la gestion de mod{\`e}les math{\'e}matiques et leur mise en lien avec des donn{\'e}es num{\'e}riques exp{\'e}rimentales, les solutions existantes sont {\`a} la fois r{\'e}centes, moins nombreuses et moins abouties. Or,les mod{\`e}les math{\'e}matiques jouent un r{\^o}le essentiel dans l'interpr{\'e}tation et la validation des r{\'e}sultats exp{\'e}rimentaux. Un syst{\`e}me de stockage ad{\'e}quat permettrait de faciliter leur gestion et d'am{\'e}liorer leur r{\'e}-utilisabilit{\'e}. L'objectif de ce travail est donc de d{\'e}velopper une base de mod{\`e}les permettant la gestion de mod{\`e}le math{\'e}matiques et de fournir un syst{\`e}me de \guillemotleft{} requ{\^e}te par les donn{\'e}es \guillemotright, afin d'aider {\`a} retrouver/reconna{\^i}tre un mod{\`e}le {\`a} partir d'un profil num{\'e}rique exp{\'e}rimental. Dans cette th{\`e}se, je pr{\'e}sente donc la conception (de la mod{\'e}lisation des donn{\'e}es, jusqu'{\`a} l'architecture logicielle) de la base de mod{\`e}les et les extensions qui permettent de r{\'e}aliser le syst{\`e}me de \guillemotleft{} requ{\^e}te par les donn{\'e}es \guillemotright. Puis, je pr{\'e}sente le prototype de la base de mod{\`e}le que j'ai impl{\'e}ment{\'e}, ainsi que les r{\'e}sultats obtenus {\`a} l'issu des tests de ce-dernier.},
  language = {fr},
  author = {Ponchateau, Cyrille},
  month = oct,
  year = {2018},
  keywords = {â No DOI found},
  file = {/home/yuri/Zotero/storage/HL3S38LU/Ponchateau_2018_Conception et exploitation d'une base de modÃ¨les.pdf;/home/yuri/Zotero/storage/X4NYKTWV/tel-01939430.html}
}
% == BibTeX quality report for ponchateauConceptionExploitationBase2018:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@book{berger-wolfProceedings2019SIAM2019,
  address = {{Philadelphia, PA}},
  title = {Proceedings of the 2019 {{SIAM International Conference}} on {{Data Mining}}},
  isbn = {978-1-61197-567-3},
  abstract = {Recent studies have demonstrated inspiring success in leveraging geo-tagged social media data for applications such as event detection, location recommendation and mobile healthcare. However, in most real-life social media streams, only a small percentage of data have explicit geo-location metadata, which hinders the power of social media from being fully unleashed.},
  language = {en},
  publisher = {{Society for Industrial and Applied Mathematics}},
  editor = {{Berger-Wolf}, Tanya and Chawla, Nitesh},
  month = may,
  year = {2019},
  file = {/home/yuri/Zotero/storage/KBHZR6Z4/Berger-Wolf and Chawla - 2019 - Proceedings of the 2019 SIAM International Confere.pdf},
  doi = {10.1137/1.9781611975673}
}
% == BibTeX quality report for berger-wolfProceedings2019SIAM2019:
% Missing required field 'author'
% ? Title looks like it was stored in title-case in Zotero

@book{changShinyWebApplication2019,
  title = {Shiny: {{Web Application Framework}} for {{R}}},
  author = {Chang, Winston and Cheng, Joe and Allaire, J. J. and Xie, Yihui and McPherson, Jonathan},
  year = {2019}
}
% == BibTeX quality report for changShinyWebApplication2019:
% Missing required field 'publisher'

@article{liRankingContinuousProbabilistic2010,
  title = {Ranking Continuous Probabilistic Datasets},
  volume = {3},
  issn = {21508097},
  language = {en},
  number = {1-2},
  journal = {Proceedings of the VLDB Endowment},
  doi = {10/gf2qds},
  author = {Li, Jian and Deshpande, Amol},
  month = sep,
  year = {2010},
  pages = {638-649},
  file = {/home/yuri/Zotero/storage/A46ZLGDU/Li and Deshpande - 2010 - Ranking continuous probabilistic datasets.pdf}
}

@inproceedings{loOLAPSequenceData2008,
  address = {{New York, NY, USA}},
  series = {{{SIGMOD}} '08},
  title = {{{OLAP}} on {{Sequence Data}}},
  isbn = {978-1-60558-102-6},
  abstract = {Many kinds of real-life data exhibit logical ordering among their data items and are thus sequential in nature. However, traditional online analytical processing (OLAP) systems and techniques were not designed for sequence data and they are incapable of supporting sequence data analysis. In this paper, we propose the concept of Sequence OLAP, or S-OLAP for short. The biggest distinction of S-OLAP from traditional OLAP is that a sequence can be characterized not only by the attributes' values of its constituting items, but also by the subsequence/substring patterns it possesses. This paper studies many aspects related to Sequence OLAP. The concepts of sequence cuboid and sequence data cube are introduced. A prototype S-OLAP system is built in order to validate the proposed concepts. The prototype is able to support "pattern-based" grouping and aggregation, which is currently not supported by any OLAP system. The implementation details of the prototype system as well as experimental results are presented.},
  booktitle = {Proceedings of the 2008 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  publisher = {{ACM}},
  doi = {10/d2d92s},
  author = {Lo, Eric and Kao, Ben and Ho, Wai-Shing and Lee, Sau Dan and Chui, Chun Kit and Cheung, David W.},
  year = {2008},
  keywords = {olap,sequence data},
  pages = {649--660},
  file = {/home/yuri/Zotero/storage/UG9DWEFR/Lo et al_2008_OLAP on Sequence Data.pdf}
}
% == BibTeX quality report for loOLAPSequenceData2008:
% ? Title looks like it was stored in title-case in Zotero

@article{aiAssociationRuleMining2018,
  title = {Association Rule Mining Algorithms on High-Dimensional Datasets},
  volume = {23},
  issn = {1614-7456},
  abstract = {The science of bioinformatics has been accelerating at a fast pace, introducing more features and handling bigger volumes. However, these swift changes have, at the same time, posed challenges to data mining applications, in particular efficient association rule mining. Many data mining algorithms for high-dimensional datasets have been put forward, but the sheer numbers of these algorithms with varying features and application scenarios have complicated making suitable choices. Therefore, we present a general survey of multiple association rule mining algorithms applicable to high-dimensional datasets. The main characteristics and relative merits of these algorithms are explained, as well, pointing out areas for improvement and optimization strategies that might be better adapted to high-dimensional datasets, according to previous studies. Generally speaking, association rule mining algorithms that merge diverse optimization methods with advanced computer techniques can better balance scalability and interpretability.},
  language = {en},
  number = {3},
  journal = {Artificial Life and Robotics},
  doi = {10/gf2qdr},
  author = {Ai, Dongmei and Pan, Hongfei and Li, Xiaoxin and Gao, Yingxin and He, Di},
  month = sep,
  year = {2018},
  keywords = {Association rule mining,Data mining algorithms,Frequent itemset mining,High-dimensional datasets},
  pages = {420-427},
  file = {/home/yuri/Zotero/storage/7IMVM25L/Prasanna and Seetha - ASSOCIATION RULE MINING ALGORITHMS FOR HIGH DIMENS.pdf;/home/yuri/Zotero/storage/HIGYTJNZ/Ai et al_2018_Association rule mining algorithms on high-dimensional datasets.pdf}
}

@article{silvaQCubeEfficientIntegration2013,
  title = {{{qCube}}: {{Efficient}} Integration of Range Query Operators over a High Dimension Data Cube},
  volume = {4},
  shorttitle = {{{qCube}}},
  number = {3},
  journal = {JIDM},
  author = {Silva, Rodrigo Rocha and Lima, Joubert de Castro and Hirata, Celso Massaki},
  year = {2013},
  keywords = {â No DOI found},
  pages = {469--482}
}

@inproceedings{silvaHybridMemoryData2015,
  title = {A {{Hybrid Memory Data Cube Approach}} for {{High Dimension Relations}}},
  isbn = {978-989-758-096-3},
  booktitle = {{{ICEIS}} 2015 - {{Proceedings}} of the 17th {{International Conference}} on {{Enterprise Information Systems}}, {{Volume}} 1, {{Barcelona}}, {{Spain}}, 27-30 {{April}}, 2015},
  publisher = {{SciTePress}},
  doi = {10/gf2qdq},
  author = {Silva, Rodrigo Rocha and Hirata, Celso Massaki and Lima, Joubert de Castro},
  editor = {Hammoudi, Slimane and Maciaszek, Leszek A. and Teniente, Ernest},
  year = {2015},
  pages = {139--149}
}
% == BibTeX quality report for silvaHybridMemoryData2015:
% ? Title looks like it was stored in title-case in Zotero

@book{kimballDataWarehouseToolkit2013,
  address = {{Indianapolis, IN}},
  edition = {Edi{\c c}{\~a}o: 3rd},
  title = {{The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling}},
  isbn = {978-1-118-53080-1},
  shorttitle = {{The Data Warehouse Toolkit}},
  abstract = {Updated new edition of Ralph Kimball's groundbreaking book on dimensional modeling for data warehousing and business intelligence! The first edition of Ralph Kimball's~The Data Warehouse Toolkit~introduced the industry to dimensional modeling, and now his books are considered the most authoritative guides in this space. This new third edition is a complete library of updated dimensional modeling techniques, the most comprehensive collection ever. It covers new and enhanced star schema dimensional modeling patterns, adds two new chapters on ETL techniques, includes new and expanded business matrices for 12 case studies, and more.  Authored by Ralph Kimball and Margy Ross, known worldwide as educators, consultants, and influential thought leaders in data warehousing and business intelligence Begins with fundamental design recommendations and progresses through increasingly complex scenarios Presents unique modeling techniques for business applications such as inventory management, procurement, invoicing, accounting, customer relationship management, big data analytics, and more Draws real-world case studies from a variety of industries, including retail sales, financial services, telecommunications, education, health care, insurance, e-commerce, and more  Design dimensional databases that are easy to understand and provide fast query response with~The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling, 3rd Edition.},
  language = {Ingl{\^e}s},
  publisher = {{John Wiley \& Sons}},
  author = {Kimball, Ralph and Ross, Margy},
  year = {2013}
}
% == BibTeX quality report for kimballDataWarehouseToolkit2013:
% ? Title looks like it was stored in title-case in Zotero

@book{inmonUsingDataWarehouse1994,
  address = {{Somerset, NJ, USA}},
  title = {Using the {{Data Warehouse}}},
  isbn = {978-0-471-05966-0},
  publisher = {{Wiley-QED Publishing}},
  author = {Inmon, W. H. and Hackathorn, Richard D.},
  year = {1994}
}
% == BibTeX quality report for inmonUsingDataWarehouse1994:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{moreiraFullPartialData2012,
  title = {Full and Partial Data Cube Computation and Representation over Commodity {{PCs}}},
  abstract = {The PnP (Pipe 'n Prune) approach is considered one of the most promising approaches for partial cube computation over distributed memory computer architectures, however it generates a huge amount of redundant data. In general, PnP does not consider data uniformity, named skew, when partitioning its workload and, thus, it imposes a maximum data redundancy even with uniform data. Due to this scenario, we implement P2CDM (acronym of Parallel Cube Computation with Distributed Memory) approach which has minimized communication and low data redundancy. Globally, at the entire cluster, P2CDM automatically generates data redundancy only for skewed values among all dimensions of a Data Warehouse. Locally, at each host, P2CDM provides cube cells pruning using MCG approach. The result is a distributed approach that computes massive full or partial data cubes over a cluster of commodity PCs. The experiments demonstrated that both approaches have similar speedup, but P2CDM approach is 20-25\% faster and consumes 30-40\% less memory at each host of the cluster, when compared to PnP approach.},
  booktitle = {2012 {{IEEE}} 13th {{International Conference}} on {{Information Reuse Integration}} ({{IRI}})},
  doi = {10/gf2vgq},
  author = {Moreira, A. A. and Lima, J. d C.},
  month = aug,
  year = {2012},
  keywords = {data redundancy,data structures,data warehouses,Redundancy,Distributed databases,Memory management,data warehouse,commodity PC,cube cells pruning,distributed memory computer architecture,MCG approach,P2CDM,parallel cube computation,partial data cube computation,partial data cube representation,pipe 'n prune approach,PnP,Runtime,Silver},
  pages = {672-679},
  file = {/home/yuri/Zotero/storage/9RXLPVU4/Moreira_Lima_2012_Full and partial data cube computation and representation over commodity PCs.pdf;/home/yuri/Zotero/storage/4N2FC2WV/6303074.html}
}
% == BibTeX quality report for moreiraFullPartialData2012:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{hanStreamCubeArchitecture2005,
  title = {Stream {{Cube}}: {{An Architecture}} for {{Multi}}-{{Dimensional Analysis}} of {{Data Streams}}},
  volume = {18},
  issn = {0926-8782},
  shorttitle = {Stream {{Cube}}},
  abstract = {Real-time surveillance systems, telecommunication systems, and other dynamic environments often generate tremendous (potentially infinite) volume of stream data: the volume is too huge to be scanned multiple times. Much of such data resides at rather low level of abstraction, whereas most analysts are interested in relatively high-level dynamic changes (such as trends and outliers). To discover such high-level characteristics, one may need to perform on-line multi-level, multi-dimensional analytical processing of stream data. In this paper, we propose an architecture, called stream\_cube, to facilitate on-line, multi-dimensional, multi-level analysis of stream data.For fast online multi-dimensional analysis of stream data, three important techniques are proposed for efficient and effective computation of stream cubes. First, a tilted time frame model is proposed as a multi-resolution model to register time-related data: the more recent data are registered at finer resolution, whereas the more distant data are registered at coarser resolution. This design reduces the overall storage of time-related data and adapts nicely to the data analysis tasks commonly encountered in practice. Second, instead of materializing cuboids at all levels, we propose to maintain a small number of critical layers. Flexible analysis can be efficiently performed based on the concept of observation layer and minimal interesting layer. Third, an efficient stream data cubing algorithm is developed which computes only the layers (cuboids) along a popular path and leaves the other cuboids for query-driven, on-line computation. Based on this design methodology, stream data cube can be constructed and maintained incrementally with a reasonable amount of memory, computation cost, and query response time. This is verified by our substantial performance study.},
  number = {2},
  journal = {Distrib. Parallel Databases},
  doi = {10/c43gmk},
  author = {Han, Jiawei and Chen, Yixin and Dong, Guozhu and Pei, Jian and Wah, Benjamin W. and Wang, Jianyong and Cai, Y. Dora},
  month = sep,
  year = {2005},
  pages = {173--197},
  file = {/home/yuri/Zotero/storage/46NKPE9J/Han et al_2005_Stream Cube.pdf}
}
% == BibTeX quality report for hanStreamCubeArchitecture2005:
% ? Possibly abbreviated journal title Distrib. Parallel Databases
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{coddProvidingOlapUseranalysts1998,
  title = {Providing Olap to User-Analysts: An It Mandate},
  shorttitle = {Providing Olap to User-Analysts},
  abstract = {Overview Recently, there has been a great deal of discussion in the trade press and elsewhere regarding the coexistence of so-called transaction databases with decision support systems. These discussions usually revolve around the argument that the physical design required for acceptable performance of each is incompatible and that therefore, data should be stored redundantly in multiple enterprise databases: one for transaction processing, and the other for decision support type activities. Also, these same arguments usually confuse physical schema with logical and conceptual schema.},
  author = {Codd, E. F. and Codd, Sharon and Salley, Curtis},
  year = {1998},
  keywords = {â No DOI found,Autopilot,Business analytics,Closing (morphology),Coexist (image),Conceptual schema,Copyright,Decision support system,Essbase,Fax,GeForce 9 series,HL7PublishingSubSection <operations>,Hyperion,Inclusion Body Myositis (disorder),Name,Online analytical processing,Operations research,Physical data model,Physical design (electronics),Published Database,Registration,Server (computer),Transaction processing},
  file = {/home/yuri/Zotero/storage/XE5FWBBT/Codd et al_1998_Providing olap to user-analysts.pdf}
}
% == BibTeX quality report for coddProvidingOlapUseranalysts1998:
% Missing required field 'booktitle'
% Missing required field 'pages'
% Missing required field 'publisher'

@phdthesis{zotero-1100,
  type = {Phdthesis}
}
% == BibTeX quality report for zotero-1100:
% Missing required field 'author'
% Missing required field 'school'
% Missing required field 'title'
% Missing required field 'year'

@phdthesis{limaSEQUENTIALPARALLELAPPROACHES2009,
  address = {{S{\~a}o Jos{\'e} dos Campos}},
  title = {{{SEQUENTIAL AND PARALLEL APPROACHES TO REDUCE THE DATA CUBE SIZE}}},
  language = {en},
  school = {Instituto Tecnol{\'o}gico de Aeron{\'a}utica},
  author = {Lima, Joubert de Castro},
  year = {2009},
  keywords = {â No DOI found},
  file = {/home/yuri/Zotero/storage/97BNQYVM/Lima - SEQUENTIAL AND PARALLEL APPROACHES TO REDUCE THE D.PDF}
}
% == BibTeX quality report for limaSEQUENTIALPARALLELAPPROACHES2009:
% ? Title looks like it was stored in title-case in Zotero

@misc{PDFComputingBIG,
  title = {(1) ({{PDF}}) {{Computing BIG Data Cubes}} with {{Hybrid Memory}}},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  language = {en},
  journal = {ResearchGate},
  howpublished = {https://www.researchgate.net/publication/292964073\_Computing\_BIG\_Data\_Cubes\_with\_Hybrid\_Memory},
  file = {/home/yuri/Zotero/storage/764967LD/292964073_Computing_BIG_Data_Cubes_with_Hybrid_Memory.html}
}
% == BibTeX quality report for PDFComputingBIG:
% Missing required field 'author'
% Missing required field 'year'
% ? Title looks like it was stored in title-case in Zotero

@article{silvaComputingDataCubes2018,
  title = {Computing Data Cubes over {{GPU}} Clusters.},
  copyright = {open access},
  abstract = {O cubo de dados {\'e} um operador relacional fundamental para sistemas de suporte {\`a} tomada de decis{\~a}o, dessa forma {\'u}til para a an{\'a}lise de Big Data. O problema apresentando nesse trabalho {\'e}: como reduzir os tempos de resposta de consultas multidimensionais complexas? Tal problema se torna ainda mais agravado se atualiza{\c c}{\~o}es recorrentes nos dados de entrada acontecem e se existe um grande volume de dados de alta dimensionalidade a ser analisado. A hip{\'o}tese deste trabalho {\'e} que uso de clusters de dispositivos CPU-GPU acelerar{\'a} consultas em cubos de dados hol{\'i}sticos de alta dimens{\~a}o que s{\~a}o constantemente atualizados. A solu{\c c}{\~a}o alternativa proposta neste trabalho, chamada de JCL-GPU-Cubing, particiona a base de dados em m{\'u}ltiplas representa{\c c}{\~o}es de cubos parciais sem introduzir redund{\^a}ncia de dados. Tais cubos parciais s{\~a}o usados para executar consultas em CPU ou CPU-GPU de maneira eficiente. As avalia{\c c}{\~o}es experimentais preliminares demonstraram que a vers{\~a}o baseada em clusters de CPU escala bem quando ambos os dados de entrada e o tamanho do cluster aumentam.},
  language = {en\_US},
  author = {Silva, Lucas Henrique Moreira},
  year = {2018},
  keywords = {â No DOI found},
  file = {/home/yuri/Zotero/storage/G3RNI2E8/Silva_2018_Computing data cubes over GPU clusters.pdf}
}
% == BibTeX quality report for silvaComputingDataCubes2018:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@inproceedings{beyerBottomupComputationSparse1999,
  address = {{New York, NY, USA}},
  series = {{{SIGMOD}} '99},
  title = {Bottom-up {{Computation}} of {{Sparse}} and {{Iceberg CUBE}}},
  isbn = {978-1-58113-084-3},
  abstract = {We introduce the Iceberg-CUBE problem as a reformulation of the datacube (CUBE) problem. The Iceberg-CUBE problem is to compute only those group-by partitions with an aggregate value (e.g., count) above some minimum support threshold. The result of Iceberg-CUBE can be used (1) to answer group-by queries with a clause such as HAVING COUNT(*) {$>$}= X, where X is greater than the threshold, (2) for mining multidimensional association rules, and (3) to complement existing strategies for identifying interesting subsets of the CUBE for precomputation.
We present a new algorithm (BUC) for Iceberg-CUBE computation. BUC builds the CUBE bottom-up; i.e., it builds the CUBE by starting from a group-by on a single attribute, then a group-by on a pair of attributes, then a group-by on three attributes, and so on. This is the opposite of all techniques proposed earlier for computing the CUBE, and has an important practical advantage: BUC avoids computing the larger group-bys that do not meet minimum support. The pruning in BUC is similar to the pruning in the Apriori algorithm for association rules, except that BUC trades some pruning for locality of reference and reduced memory requirements. BUC uses the same pruning strategy when computing sparse, complete CUBEs.
We present a thorough performance evaluation over a broad range of workloads. Our evaluation demonstrates that (in contrast to earlier assumptions) minimizing the aggregations or the number of sorts is not the most important aspect of the sparse CUBE problem. The pruning in BUC, combined with an efficient sort method, enables BUC to outperform all previous algorithms for sparse CUBEs, even for computing entire CUBEs, and to dramatically improve Iceberg-CUBE computation.},
  booktitle = {Proceedings of the 1999 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  publisher = {{ACM}},
  doi = {10.1145/304182.304214},
  author = {Beyer, Kevin and Ramakrishnan, Raghu},
  year = {1999},
  pages = {359--370},
  file = {/home/yuri/Zotero/storage/MXPZ8HAI/Beyer_Ramakrishnan_1999_Bottom-up Computation of Sparse and Iceberg CUBE.pdf}
}
% == BibTeX quality report for beyerBottomupComputationSparse1999:
% ? Title looks like it was stored in title-case in Zotero

@article{yixinchenRegressionCubesLossless2006,
  title = {Regression {{Cubes}} with {{Lossless Compression}} and {{Aggregation}}},
  volume = {18},
  issn = {1041-4347},
  abstract = {As OLAP engines are widely used to support multidimensional data analysis, it is desirable to support in data cubes advanced statistical measures, such as regression and filtering, in addition to the traditional simple measures such as count and average. Such new measures allow users to model, smooth, and predict the trends and patterns of data. Existing algorithms for simple distributive and algebraic measures are inadequate for efficient computation of statistical measures in a multidimensional space. In this paper, we propose a fundamentally new class of measures, compressible measures, in order to support efficient computation of the statistical models. For compressible measures, we compress each cell into an auxiliary matrix with a size independent of the number of tuples. We can then compute the statistical measures for any data cell from the compressed data of the lower-level cells without accessing the raw data. Time- and space-efficient lossless aggregation formulae are derived for regression and filtering measures. Our analytical and experimental studies show that the resulting system, regression cube, substantially reduces the memory usage and the overall response time for statistical analysis of multidimensional data},
  number = {12},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  doi = {10.1109/TKDE.2006.196},
  author = {{Yixin Chen} and {Guozhu Dong} and {Jiawei Han} and {Jian Pei} and {Benjamin W. Wah} and {Jianyong Wang}},
  month = dec,
  year = {2006},
  pages = {1585-1599},
  file = {/home/yuri/Zotero/storage/7NDLAMX8/Yixin Chen et al_2006_Regression Cubes with Lossless Compression and Aggregation.pdf;/home/yuri/Zotero/storage/T4KBC9I9/Yixin Chen et al_2006_Regression Cubes with Lossless Compression and Aggregation.pdf;/home/yuri/Zotero/storage/5BJBM6V9/1717417.html;/home/yuri/Zotero/storage/WMEXDKAZ/1717417.html}
}
% == BibTeX quality report for yixinchenRegressionCubesLossless2006:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{dongxinCCubingEfficientComputation2006,
  title = {C-{{Cubing}}: {{Efficient Computation}} of {{Closed Cubes}} by {{Aggregation}}-{{Based Checking}}},
  shorttitle = {C-{{Cubing}}},
  abstract = {It is well recognized that data cubing often produces huge outputs. Two popular efforts devoted to this problem are (1) iceberg cube, where only significant cells are kept, and (2) closed cube, where a group of cells which preserve roll-up/drill-down semantics are losslessly compressed to one cell. Due to its usability and importance, efficient computation of closed cubes still warrants a thorough study. In this paper, we propose a new measure, called closedness, for efficient closed data cubing. We show that closedness is an algebraic measure and can be computed efficiently and incrementally. Based on closedness measure, we develop an an aggregation-based approach, called C-Cubing (i.e., Closed-Cubing), and integrate it into two successful iceberg cubing algorithms: MM-Cubing and Star-Cubing. Our performance study shows that C-Cubing runs almost one order of magnitude faster than the previous approaches. We further study how the performance of the alternative algorithms of C-Cubing varies w.r.t the properties of the data sets.},
  booktitle = {22nd {{International Conference}} on {{Data Engineering}} ({{ICDE}}'06)},
  doi = {10.1109/ICDE.2006.31},
  author = {{Dong Xin} and {Zheng Shao} and {Jiawei Han} and {Hongyan Liu}},
  month = apr,
  year = {2006},
  keywords = {Relational databases,Data engineering,Usability},
  pages = {4-4},
  file = {/home/yuri/Zotero/storage/47LPJ6CD/Dong Xin et al_2006_C-Cubing.pdf;/home/yuri/Zotero/storage/L2BS4SXP/Dong Xin et al_2006_C-Cubing.pdf;/home/yuri/Zotero/storage/76R54SYT/1617372.html;/home/yuri/Zotero/storage/RXMDQVWA/1617372.html}
}
% == BibTeX quality report for dongxinCCubingEfficientComputation2006:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{lakshmananQuotientCubeHow2002,
  series = {{{VLDB}} '02},
  title = {Quotient {{Cube}}: {{How}} to {{Summarize}} the {{Semantics}} of a {{Data Cube}}},
  shorttitle = {Quotient {{Cube}}},
  abstract = {Partitioning a data cube into sets of cells with "similar behavior" often better exposes the semantics in the cube. E.g., if we find that average boots sales in the West 10th store of Walmart was the same for winter as for the whole year, it signifies something interesting about the trend of boots sales in that location in that year. In this paper, we are interested in finding succinct summaries of the data cube, exploiting regularities present in the cube, with a clear basis. We would like the summary: (i) to be as concise as possible, (ii) to itself form a lattice preserving the rollup/drilldown semantics of the cube, and (iii) to allow the original cube to be fully recovered. We illustrate the utility of solving this problem and discuss the inherent challenges. We develop techniques for partitioning cube cells for obtaining succinct summaries, and introduce the quotient cube. We give efficient algorithms for computing it from a base table. For monotone aggregate functions (e.g., COUNT, MIN, MAX, SUM on non-negative measures, etc.), our solution is optimal (i.e., quotient cube of the least size). For nonmonotone functions (e.g., AVG), we obtain a locally optimal solution. We experimentally demonstrate the efficacy of our ideas and techniques and the scalability of our algorithms.},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Very Large Data Bases}}},
  publisher = {{VLDB Endowment}},
  doi = {10.1016/B978-155860869-6/50074-3},
  author = {Lakshmanan, Laks V. S. and Pei, Jian and Han, Jiawei},
  year = {2002},
  pages = {778--789},
  file = {/home/yuri/Zotero/storage/YLXY34CQ/Lakshmanan et al_2002_Quotient Cube.pdf}
}
% == BibTeX quality report for lakshmananQuotientCubeHow2002:
% ? Title looks like it was stored in title-case in Zotero

@book{larsonSpaceMissionAnalysis1999,
  address = {{El Segundo, Calif. : Dordrecht ; Boston}},
  edition = {3rd edition},
  title = {Space {{Mission Analysis}} and {{Design}}, 3rd Edition},
  isbn = {978-1-881883-10-4},
  abstract = {This practical handbook for Space Mission Engineering draws on leading aerospace experts to carry readers through mission design, from orbit selection to ground ops. SMAD III updates the technology, provides greater emphasis on small spacecraft design and the cost-reduction process, and includes more detail on multi-satellite manufacturing, space computers, payload design and autonomous systems.},
  language = {English},
  publisher = {{Microcosm}},
  editor = {Larson, Wiley J. and Wertz, James R.},
  month = oct,
  year = {1999}
}
% == BibTeX quality report for larsonSpaceMissionAnalysis1999:
% Missing required field 'author'

@article{kragCmSpaceDebris2017,
  title = {A 1~Cm Space Debris Impact onto the {{Sentinel}}-{{1A}} Solar Array},
  volume = {137},
  issn = {0094-5765},
  abstract = {Sentinel-1A is a 2-ton spacecraft of the Copernicus Earth observation program operated by ESA's Space Operations Centre in Darmstadt, Germany. Sentinel-1A and its sister spacecraft Sentinel-1B operate in a sun-synchronous orbit at about 700~km altitude. On 2016/08/23 17:07:37 UTC, Sentinel-1A suffered from an anomaly resulting in a sudden permanent partial power loss and significant impulsive orbit and attitude changes. A deeper investigation identified that an impulsive orbit change against flight direction of 0.7~mm/s, estimated at the time of the event, gave the best results in terms of GPS residuals. At the same time, a peak attitude off-pointing of 0.7\textdegree{} (around the spacecraft yaw axis) and peak attitude rate increase of 0.04\textdegree/s (around the same axis) were observed. The simultaneous occurrence of these anomalies, starting from a sudden attitude change and ending with a permanent partial power loss, made an MMOD (Micro-Meteoroid and Orbital Debris) impact onto a solar array a possible explanation for this event. While the spacecraft is able to continue its mission nominally, a detailed investigation involving ESA's Space Debris and Flight Dynamics experts was conducted. An MMOD impact as an explanation gained further credibility, due to the pictures of the solar array taken by the on-board camera displaying a significant damage area. On September 7th, JSpOC (US Joint Space Operations Centre) informed SDO on 8 tracked fragments that are considered to be released by Sentinel-1A after the impact. This paper addresses the analysis that was performed on the data characterising the attitude and orbit change, the on-board camera image, and the tracked fragments. The data helped to identify the linear momentum vector while a flux analysis helped to identify the origin of the impactor and allowed to understand its mass and size characteristics.},
  journal = {Acta Astronautica},
  doi = {10.1016/j.actaastro.2017.05.010},
  author = {Krag, H. and Serrano, M. and Braun, V. and Kuchynka, P. and Catania, M. and Siminski, J. and Schimmerohn, M. and Marc, X. and Kuijper, D. and Shurmer, I. and O'Connell, A. and Otten, M. and Mu{\~n}oz, Isidro and Morales, J. and Wermuth, M. and McKissock, D.},
  month = aug,
  year = {2017},
  keywords = {Anomaly,Hypervelocity impact,Sentinel,Space debris,Space surveillance},
  pages = {434-443},
  file = {/home/yuri/Zotero/storage/3H577VX4/Krag et al_2017_A 1 cm space debris impact onto the Sentinel-1A solar array.pdf;/home/yuri/Zotero/storage/GFW2VW6I/S0094576517304125.html}
}
% == BibTeX quality report for kragCmSpaceDebris2017:
% Missing required field 'number'

@article{kacfahemaniUnderstandableBigData2015,
  title = {Understandable {{Big Data}}: {{A}} Survey},
  volume = {17},
  issn = {1574-0137},
  shorttitle = {Understandable {{Big Data}}},
  abstract = {This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.},
  journal = {Computer Science Review},
  doi = {10.1016/j.cosrev.2015.05.002},
  author = {Kacfah Emani, Cheikh and Cullot, Nadine and Nicolle, Christophe},
  month = aug,
  year = {2015},
  keywords = {Hadoop,Big data,Coreference resolution,Entity linking,Information extraction,Ontology alignment,Reasoning},
  pages = {70-81},
  file = {/home/yuri/Zotero/storage/ZNX9J5D4/Kacfah Emani et al_2015_Understandable Big Data.pdf;/home/yuri/Zotero/storage/DTVHNSAU/S1574013715000064.html}
}
% == BibTeX quality report for kacfahemaniUnderstandableBigData2015:
% Missing required field 'number'

@article{naoualiApproximationCubesOLAP2006,
  title = {Approximation Des Cubes {{OLAP}} et G{\'e}n{\'e}ration de R{\`e}gles Dans Les Entrep{\^o}ts de Donn{\'e}es},
  issn = {1114-8802},
  abstract = {DOAJ is an online directory that indexes and provides access to quality open access, peer-reviewed journals.},
  language = {en},
  number = {2},
  journal = {Electronic Journal of Information Technology},
  author = {Naouali, Sami and Missaoui, Rokia},
  month = apr,
  year = {2006},
  file = {/home/yuri/Zotero/storage/WZCHFY8G/ae80a932f71b4ddb9c6f023ebfa6cf7a.html}
}
% == BibTeX quality report for naoualiApproximationCubesOLAP2006:
% Missing required field 'pages'
% Missing required field 'volume'

@article{naoualiOLAPCubeApproximation2006,
  title = {{OLAP Cube Approximation and Rule Generation in Data Warehouses}},
  volume = {0},
  copyright = {Copyright (c) 2006 Sami Naouali, Rokia Missaoui},
  issn = {1114-8802},
  abstract = {This paper presents a new approach toward approximate query answering in data warehouses. The approach is based on an adaptation of rough set theory to multidimensional data, and offers cube exploration and mining facilities. The objective of this work is to integrate approximation mechanisms and associated operators into data cubes in order to produce views that can then be explored using OLAP or data mining techniques. The integration of data approximation capabilities with OLAP techniques offers additional facilities for cube exploration and analysis. The proposed approach allows the user to work either in a restricted mode using a cube lower approximation or in a relaxed mode using cube upper approximation. The former mode is useful when the query output is large, and hence allows the user to focus on a reduced set of fully matching tuples. The latter is useful when a query returns an empty or small answer set, and hence helps relax the query conditions so that a superset of the answer is returned.},
  language = {fr},
  number = {2},
  journal = {Electronic Journal of Information Technology},
  author = {Naouali, Sami and Missaoui, Rokia},
  month = apr,
  year = {2006},
  file = {/home/yuri/Zotero/storage/FNWQH5X3/Naouali_Missaoui_2006_OLAP Cube Approximation and Rule Generation in Data Warehouses.pdf;/home/yuri/Zotero/storage/TJSSWCVT/64.html}
}
% == BibTeX quality report for naoualiOLAPCubeApproximation2006:
% Missing required field 'pages'
% ? Title looks like it was stored in title-case in Zotero

@article{cuzzocreaTopdownApproachCompressing2010,
  title = {A Top-down Approach for Compressing Data Cubes under the Simultaneous Evaluation of Multiple Hierarchical Range Queries},
  volume = {34},
  issn = {1573-7675},
  abstract = {A novel top-down compression technique for data cubes is introduced and experimentally assessed in this paper. This technique considers the previously unrecognized case in which multiple Hierarchical Range Queries (HRQ), a very useful class of OLAP queries, must be evaluated against the target data cube simultaneously. This scenario makes traditional data cube compression techniques ineffective, as, contrary to the aim of our work, these techniques take into consideration one constraint only (e.g., a given storage space bound). The result of our study consists in introducing an innovative multiple-objective OLAP computational paradigm, and a hierarchical multidimensional histogram, whose main benefit is meaningfully implementing an intermediate compression of the input data cube able to simultaneously accommodate an even large family of different-in-nature HRQ. A complementary contribution of our work is represented by a wide experimental evaluation of the performance of our technique against both benchmark and real-life data cubes, also in comparison with state-of-the-art histogram-based compression techniques.},
  language = {en},
  number = {3},
  journal = {Journal of Intelligent Information Systems},
  doi = {10.1007/s10844-009-0099-2},
  author = {Cuzzocrea, Alfredo},
  month = jun,
  year = {2010},
  keywords = {Advanced OLAP,Compressing data cubes under simultaneous multiple OLAP queries,Multi-objective compression of data cubes,Multiple-query data cube compression techniques},
  pages = {305-343},
  file = {/home/yuri/Zotero/storage/2DGDS3B7/Cuzzocrea_2010_A top-down approach for compressing data cubes under the simultaneous.pdf;/home/yuri/Zotero/storage/YVFDRI8V/2853843.html}
}

@article{heidornSheddingLightDark2008,
  title = {Shedding {{Light}} on the {{Dark Data}} in the {{Long Tail}} of {{Science}}},
  volume = {57},
  issn = {1559-0682},
  abstract = {ARRAY(0x558f3c8a2a10)},
  language = {en},
  number = {2},
  journal = {Library Trends},
  doi = {10.1353/lib.0.0036},
  author = {Heidorn, P. Bryan},
  year = {2008},
  pages = {280-299},
  file = {/home/yuri/Zotero/storage/XQ8JDPCG/Heidorn_2008_Shedding Light on the Dark Data in the Long Tail of Science.pdf}
}
% == BibTeX quality report for heidornSheddingLightDark2008:
% ? Title looks like it was stored in title-case in Zotero

@article{oussousBigDataTechnologies2018,
  title = {Big {{Data}} Technologies: {{A}} Survey},
  volume = {30},
  issn = {1319-1578},
  shorttitle = {Big {{Data}} Technologies},
  abstract = {Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications' requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.},
  number = {4},
  journal = {Journal of King Saud University - Computer and Information Sciences},
  doi = {10.1016/j.jksuci.2017.06.001},
  author = {Oussous, Ahmed and Benjelloun, Fatima-Zahra and Ait Lahcen, Ayoub and Belfkih, Samir},
  month = oct,
  year = {2018},
  keywords = {Big Data,Hadoop,NoSQL,Machine learning,Big Data analytics,Big Data distributions},
  pages = {431-448},
  file = {/home/yuri/Zotero/storage/DQJL4RSG/Oussous et al_2018_Big Data technologies.pdf;/home/yuri/Zotero/storage/NJL9D6W8/S1319157817300034.html}
}

@article{chenPrioritybasedConflictavoidanceHeuristics2018,
  title = {Priority-Based and Conflict-Avoidance Heuristics for Multi-Satellite Scheduling},
  volume = {69},
  issn = {1568-4946},
  abstract = {In this paper we address the problem of multi-satellite scheduling with limited observing ability. As with other computationally hard combinatorial optimization problems, a two-stage heuristic method is developed to obtain high quality solutions in a reasonable amount of computation time. The first stage involves the determination of an observing sequence and the generation of a feasible scheduling scheme. We propose several priority-based and conflict-avoidance heuristic strategies and develop the time-based greedy approaches, the weight-based greedy approaches, and an improved differential evolution (DE) algorithm. The second stage consists of further improvement strategies under different resource contentions, thus improving the scheduling results further. Finally, we design different classes of instances to test the efficiency and applicability of the methods. Computational results reveal that the new proposed methods routinely delivered very close to optimal solutions.},
  journal = {Applied Soft Computing},
  doi = {10.1016/j.asoc.2018.04.021},
  author = {Chen, Xiaoyu and Reinelt, Gerhard and Dai, Guangming and Wang, Maocai},
  month = aug,
  year = {2018},
  keywords = {Optimization,Scheduling,Differential evolution,Earth observing satellites,Heuristic},
  pages = {177-191},
  file = {/home/yuri/Zotero/storage/S3DYN34A/Chen et al_2018_Priority-based and conflict-avoidance heuristics for multi-satellite scheduling.pdf;/home/yuri/Zotero/storage/GZU6LVF7/S1568494618302126.html}
}
% == BibTeX quality report for chenPrioritybasedConflictavoidanceHeuristics2018:
% Missing required field 'number'

@article{nguyenEfficientAlgorithmsMining2017,
  title = {Efficient Algorithms for Mining Colossal Patterns in High Dimensional Databases},
  volume = {122},
  issn = {0950-7051},
  abstract = {Mining association rules plays an important role in decision support systems. To mine strong association rules, it is necessary to mine frequent patterns. There are many algorithms that have been developed to efficiently mine frequent patterns, such as Apriori, Eclat, FP-Growth, PrePost, and FIN. However, these are only efficient with a small number of items in the database. When a database has a large number of items (from thousands to hundreds of thousands) but the number of transactions is small, these algorithms cannot run when the minimum support threshold is also small (because the search space is huge). This thus causes the problem of mining colossal patterns in high dimensional databases. In 2012, Sohrabi and Barforoush proposed the BVBUC algorithm for mining colossal patterns based on a bottom-up scheme. However, this needs more time to check subsets and supersets, because it generates a lot of candidates and consumes more memory to store these. In this paper we propose new, efficient algorithms for mining colossal patterns. Firstly, the CP (Colossal Pattern)-tree is designed. Next, we develop two theorems to rapidly compute patterns of nodes and prune nodes without the loss of information in colossal patterns. Based on the CP-tree and these theorems, an algorithm (named CP-Miner) is proposed to solve the problem of mining colossal patterns. A sorting strategy for efficiently mining colossal patterns is thus developed. This strategy helps to reduce the number of significant candidates and the time needed to check subsets and supersets. The PCP-Miner algorithm, which uses this strategy, is then proposed, and we also conduct experiments to show the efficiency of these algorithms.},
  journal = {Knowledge-Based Systems},
  doi = {10.1016/j.knosys.2017.01.034},
  author = {Nguyen, Thanh-Long and Vo, Bay and Snasel, Vaclav},
  month = apr,
  year = {2017},
  keywords = {Data mining,Bottom up,Colossal patterns,High dimensional databases},
  pages = {75-89},
  file = {/home/yuri/Zotero/storage/PWZ2IRJ3/Nguyen et al_2017_Efficient algorithms for mining colossal patterns in high dimensional databases.pdf;/home/yuri/Zotero/storage/HSRZT7V6/S095070511730045X.html}
}
% == BibTeX quality report for nguyenEfficientAlgorithmsMining2017:
% Missing required field 'number'

@article{guntherDebatingBigData2017,
  title = {Debating Big Data: {{A}} Literature Review on Realizing Value from Big Data},
  volume = {26},
  issn = {0963-8687},
  shorttitle = {Debating Big Data},
  abstract = {Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.},
  number = {3},
  journal = {The Journal of Strategic Information Systems},
  doi = {10.1016/j.jsis.2017.07.003},
  author = {G{\"u}nther, Wendy Arianne and Rezazade Mehrizi, Mohammad H. and Huysman, Marleen and Feldberg, Frans},
  month = sep,
  year = {2017},
  keywords = {Analytics,Big data,Interconnectivity,Literature review,Portability,Value realization},
  pages = {191-209},
  file = {/home/yuri/Zotero/storage/WKLUJHWU/GÃ¼nther et al_2017_Debating big data.pdf;/home/yuri/Zotero/storage/JRGAYUJI/S0963868717302615.html}
}

@article{chenMixedIntegerLinear2019,
  title = {A Mixed Integer Linear Programming Model for Multi-Satellite Scheduling},
  volume = {275},
  issn = {0377-2217},
  abstract = {We address the multi-satellite scheduling problem with limited observation capacities that arises from the need to observe a set of targets on the Earth's surface using imaging resources installed on a set of satellites. We define and analyze the conflict indicators of all available visible time windows of missions, as well as the feasible time intervals of resources. The problem is then formulated as a mixed integer linear programming model, in which constraints are derived from a careful analysis of the interdependency between feasible time intervals that are eligible for observations. We apply the proposed model to several different problem instances that reflect real-world situations. The computational results verify that our approach is effective for obtaining optimum solutions or solutions with a very good quality.},
  number = {2},
  journal = {European Journal of Operational Research},
  doi = {10.1016/j.ejor.2018.11.058},
  author = {Chen, Xiaoyu and Reinelt, Gerhard and Dai, Guangming and Spitz, Andreas},
  month = jun,
  year = {2019},
  keywords = {Earth observing satellites,Integer programming,Mathematical programming,Scheduling},
  pages = {694-707},
  file = {/home/yuri/Zotero/storage/WWNQMIUZ/Chen et al_2019_A mixed integer linear programming model for multi-satellite scheduling.pdf;/home/yuri/Zotero/storage/Y3R39PMT/S0377221718309998.html}
}

@article{sivarajahCriticalAnalysisBig2017,
  title = {Critical Analysis of {{Big Data}} Challenges and Analytical Methods},
  volume = {70},
  issn = {0148-2963},
  abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 \textendash{} What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 \textendash{} What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.},
  journal = {Journal of Business Research},
  doi = {10.1016/j.jbusres.2016.08.001},
  author = {Sivarajah, Uthayasankar and Kamal, Muhammad Mustafa and Irani, Zahir and Weerakkody, Vishanth},
  month = jan,
  year = {2017},
  keywords = {Big Data,Big Data Analytics,Challenges,Methods,Systematic literature review},
  pages = {263-286},
  file = {/home/yuri/Zotero/storage/UYC24FTN/Sivarajah et al_2017_Critical analysis of Big Data challenges and analytical methods.pdf;/home/yuri/Zotero/storage/YX99ENEI/S014829631630488X.html}
}
% == BibTeX quality report for sivarajahCriticalAnalysisBig2017:
% Missing required field 'number'

@article{xiaoluMultiSatellitesScheduling2014,
  title = {Multi Satellites Scheduling Algorithm Based on Task Merging Mechanism},
  volume = {230},
  issn = {0096-3003},
  abstract = {Earth observation satellites are platforms equipped with optical instruments that orbit the earth to take photographs of specific areas at users' requests. Compared with huge user requests, satellites are still scanty resources. For some task, the satellite has to roll its camera to take the desired image. However, many satellites are rigidly restricted on maneuverability. As a result, the performances of satellites are greatly confined. Therefore, we need a scientific observation plan to weaken the constraints arising from satellites' poor slew ability. To solve the problem we present a multi satellites scheduling algorithm based on task merging mechanism. The algorithm partitions the problem into two sub-problems: task assignment and task merging. In task assignment, we propose an adaptive ant colony optimization algorithm to select specific time window for each task, creating a task list for each satellite. In task merging, we propose the concept of task combination and develop a dynamic programming algorithm to find the best merging plan for each satellite. The two sub-problems are logically coupled; a valid observation plan will be got after much iteration. Finally, a series of test examples are given out, which demonstrate our algorithm to be effective.},
  journal = {Applied Mathematics and Computation},
  doi = {10.1016/j.amc.2013.12.109},
  author = {Xiaolu, Liu and Baocun, Bai and Yingwu, Chen and Feng, Yao},
  month = mar,
  year = {2014},
  keywords = {Adaptive ant colony optimization,Decomposition optimization,Dynamic programming,Earth observation satellite,Scheduling,Task merging},
  pages = {687-700},
  file = {/home/yuri/Zotero/storage/FP746JSK/Xiaolu et al_2014_Multi satellites scheduling algorithm based on task merging mechanism.pdf;/home/yuri/Zotero/storage/UYHJ6TQC/S0096300313013994.html}
}
% == BibTeX quality report for xiaoluMultiSatellitesScheduling2014:
% Missing required field 'number'

@article{sivarajahCriticalAnalysisBig2017a,
  title = {Critical Analysis of {{Big Data}} Challenges and Analytical Methods},
  volume = {70},
  issn = {0148-2963},
  abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 \textendash{} What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 \textendash{} What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.},
  journal = {Journal of Business Research},
  doi = {10.1016/j.jbusres.2016.08.001},
  author = {Sivarajah, Uthayasankar and Kamal, Muhammad Mustafa and Irani, Zahir and Weerakkody, Vishanth},
  month = jan,
  year = {2017},
  keywords = {Big Data,Big Data Analytics,Challenges,Methods,Systematic literature review},
  pages = {263-286},
  file = {/home/yuri/Zotero/storage/55AM35RA/Sivarajah et al_2017_Critical analysis of Big Data challenges and analytical methods.pdf;/home/yuri/Zotero/storage/3F3TQ2SJ/S014829631630488X.html}
}
% == BibTeX quality report for sivarajahCriticalAnalysisBig2017a:
% Missing required field 'number'

@article{keshavarziMuonG2Experiment2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.00497},
  primaryClass = {hep-ex, physics:hep-ph},
  title = {The {{Muon}} \$g-2\$ Experiment at {{Fermilab}}},
  abstract = {The current \$\textbackslash{}sim3.5\textbackslash{}sigma\$ discrepancy between the experimental measurement and theoretical prediction of the muon magnetic anomaly, \$a\_\{\textbackslash{}mu\}\$, stands as a potential indication of the existence of new physics. The Muon \$g-2\$ experiment at Fermilab is set to measure \$a\_\{\textbackslash{}mu\}\$ with a four-fold improvement in the uncertainty with respect to previous experiment, with an aim to determine whether the \$g-2\$ discrepancy is well established. The experiment recently completed its first physics run and a summer programme of essential upgrades, before continuing on with its experimental programme. The Run-1 data alone are expected to yield a statistical uncertainty of 350 ppb and the publication of the first result is expected in late-2019.},
  journal = {arXiv:1905.00497 [hep-ex, physics:hep-ph]},
  author = {Keshavarzi, Alexander},
  month = may,
  year = {2019},
  keywords = {High Energy Physics - Experiment,High Energy Physics - Phenomenology},
  file = {/home/yuri/Zotero/storage/QY5SKTCK/Keshavarzi_2019_The Muon $g-2$ experiment at Fermilab.pdf;/home/yuri/Zotero/storage/PJGLP5AK/1905.html}
}
% == BibTeX quality report for keshavarziMuonG2Experiment2019:
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'
% ? Possibly abbreviated journal title arXiv:1905.00497 [hep-ex, physics:hep-ph]


