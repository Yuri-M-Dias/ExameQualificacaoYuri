%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{FUNDAMENTAÇÃO}
\label{ch:fun}

Este capítulo apresenta os conceitos fundamentais relacionados a essa proposta, começando pelo básico de operação dos satélites, apresentando os conceito de \textit{Data Warehouse}, \textit{OLAP} e Cubo de Dados, e por fim a definição de \textit{Big Data} utilizada no trabalho.

\section{Operação de Satélites}
\label{ch:fun:operations}

{\color{red}
\textbf{Dataflow? Explicar a operação de satélites bem por cima}
}

\section{Data Warehouse}
\label{ch:fun:dw}

Um Armazém de Dados ou Data Warehouse (DW) é um repositório de dados orientado por assunto, integrado, variado ou particionado em função do tempo e não volátil, que auxilia no gerenciamento do processo de tomada decisões~\cite{inmonUsingDataWarehouse1994}.
Essa definição pode ser dividida em:

\begin{itemize}
	\item \textbf{Orientado por assunto}: o DW é utilizado para a análise de uma área em específico.
Por exemplo, é de interesse analisar especialmente os dados da carga útil de uma forma específica.
	\item \textbf{Integrado}: o DW deve integrar dados vindos de múltiplas fontes de uma forma estrutura.
Por exemplo, mesmo que existam duas representações diferentes para um mesmo produto, o DW deve possuir apenas uma representação.
Isso requer o uso de técnicas de limpeza e integração dos dados, de modo a garantir a consistência dos dados.
	\item \textbf{Variado em função do tempo}: o DW deve conter, explícita ou implicitamente a perspectiva de tempo.
Isso quer dizer que o DW possui dados históricos e eles podem ser consultados durante a análise.
Por exemplo, pode se querer saber de dados de dias, meses ou anos atrás.
	\item \textbf{Não volátil}: uma vez dentro do DW, os dados não são removidos ou atualizados, sendo um requisito para a consulta de dados históricos.
\end{itemize}

Essas características diferem o \textit{Data Warehouse} de outros sistemas de repositório, como sistemas de banco de dados, sistemas de processamento de transações e sistemas de arquivos~\cite{silva:2015:abordagensParaCubo,bimonteOpenIssuesBig2016}.

Um DW é geralmente representado por um modelo dimensional que permite eficiência na organização dos dados e na recuperação de informações gerenciais~\cite{kimballDataWarehouseToolkit2013}.
Neste modelo são definidos fatos, dimensões e medidas.
Um fato corresponde ao assunto de negócio a ser analisado, cada dimensão é uma perspectiva de visualização do assunto de negócio e medidas são valores numéricos que quantificam o assunto de negócio.
Uma das dimensões é sempre temporal para permitir a análise do assunto ao longo do tempo~\cite{silva:2015:abordagensParaCubo}.

\section{OLAP}
\label{ch:fun:olap}

\textit{On-line Analytical Processing} (OLAP) é um termo que se refere a um conjunto de ferramentas que são utilizadas para resumir, consolidar, visualizar, aplicar formulações e sintetizar dados de acordo com múltiplas dimensões~\cite{coddProvidingOlapUseranalysts1998}.

Um sistema OLAP permite a resposta de consultas multidimensionais usando dados armazenados no \textit{Data Warehouse}~\cite{kimballDataWarehouseToolkit2013}, sendo que as características principais são~\cite{bimonteOpenIssuesBig2016}:

\begin{itemize}
	\item \textbf{Consultas Online}: as consultas devem ser feitas \textit{Online}, isto é, em tempo real para o usuário.
	\item \textbf{Consultas Multidimensionais}: Consultas são definidas utilizando as dimensões e medidas providas pelo \textit{Data Warehouse}, que esperam dados de alta qualidade.
	\item \textbf{Representação simples}: os resultados das consultas devem ser representados utilizando tabelas e gráficos, pois os usuários finais geralmente são tomadores de decisão que precisam de visualizações relevantes.
	\item \textbf{Exploratórias}: as consultas são utilizadas em carácter exploratório, pois geralmente os usuários não conhecem de antemão todos os dados disponíveis para consultas.
\end{itemize}

Cada ferramenta OLAP deve manipular um novo tipo abstrato de dados (TAD), chamado de cubo de dados, utilizando estratégias específicas devido ao modo de como os dados são armazenados, sendo classificadas em~\cite{moreiraFullPartialData2012}:

\begin{itemize}
	\item \textbf{\textit{Relational OLAP} (ROLAP)}: utilizam Sistemas de Gerenciamento de Banco de Dados (\textit{Data base Management System} - DBMS) relacionais para o gerenciamento e armazenamento dos cubos de dados.
Ferramentas ROLAP incluem otimizações para cada DBMS, implementação da lógica de navegação em agregações, serviços e ferramentas adicionais;
	\item \textbf{\textit{Multidimensional} OLAP (MOLAP)}: implementam estruturas de dados multidimensionais para armazenar cubo de dados em memória principal ou em memória externa.
Não há utilização de repositórios relacionais para armazenar dados multidimensionais e a lógica de navegação já é integrada a estrutura proposta;
	\item \textbf{\textit{Hybrid} OLAP (HOLAP)}: combinam técnicas ROLAP e MOLAP, onde normalmente os dados detalhados são armazenados em base de dados relacionais (ROLAP), e as agregações são armazenadas em estruturas de dados multidimensionais (MOLAP).
\end{itemize}

Além desses, existem sistemas OLAP voltados para um domínio ou estilo de dados específico, como é o caso do \textit{Spatial OLAP} (SOLAP), voltado para consultas espaciais~\cite{viswanathanUsercentricSpatialData2014}.

É importante ressaltar a diferença entre OLAP e \textit{Online Transaction Processing} (OLPT), visto que sistemas comuns de banco de dados utilizam apenas OLTP, que tem o objetivo de realizar transações e processar consultas online.
Isso cobre a grande maioria das operações do dia-a-dia, como controle de estoque, operações bancárias, etc, servindo a diversos usuários de uma organização.
Já o OLAP é utilizado por tomadores de decisão e analistas de dados, sendo voltado para decisões de mais alto nível na organização~\cite{hanDataMiningConcepts2011}.

\section{Cubo de Dados}
\label{ch:fun:cube}

O Cubo de Dados originalmente foi criado como um operador relacional que gera todas as combinações possíveis de seus atributos de acordo com uma medida~\cite{grayDataCubeRelational1996}.

Um cubo de dados consiste em um conjunto de medidas para análise e um conjunto de dimensões que provêm o contexto das medidas.
Uma medida é um atributo cujos valores definem os fatos de interesse na análise e em cujos valores são aplicadas as funções de agregação (soma, quantidade, média, sentimento, união, interseção de geo-objetos, etc.).
Uma dimensão é um elemento que participa de um fato e que determina o contexto de um assunto de negócios, sendo que podem ser compostas por membros que podem conter hierarquias.
Membros são as possíveis divisões ou classificações de uma dimensão~\cite{silva:2015:abordagensParaCubo}.
Por exemplo, uma dimensão de tempo pode ser dividida em: década, ano, mês, dia e hora.

A organização de um cubo de dados possibilita ao usuário a flexibilidade de visualização dos dados a partir de diferentes perspectivas, já que o operador gera combinações através do conceito do valor \textit{ALL}, onde este conceito representa a agregação de todas as combinações possíveis de um conjunto de valores de atributos.
Operações em cubos de dados existem a fim de materializar estas diferentes visões, permitindo busca e análise interativa dos dados armazenados~\cite{silva:2015:abordagensParaCubo}.

Um cubo de dados é composto por células e cada célula possui valores para cada dimensão, incluindo \textit{ALL}, e valores para as medidas.
O valor de uma medida é computado para uma determinada célula utilizando níveis de agregação inferiores para gerar os valores dos níveis de agregação superiores na estratégia \textit{Top-down}, com a ordem inversa sendo a \textit{Bottom-up}~\cite{silva:2015:abordagensParaCubo}.

A computação do cubo de dados é um problema exponencial em relação ao tempo de execução e ao consumo de memória, portando dada uma relação de entrada R com tuplas de tamanho $n$, a saída é $2^n$, onde $n$ é o número de dimensões de um cubo.

{\color{red} Traduzir exemplo do rodrigo do curso p/ satélites?!}

\subsection{Modelagem dimensional}
\label{ch:fun:cube:dimm}

\subsubsection{Esquema estrela}
\label{ch:fun:cube:dimm:star}

{\color{red} Imagem}

\subsubsection{Esquema Floco de Neve}
\label{ch:fun:cube:dimm:snow}

{\color{red} Imagem}

\subsubsection{Esquema Constelação de Fatos}
\label{ch:fun:cube:dimm:constellation}

{\color{red} Imagem}

\subsection{Hierarquias de conceito}
\label{ch:fun:cube:concept}

Uma hierarquia de conceitos é utilizada para definir uma sequência de mapeamento entre um conjunto de conceitos de baixo nível para um conjunto de conceitos de alto nível, mais gerais.
É um estilo de agrupamento e discretização, pois agrupa os valores de modo a reduzir a cardinalidade de uma dimensão~\cite{hanDataMiningConcepts2011}.
Elas ajudam a tornar a análise mais fácil de ser entendida, pois as operações traduzem os dados de baixo nível em uma representação que é mais fácil para o usuário final, assim facilitando a execução das consultas e o seu subsequente uso.

\subsection{Medidas}

According to [@hanDataMiningConcepts2011], measures can be classified into three different types: *distributive*, *algebraic* and *holistic*. A **distributive** measure is a measure whose calculation can be partitioned and later joined, and the result would be the same as if the executing the calculation with all the available data at once. In figure [\@ref(fig:datacube)], the *mean* is a distributive measure.
An **algebraic** measure is a measure whose calculation can be made from two or more distributive measures, like an *average* operation, which can be performed with an *sum* and an *count* measures.
A measure is **holistic** if it can only be computed without a distributive measure, meaning that it can't be divided into smaller operations and must be computed for all the data, like the measure *standard deviation* in figure [\@ref(fig:datacube)].

Holistic measures are the hardest to calculate, and so it's an area with increased research attention [@silva:2015:abordagensParaCubo].

\subsection{Operações OLAP}
\label{ch:fun:olap:ops}

Since that the focus is on analysis, an OLAP architecture generally needs to support some common operators: *rollup*, which increases the level of aggregation or climbs a concept hierarchy; *drill-down*, which decreases the level of aggregation, going from more general data to more specific data, sometimes along one or more dimension hierarchies; *slice and dice*, which create selections and projections on the data; and *pivot*, which re-orients the visualization of the data.

There are other operators, like *drill-across* and *drill-through*, but the support and existence of these will depend on the type of OLAP that is necessary [@hanDataMiningConcepts2011].

\subsection{Computação do cubo de dados}

To properly calculate a data cube for some measures and dimensions, you have to count the cardinality of each dimension against the cardinality of all other dimensions.
While manageable for a few dimensions, this computation becomes almost impossible for cubes with high dimensions as the number of combinations becomes too much for a single computer to handle.
This lead to the development of data cube algorithms that optimize for the most relevant measures in the data cube [@silva:2015:abordagensParaCubo].

A *cuboid* is a part of a data cube. For example, if you have three dimensions: temperature, tension and time, a 2-D cuboid could be made from the dimensions temperature and tension, and a 3-D cuboid would be the same as the full data cube, like figure [\@ref(fig:datacube)].
The data cube algorithms focus on the computation of cuboids, as the every cube is composed of these smaller cuboids.
This leads to the existence of the *curse of dimensionality*, as for $n$ of dimensions there will be $2^n$ possible cuboid computations, making full materialization very difficult after a few dimensions [@hanDataMiningConcepts2011;@silva:2015:abordagensParaCubo].

For the algorithms, they can be in three different categories: Computing all the cuboids for the data cube leads to a fully materialized data cube; not computing any cuboid beforehand leads to a non-materialized data cube, and partially computing some cuboids leads to partial materialization.

The non-materialized cube has the lowest amount of required memory, but the highest query response time.
The fully materialized cube leads to the lowest query response times, as all combinations are already computed, but it needs the highest amount of memory and is thus very hard to compute.
As for the partially materialized cube, it is the main issue for most of the algorithms: how to materialize only the most relevant cuboids, and thus achieve a good compromise between memory usage and query performance?

There's some different types of partially materialized cubes, like the *iceberg*, which is a cube with only cells that have passed a certain condition; *shell fragments* compute only cubes with a few dimensions (from 3 to 5) and aggregate those cubes when a bigger number of dimensions is required and *closed cubes* are cubes whose cells with identical measures are grouped into a single abstraction, also called *closed cells*.

To choose which cuboids to materialize, there's a plethora of different algorithms.
Two of the classical ones are *Bottom-up* or *Top-down* strategies.
Bottom-up starts from the most specific cuboid, called the base cuboid, and goes to the less specific cuboid.
Top-down is the inverse: it starts from the least specific cuboid, called the apex cuboid, and goes to the base cuboid.
Most of these are tested and overviewed in [@silva:2015:abordagensParaCubo], and won't be repeated here for brevity.

[BUC, Top-down, Bottom-up]

\section{Big Data}
\label{ch:fun:bigdata}

O termo \textit{Big Data} vem evoluindo ao longo dos anos, e para este trabalho vamos utilizar a definição dos \textit{5 Vs}~\cite{bimonteOpenIssuesBig2016}: Volume, Variedade, Velocidade, Valor e Veracidade. Em detalhes:

\begin{itemize}
	\item \textbf{Volume}: esse termo geralmente especifica uma quantidade de dados em que um sistema tradicional de gerenciamento de banco de dados é ineficaz.
É importante ressaltar que isso não se trata apenas do armazenamento dos dados, mas também do seu processamento~\cite{boussoufBigDataBased2018}.
Usar um grande volume de dados geralmente implica em modelos melhores, que então produzem análises melhores, justificando a coleta de uma grande quantidade de dados.
	\item \textbf{Variedade}: dados são provenientes de fontes diferentes, com formatos diferentes, sem um esquema de modelagem padronizado, como dados advindos de \textit{logs} de computadores, dados de sensores, dados multimídia, etc.
Como consequência, esses dados devem ser utilizados da forma mais transparente o possível na análise.
	\item \textbf{Velocidade}: dados são disponibilizados de uma forma muito rápida, e devem ser analisados da forma mais rápida o possível.
Isso implica que os dados podem ser guardados e analisados até em tempo real.
	\item \textbf{Valor}: os dados devem ser armazenados para criar algum valor para os seus usuários, seja ele econômico, científico, social, organizacional, etc.
	\item \textbf{Veracidade}: os dados não possuem garantias quanto a sua qualidade, como inconsistências e falta de acurácia, porém a análise deve ser de alta qualidade de qualquer forma.
\end{itemize}

Estes V's estão relacionados com a construção de um \textit{Data Warehouse}, sendo que também podem ser vistos como requisitos para a crição de um para um conjunto de dados caracterizado como \textit{Big Data}~\cite{zhangBigDataFramework2017}.
Em especial, existe um certo relacionamento com a ideia de "\textit{NoSQL}" ("Não apenas SQL", em inglês), em que não apenas sistemas de banco de dados relacionais são utilizados, mas também outros paradigmas são utilizados, como orientados a documentos, chave e valor, etc~\cite{bimonteOpenIssuesBig2016}.

